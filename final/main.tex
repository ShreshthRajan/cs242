\documentclass[10pt]{article}

\usepackage[margin=0.3in]{geometry} % very small margins
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{multicol}
\usepackage{amsmath}

% Tighter lists and headings
\setlist{nosep,leftmargin=*}
\titlespacing*{\section}{0pt}{0.6ex plus 0.2ex}{0.4ex}
\titlespacing*{\subsection}{0pt}{0.5ex plus 0.2ex}{0.3ex}

% Column separation
\setlength{\columnsep}{0.2in}

\pagestyle{empty} % no headers/footers to save space

\title{CS 2420 -- Lec 1}
\author{}
\date{}

\begin{document}

\begin{multicols*}{2}

\section*{Lec 1: Big Picture, Deep Learning, LLMs}

\section*{1.\ Big Picture: Why This Course?}

\subsection*{Rapidly advancing AI}
\begin{itemize}
  \item With large language models (LLMs), a few prompts can:
  \begin{itemize}
    \item Generate images and videos.
    \item Write and debug code.
    \item Search, summarize, and organize information.
  \end{itemize}
  \item AI is already deployed in the real world:
  \begin{itemize}
    \item Self-driving vehicles on the streets.
    \item Humanoid robots that can dance, play games, and eventually work.
    \item Automated workflows with AI agents inside enterprises.
  \end{itemize}
\end{itemize}

\subsection*{AI is self-improving}
\begin{itemize}
  \item AI systems can:
  \begin{itemize}
    \item Generate synthetic data.
    \item Learn from user interaction logs.
  \end{itemize}
  \item This creates a recursive learning cycle:
  \begin{itemize}
    \item AI helps build better AI.
    \item Pace of innovation increases over time.
  \end{itemize}
\end{itemize}

\subsection*{A new era: ``General Special Computing''}
\begin{itemize}
  \item We can build highly optimized special-purpose AI accelerators (CPU, GPU, TPU, special chips, computing-in-memory).
  \item Deep neural networks (DNNs) apply to many applications, so DNN accelerators help all of them.
  \item Many companies and startups are designing AI chips; this is a new era the field has waited decades for.
\end{itemize}

\section*{2.\ Deep Learning Refresher and DNNs}

\subsection*{Deep learning underlies modern ML/AI}
\begin{itemize}
  \item Deep learning is a major driver of recent excitement in ML and AI.
\end{itemize}

\subsection*{What is an AI model? (high-school-level view)}
\begin{itemize}
  \item A model is a function mapping inputs to outputs:
  \[
    x \mapsto y.
  \]
  \item Simple example: quadratic regression with parameters $\theta_0, \theta_1, \theta_2$:
  \[
    y = \theta_0 + \theta_1 x + \theta_2 x^2.
  \]
  \item Given training examples $(x_i, y_i)$, we learn a curve to predict $y$ for unseen $x$.
  \item For accurate prediction:
  \begin{itemize}
    \item Use many parameters $\theta_1, \theta_2, \dots$.
    \item Use many training examples to avoid overfitting.
  \end{itemize}
  \item Deep learning: automatically learns a large number of parameters in a neural-network fitting function.
\end{itemize}

\subsection*{Deep Neural Networks (DNNs)}
\begin{itemize}
  \item Inspired by the brain:
  \begin{itemize}
    \item Many layers of neurons between input and output.
    \item Every connection has a weight (parameter).
  \end{itemize}
  \item Training:
  \begin{itemize}
    \item Minimize loss (prediction error vs.\ labels) over training data.
    \item Use gradient descent in a high-dimensional parameter space (dimension can be billions).
  \end{itemize}
  \item Inference: forward pass through the network.
  \item Training: backward pass for gradient-based optimization.
  \item Training large models requires massive data and compute.
\end{itemize}

\section*{3.\ CNNs and Advances in Large Models}

\subsection*{Convolutional Neural Networks (CNNs)}
\begin{itemize}
  \item A deep learning example for computer vision.
  \item Two major parts:
  \begin{enumerate}
    \item \textbf{Feature extraction} (CONV layers + nonlinearities like ReLU).
    \item \textbf{Classification} (fully connected layers).
  \end{enumerate}
\end{itemize}

\subsection*{ReLU nonlinearity}
\begin{itemize}
  \item $\operatorname{ReLU}(z) = \max\{0, z\}$.
  \item Nonlinear: $f(a x + y) \neq a f(x) + f(y)$, e.g.\ $f(-2+1)=0$ but $f(-2)+f(1)=1$.
  \item Nonlinearity lets networks approximate complex (nonlinear) functions.
\end{itemize}

\subsection*{CNN pipeline}
\begin{itemize}
  \item CONV layers extract features (edges, shapes, textures).
  \item FC layers classify objects based on extracted features.
\end{itemize}

\subsection*{Advances of large models (2012--present)}
\begin{itemize}
  \item 2012--: CNNs (AlexNet, GANs, ResNet-50).
  \item 2017--: Transformers (``Attention Is All You Need'', BERT, ViT, CLIP).
  \item 2022+: GPT-3, ChatGPT, Stable Diffusion, DALL-E, BLIP, GPT-4, Gemini.
  \item 2024--25: GPT-5, Claude, LLaMA, Grok-2, Mistral, DeepSeek, etc.
\end{itemize}

\section*{4.\ LLMs, GenAI, and Embeddings}

\subsection*{LLMs as compressed knowledge databases}
\begin{itemize}
  \item LLMs show human-like intelligence when model size crosses certain thresholds.
  \item Kolmogorov complexity: minimal bits to describe a sequence.
  \item Training an LLM is like lossy compression:
  \begin{itemize}
    \item Example: 600B tokens $\to$ 30B parameters ($\sim$20:1 compression).
  \end{itemize}
\end{itemize}

\subsection*{Generative AI (GenAI)}
\begin{itemize}
  \item GenAI can generate text, images, video, code, etc.\ from prompts.
  \item LLMs like GPT/ChatGPT have impressive and rapidly improving capabilities:
  \begin{itemize}
    \item GPT-2 (2019) $\to$ GPT-4 (2023): ``pre-schooler to high-schooler''.
    \item GPT-4 $\to$ GPT-next: projected ``high-schooler to R\&D professional''.
  \end{itemize}
\end{itemize}

\subsection*{What is a language model?}
\begin{itemize}
  \item GPT = \textbf{G}enerative \textbf{P}re-trained \textbf{T}ransformer.
  \item ChatGPT: an LLM specialized for conversational interaction.
  \item A language model is a mapping:
  \[
    \text{text} \;\to\; \text{embedding vector}
  \]
  where embeddings encode meanings (similar texts $\Rightarrow$ nearby vectors).
  \item Trained via next-token prediction on large text corpora.
\end{itemize}

\subsection*{Embeddings example}
\begin{itemize}
  \item Conceptual 2D embedding space: “Cat”, “Dog”, “Cow” near each other; “Car” elsewhere.
  \item Embedding function maps related concepts to nearby points.
\end{itemize}

\subsection*{AI training vs.\ inference; pretraining vs.\ fine-tuning}
\begin{itemize}
  \item Training:
  \begin{itemize}
    \item Learn embedding functions from data (computing can be expensive).
  \end{itemize}
  \item Inference:
  \begin{itemize}
    \item Predict properties of inputs based on embeddings (sensing can be expensive).
  \end{itemize}
  \item Pretraining:
  \begin{itemize}
    \item Self-supervised, huge unlabeled datasets (e.g.\ DINO).
    \item Produces a large pretrained model representing ``knowledge''.
  \end{itemize}
  \item Fine-tuning:
  \begin{itemize}
    \item Smaller datasets, fewer steps, adapt to specific tasks.
    \item Can incorporate RLHF to align with human preferences.
  \end{itemize}
\end{itemize}

\subsection*{LLMs: a new ``Moore's Law'' and sustainability}
\begin{itemize}
  \item State-of-the-art LLM size has been increasing $\sim$10$\times$/year.
  \item AI ``factories'' (data $\to$ models) have huge appetite for compute and electricity.
  \item High-end AI chips upgrade yearly (vs.\ 4--5 years for CPUs) to train bigger models and serve more users.
  \item A major course theme: reduce energy consumption for AI computation.
\end{itemize}

\section*{5.\ Risks, Reasoning, and Device AI}

\subsection*{Risks in using GenAI}
\begin{itemize}
  \item Output may be untrue or incorrect (bad data/models).
  \item Output may be unsafe (imperfect safeguards).
  \item Systems may be insecure (attacks).
  \item Mitigating these risks leads to many interesting computing problems.
\end{itemize}

\subsection*{System optimizations and reasoning}
\begin{itemize}
  \item Use distributed computing and overlap communication with computation.
  \item Use reasoning (e.g., chain-of-thought, tool use) to let smaller models approach large-model accuracy.
\end{itemize}

\subsection*{Human vs.\ AI thinking: fast and slow}
\begin{itemize}
  \item Human: System 1 (fast, intuitive) vs System 2 (slow, deliberate) -- Kahneman.
  \item AI analogy:
  \begin{itemize}
    \item Earlier models (CNNs) $\Rightarrow$ fast, intuitive pattern recognition on sensor data.
    \item Recent GenAI/LLMs $\Rightarrow$ slower, more deliberate reasoning (math, code, planning).
  \end{itemize}
\end{itemize}

\subsection*{Device AI and design evolution}
\begin{itemize}
  \item Goal: small, energy-efficient, customizable AI on edge devices (mobiles, wearables, AI bracelets).
  \item Use reasoning to let small models reach large-model accuracy.
  \item Use LLMs to help evolve designs automatically (guided evolution, not just random mutations).
\end{itemize}

\section*{6.\ AI Chip Strategies, Scenarios, and Solution Approaches}

\subsection*{Two important AI computing scenarios}
\begin{itemize}
  \item \textbf{(1) Heavy AI computation (data centers)}
  \begin{itemize}
    \item Training and serving large pretrained models (LLMs, multi-modal models like CLIP).
  \end{itemize}
  \item \textbf{(2) Efficient AI computation (edge)}
  \begin{itemize}
    \item Compressed or small models on memory/compute/bandwidth-constrained devices.
  \end{itemize}
\end{itemize}

\subsection*{Challenges and metrics}
\begin{itemize}
  \item Metrics:
  \begin{itemize}
    \item Throughput, latency, energy, memory footprint.
    \item Accuracy (supervised, zero-shot, few-shot).
    \item Memory bandwidth usage.
    \item Implementation cost, turnaround time.
    \item Ease of use (libraries, drop-in replacements).
    \item Security and privacy.
  \end{itemize}
  \item Trade-offs:
  \begin{itemize}
    \item E.g.\ larger local memory $\Rightarrow$ more data reuse, lower bandwidth/energy, but more area/cost.
  \end{itemize}
  \item Need co-design of models, algorithms, hardware, software; design space is huge.
\end{itemize}

\subsection*{Solution approaches (slides up to 36)}
\begin{itemize}
  \item \textbf{Must have (basic)}
  \begin{itemize}
    \item Data parallelism (run multiple samples in a batch simultaneously).
    \item Model pruning and fine-tuning (remove small/unimportant weights, then fine-tune).
    \item Quantization (e.g., FP32 $\to$ INT8 on existing models).
  \end{itemize}
  \item \textbf{Nice to have (differentiation)}
  \begin{itemize}
    \item Model parallelism (different model partitions operate concurrently on the same input).
    \item Function approximation: low-rank filters (depthwise separable conv), multiplication-free matmul via codebooks and hashing.
    \item Bit-level sparsity: signed-digit representations (e.g., $31 = 10000\overline{1}$ where $\overline{1}=-1$).
    \item Streamlined student models via knowledge distillation from larger teachers.
  \end{itemize}
  \item \textbf{Advanced techniques (newest capabilities)}
  \begin{itemize}
    \item Distributed collaborative learning: federated learning, splitting inference over devices/edge/cloud.
    \item Analog computing and computing-in-memory (CIM), low-energy A/D conversion.
  \end{itemize}
\end{itemize}

\section*{Lec 2}

\section*{3.\ CNNs, MLPs, and MM}

\subsection*{CNN for Computer Vision}

\begin{itemize}
  \item Key layers:
  \begin{itemize}
    \item CONV (convolution) layers for feature extraction.
    \item Pooling (e.g., max-pooling) reduces spatial size.
    \item FC (fully connected) layers for classification.
  \end{itemize}
  \item Inference diagram:
  \begin{itemize}
    \item Input $\to$ CONV/Pooling stack (feature extraction) $\to$ FC layers (classification).
  \end{itemize}
\end{itemize}

\subsection*{Multi-Layer Perceptron (MLP)}

\begin{itemize}
  \item MLP: one or more FC (linear + activation) layers.
  \item Used for expanding/shrinking feature dimensionality.
  \item ReLU activation:
  \begin{itemize}
    \item Nonlinear: $f(ax+y) \neq a f(x) + f(y)$.
    \item Example: $f(-2+1)=0$ but $f(-2)+f(1) = 0+1=1$.
    \item Other activations: Sigmoid, Tanh.
  \end{itemize}
\end{itemize}

\subsection*{CNN as Feature Extractor + Classifier}

\begin{itemize}
  \item Feature extraction: CONV layers produce feature maps.
  \item Classification: FC layers over flattened features.
\end{itemize}

\subsection*{Matrix Multiplications (MMs) in CNN}

\begin{itemize}
  \item CONV and FC layers can be expressed as matrix operations.
  \item CNN with multiple layers:
  \begin{itemize}
    \item Convolution layers $\to$ matrix-matrix multiplies.
    \item FC layer $\to$ matrix-vector or matrix-matrix multiply.
  \end{itemize}
\end{itemize}

\section*{4.\ Very Deep ResNets}

\begin{itemize}
  \item ResNet uses \emph{skip connections} (residual connections):
  \[
    y = F(x) + x
  \]
  \item Benefits:
  \begin{itemize}
    \item Mitigates vanishing gradients.
    \item Enables very deep networks to train effectively.
  \end{itemize}
  \item Residual blocks form deep, stable CNNs (ResNet-50, etc.).
\end{itemize}

\section*{5.\ Filters, Convolution, and MM}

\subsection*{Using Filters to Detect Patterns}

\begin{itemize}
  \item Input $x = (x_1,\dots,x_5)^T$, filter $f=(w_1,\dots,w_5)^T$.
  \item Dot product $x^T f = \sum_i x_i w_i$ measures pattern match.
  \item Bias term as $x_1=1$ (or extra dimension).
  \item Large $x^T f$ $\Rightarrow$ strong match to filter.
  \item Cosine similarity: similarity via angle between vectors.
\end{itemize}

\subsection*{1D Convolution: Single Filter $\Rightarrow$ MVM}

\begin{itemize}
  \item 1D filter $f=(w_1,w_2,w_3)^T$ slides over input
  $x = (x_1,\dots,x_9)^T$.
  \item Stride = 3, positions:
  \[
    (x_1,x_2,x_3), (x_4,x_5,x_6), (x_7,x_8,x_9)
  \]
  \item Each position $\Rightarrow$ dot product $\Rightarrow$ feature map $r$.
  \item Equivalent to \emph{matrix-vector multiplication (MVM)}: input
  windows stacked as rows of a matrix.
\end{itemize}

\subsection*{1D Convolution: Multiple Filters $\Rightarrow$ MMM}

\begin{itemize}
  \item Two filters $f_1, f_2$ with weight vectors
  $(w_{11},w_{21},w_{31})^T$, $(w_{12},w_{22},w_{32})^T$.
  \item Stack weight vectors $\Rightarrow$ weight matrix.
  \item Input windows stacked $\Rightarrow$ input matrix.
  \item Computation = \emph{matrix-matrix multiplication (MMM)}:
  \[
    \text{Result} = \text{Input Matrix} \times \text{Weight Matrix}
  \]
  \item Each filter produces its own feature map; result is a matrix.
\end{itemize}

\subsection*{2D and 3D Convolution}

\begin{itemize}
  \item 2D convolution:
  \begin{itemize}
    \item 2D filters slide over 2D image.
    \item Dot products at each position produce 2D feature maps.
    \item Multiple filters $\Rightarrow$ multiple feature maps $\Rightarrow$ 3D tensor.
  \end{itemize}
  \item 2D with depth (e.g., RGB or previous-layer channels):
  \begin{itemize}
    \item 3D input $h \times w \times d$.
    \item Each filter: 3D kernel $3\times3\times d$ sliding spatially.
  \end{itemize}
  \item 3D convolution (e.g., CT/MRI):
  \begin{itemize}
    \item 4D filters sliding over 4D tensors (3D object + depth).
    \item Batch dimension $\Rightarrow$ 5D filters over 5D input.
  \end{itemize}
\end{itemize}

\section*{6.\ im2col and MMM for CONV}

\subsection*{Linearizing Tensors: im2col}

\begin{itemize}
  \item Convolution can be converted to MMM via \emph{im2col}:
  \begin{itemize}
    \item Extract each receptive field (patch) into a column or row.
    \item Flatten filters and inputs consistently (e.g., row-major).
  \end{itemize}
  \item CONV layer becomes:
  \[
    \text{Weight Matrix} \times \text{Data Matrix} \to \text{Feature Maps}
  \]
  \item Enables reuse of highly optimized MMM kernels.
\end{itemize}

\subsection*{CONV as MMM}

\begin{itemize}
  \item Each filter vector is applied to all data vectors.
  \item Each data vector is applied to all filters.
  \item MMM has:
  \begin{itemize}
    \item Regular 3-nested loop structure.
    \item High opportunity for data reuse.
    \item Amenable to systolic arrays and accelerators.
  \end{itemize}
\end{itemize}

\includegraphics[scale=0.4]{1.png}

\subsection*{Linear Layers and MVM}

\begin{itemize}
  \item FC layer = linear layer + activation.
  \item Single input vector $\Rightarrow$ MVM.
  \item Batch of inputs $\Rightarrow$ MMM.
  \item MVM has less data reuse than MMM $\Rightarrow$ need high memory bandwidth.
\end{itemize}

\section*{7.\ Acceleration Opportunities for MMM}

\begin{itemize}
  \item MMM is compute-bound and highly regular.
  \item Good target for:
  \begin{itemize}
    \item Systolic arrays.
    \item SIMD and tensor cores.
  \end{itemize}
  \item Challenges:
  \begin{itemize}
    \item Exploit sparsity in weights/activations.
    \item Minimize data movement and im2col overhead.
    \item Reuse data in memory hierarchies (caches / on-chip SRAM).
  \end{itemize}
\end{itemize}

\section*{8.\ Transformers vs CNNs}

\subsection*{Motivation}

\begin{itemize}
  \item CNN: spatial data (images), fast, intuitive pattern recognition.
  \item Transformer: sequential data (language), more thoughtful inference.
\end{itemize}

\subsection*{RNNs Before Transformers}

\begin{itemize}
  \item RNNs used for sequence modeling (language, etc.).
  \item Problems:
  \begin{itemize}
    \item Slow for long sequences.
    \item Vanishing gradients in backprop (product of many gradients $\to$ underflow).
  \end{itemize}
\end{itemize}

\subsection*{Transformer Architecture Overview}

\begin{itemize}
  \item For language translation:
  \begin{itemize}
    \item Encoder stack (N layers).
    \item Decoder stack (N layers).
    \item Final linear layer to vocabulary probabilities.
  \end{itemize}
  \item Each layer:
  \begin{itemize}
    \item Self-attention (across elements).
    \item MLP/FFN (element-wise).
    \item Layer normalization.
  \end{itemize}
\end{itemize}

\subsection*{Self-Attention}

\begin{itemize}
  \item Input $V$ with $n$ tokens and dimension $d$ (e.g., $n=6$, $d=512$).
  \item Build queries $Q$, keys $K$, values $V$ (often same base matrix).
  \item For a query $q$:
  \begin{itemize}
    \item Compare $q$ with all keys $k_i$ (e.g., dot products).
    \item Softmax over similarities $\Rightarrow$ attention weights.
    \item Output: weighted sum of corresponding values $v_i$.
  \end{itemize}
  \item Cost: $O(n^2 d)$.
  \item Intuition: each token attends to other tokens based on relevance.
\end{itemize}

\subsection*{Self-Attention Example}

\begin{itemize}
  \item Example with $n=3$, $d=2$:
  \begin{itemize}
    \item $V_1 = (1,2)^T$, $V_2=(2,-1)^T$, $V_3=(3,2)^T$.
    \item Compute dot products, apply softmax for each token.
    \item Each output $V_i'$ is a weighted combination of $V_1,V_2,V_3$.
    \item Demonstrates that some tokens (e.g., $v_3$) may contribute more.
  \end{itemize}
\end{itemize}

\subsection*{Multi-Head Self-Attention}

\begin{itemize}
  \item Multiple attention heads:
  \begin{itemize}
    \item Each head has its own $Q,K,V$ projections.
    \item Captures different types of relationships.
  \end{itemize}
  \item Outputs from heads concatenated and linearly projected.
\end{itemize}

\subsection*{MMM and MVM Recap}

\begin{itemize}
  \item MVM ($A b = c$):
  \begin{itemize}
    \item Used in FC/linear layers, MLP, LSTM, etc.
  \end{itemize}
  \item MMM ($A B = C$):
  \begin{itemize}
    \item Used in CONV, Transformer attention heads, FC layers with batches (e.g., mini-batch SGD).
  \end{itemize}
\end{itemize}

\section*{9.\ SIMD (Single Instruction Multiple Data)}

\subsection*{SIMD Basics}

\begin{itemize}
  \item Same instruction operates on multiple data elements in parallel.
  \item Example flow:
  \begin{itemize}
    \item Load a 4-element vector into register R1.
    \item Multiply 4 elements in R2 with one SIMD multiply.
    \item Store result vector from R3 to RAM.
  \end{itemize}
  \item Reduces latency by factor equal to vector width (e.g., 4x).
\end{itemize}

\subsection*{MVM Using SIMD for 16x Speedup}

\begin{itemize}
  \item Example: 64$\times$64 matrix $A$, vector $b$, result $c$.
  \item Partition $A$ into 4 column tiles $A_1,\dots,A_4$, each of width 16 (8-bit).
  \item Partition $b$ into $b_1,\dots,b_4$.
  \item Compute:
  \[
    c = A_1 b_1 + A_2 b_2 + A_3 b_3 + A_4 b_4
  \]
  \item Using 128-bit registers (16$\times$8-bit):
  \begin{itemize}
    \item Each tile MVM can be done in 64 clocks (one per row).
    \item If we can:
    \begin{itemize}
      \item Keep tile $A_1$ on-chip.
      \item Load/store 16 elements per clock.
      \item Use adder tree for sums.
    \end{itemize}
    \item Then SIMD achieves 16x speedup for MVM.
  \end{itemize}
\end{itemize}

\subsection*{Example: Intel SSE2}

\begin{itemize}
  \item 128-bit SIMD registers.
  \item Support vectors of different bitwidths (8-,16-,32-bit, etc.).
  \item SIMD instructions perform add, multiply, etc.\ on all packed elements.
\end{itemize}


\subsection*{CPU and GPU Architectures}

\begin{itemize}
  \item Both have many SIMD cores and local memories.
  \item CPU: fewer, more powerful SIMD cores, larger caches.
  \item GPU: many SIMD cores (warps/SMs) with on-chip local memory (LM).
\end{itemize}

\section*{10.\ Systolic Arrays and TPU}

\subsection*{Why Systolic Arrays?}

\begin{itemize}
  \item Dataflow architecture:
  \begin{itemize}
    \item No instruction fetch/decoding inside array.
  \end{itemize}
  \item Regular grid layout of processing elements (PEs):
  \begin{itemize}
    \item Simple placement and routing.
  \end{itemize}
  \item Local, uniform inter-PE communication (mesh):
  \begin{itemize}
    \item Short wires, predictable timing.
  \end{itemize}
  \item Data reuse:
  \begin{itemize}
    \item Reduced memory bandwidth requirements.
  \end{itemize}
\end{itemize}

\subsection*{Google TPU}

\begin{itemize}
  \item TPU uses a systolic array matrix multiply unit.
  \item Systolic array:
  \begin{itemize}
    \item Inputs (data) stream into array.
    \item Partial sums accumulate inside and exit to accumulators/buffers.
    \item DRAM feeds array; array is specialized for MMM.
  \end{itemize}
\end{itemize}

\subsection*{Weight-Stationary Systolic Array}

\begin{itemize}
  \item Weights (filters) are stored and remain stationary in PEs.
  \item Data rows enter from bottom, skewed for synchronization.
  \item Each data row interacts with each filter row; dot products flow out to the right.
  \item Systolic cell:
  \[
    y_{\text{out}} = y_{\text{in}} + w \cdot x_{\text{in}}, \quad x_{\text{out}} = x_{\text{in}}
  \]
  \item Cell cycle time: time to perform MAC.
\end{itemize}

\subsection*{Design Steps for Weight-Stationary MMM Array}

\begin{itemize}
  \item Step 1: For MMM, need dot product for each pair $(f_i, d_j)$ of filter row and data column.
  \item Step 2: Push data vectors up through array; each meets all pre-stored filter rows.
  \item Step 3: Skew inputs/outputs for synchronization (mechanical step).
  \item Variants: weight-stationary, data-stationary, result-stationary (trade-offs).
\end{itemize}

\subsection*{Operation vs SIMD}

\begin{itemize}
  \item In systolic array:
  \begin{itemize}
    \item Inputs propagate spatially through array.
    \item Partial sums accumulate along paths.
  \end{itemize}
  \item Conventional SIMD:
  \begin{itemize}
    \item Collection of PEs without neighbor-to-neighbor communication.
    \item Data moved from memory into PEs, results written back.
  \end{itemize}
\end{itemize}

\subsection*{Cross-Layer Data Movement and Corner Turning}

\begin{itemize}
  \item Output of one layer is input of next.
  \item Often need \emph{corner turning} (matrix transposition) between layers.
  \item Cross-layer pipelining:
  \begin{itemize}
    \item Layers arranged as pipeline (Layer 1 $\to$ Layer 2 $\to$ Layer 3).
    \item Systolic arrays per layer with corner turning between them.
  \end{itemize}
\end{itemize}

\section*{Lec 3}

\section*{2.\ Memory Hierarchy and Memory Wall}

\subsection*{Local vs external memory}

\begin{itemize}
  \item On-chip SRAM (local memory):
  \begin{itemize}
    \item Fast, low power, expensive ($\sim$10–50x cost/bit vs DRAM).
  \end{itemize}
  \item Off-chip DRAM (external memory):
  \begin{itemize}
    \item Large capacity, high latency, lower bandwidth.
  \end{itemize}
  \item High-performance systems add HBM stacks to increase bandwidth and capacity.
\end{itemize}

\subsection*{Memory bottleneck (memory wall)}

\begin{itemize}
  \item Compute throughput has grown much faster than DRAM bandwidth/latency.
  \item Hard to:
  \begin{itemize}
    \item Increase chip I/O bandwidth for off-chip communication.
    \item Scale DRAM bandwidth/size to match application demands.
  \end{itemize}
  \item HPC/AI systems:
  \begin{itemize}
    \item Use SRAM caches/registers to reduce DRAM traffic via data reuse.
    \item Sometimes use NVRAM to store weights (but NVRAM still lags in performance).
  \end{itemize}
  \item Tiny MCUs / low-end systems:
  \begin{itemize}
    \item Limited compute anyway, so memory wall is less dominant.
  \end{itemize}
\end{itemize}

\subsection*{Mitigating memory wall}

\begin{enumerate}
  \item \textbf{Large local memory}:
  \begin{itemize}
    \item Keep more data in on-chip SRAM/caches to avoid repeated DRAM access.
  \end{itemize}
  \item \textbf{Clever scheduling}:
  \begin{itemize}
    \item Schedule computation to maximize reuse of data in local memory.
    \item Example: outer-product MMM, CAKE block scheduling (later).
  \end{itemize}
\end{enumerate}

\section*{3.\ Arithmetic Intensity (AI) $\alpha$}

\subsection*{Definition}

\begin{itemize}
  \item \textbf{Arithmetic intensity}:
  \[
    \alpha = \frac{\text{amount of computation}}{\text{amount of memory I/O}}.
  \]
  \item Here:
  \begin{itemize}
    \item \emph{Computation} = number of multiplications (ignoring adds).
    \item \emph{Memory I/O} = number of words moved to/from external memory.
  \end{itemize}
\end{itemize}

\subsection*{Examples (assuming large local memory)}

\begin{itemize}
  \item Multiply two $n \times n$ matrices:
  \[
    \text{multiplies} \sim n^3,\quad \text{I/O} \sim 3n^2 \quad\Rightarrow\quad
    \alpha \approx \frac{n^3}{3n^2} = O(n).
  \]
  \item Multiply $n\times n$ matrix by $n\times1$ vector (MVM):
  \[
    \text{multiplies} \sim n^2,\quad \text{I/O} \sim n^2 + 2n \quad\Rightarrow\quad
    \alpha = O(1).
  \]
  \item \textbf{Key point}: MMM has much higher arithmetic intensity than MVM.
\end{itemize}

\subsection*{Relationship: $\alpha = \text{CT} / \text{MB}$}

\begin{itemize}
  \item Terminology:
  \begin{itemize}
    \item CT = computation throughput (ops/s).
    \item MB = memory bandwidth (words/s or bytes/s).
  \end{itemize}
  \item We can rewrite:
  \[
    \alpha = \frac{\text{computation (ops)}}{\text{memory I/O (words)}}
          = \frac{\text{computation (ops)}/\text{time}}{\text{memory I/O (words)}/\text{time}}
          = \frac{\text{CT}}{\text{MB}}.
  \]
  \item So:
  \[
    \text{CT} = \alpha \cdot \text{MB}.
  \]
  \item \textbf{Interpretation}:
  \begin{itemize}
    \item For fixed hardware (fixed MB), we can increase CT by:
    \begin{itemize}
      \item Buying more memory bandwidth (more expensive hardware), or
      \item Increasing $\alpha$ via better scheduling / more data reuse.
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection*{Data reuse across memory hierarchy}

\begin{itemize}
  \item Increase $\alpha$ at each level:
  \begin{itemize}
    \item L2 reuse: reduce traffic to L3.
    \item L3 reuse: reduce traffic to DRAM.
  \end{itemize}
  \item Each level reuses data to reduce bandwidth demand on the next level.
\end{itemize}

\section*{4.\ Inner vs Outer Product MMM}

\subsection*{Linear algebra view}

\begin{itemize}
  \item MMM: $C = A B$, where $A \in \mathbb{R}^{M\times K}$, $B \in \mathbb{R}^{K\times N}$.
  \item \textbf{Inner-product form}:
  \[
    C_{ij} = \sum_{k=1}^K A_{ik} B_{kj} \quad\text{(dot product of row $i$ of $A$ and column $j$ of $B$)}.
  \]
  \item \textbf{Outer-product form}:
  \[
    C = \sum_{k=1}^K A_{:k} B_{k:},
  \]
  where $A_{:k}$ is column $k$ of $A$ and $B_{k:}$ is row $k$ of $B$; each term is a rank-1 outer product.
\end{itemize}

\subsection*{Loop-nest view}

\begin{itemize}
  \item \textbf{Inner-product MMM} (K innermost):
\begin{verbatim}
for i = 1..M
  for j = 1..N
    C[i,j] = 0
    for k = 1..K
      C[i,j] += A[i,k] * B[k,j]
\end{verbatim}
  \item \textbf{Outer-product MMM} (K outermost):
\begin{verbatim}
for k = 1..K
  for i = 1..M
    for j = 1..N
      C[i,j] += A[i,k] * B[k,j]
\end{verbatim}
  \item Both compute the same $C$, but access memory in different patterns.
\end{itemize}

\subsection*{Why outer-product MMM is better for memory}

\begin{itemize}
  \item Outer-product schedule:
  \begin{itemize}
    \item Accumulates intermediate values of $C$ \emph{in place} in local memory.
    \item Avoids repeated reading/writing of partial sums of $C$ to external memory.
    \item Each element of $C$ gets updated $K$ times while staying local $\Rightarrow$ reused $K$ times.
  \end{itemize}
  \item Reducing memory accesses for $C$ is critical because:
  \begin{itemize}
    \item $C$ has $M N$ elements, often large.
  \end{itemize}
\end{itemize}

\subsection*{Arithmetic intensity of tiled MMM}

\begin{itemize}
  \item Consider MMM between two tiles:
  \[
    \text{tile of } A: m\times k,\quad \text{tile of } B: k\times n,\quad \text{tile of } C: m\times n.
  \]
  \item \textbf{Inner-product tile}:
  \begin{itemize}
    \item For each inner product, comp = $k$, I/O $\approx 2k+1$ (load row/col + read/write C).
    \item $\alpha = O(1)$ (low intensity).
  \end{itemize}
  \item \textbf{Outer-product tile}:
  \begin{itemize}
    \item For each outer product, comp = $m n$ (one full tile), I/O $\approx m + n$ (load one column of $A$ and one row of $B$, $C$ stays local).
    \item If $m = r n$, then:
    \[
      \alpha = \frac{m n}{m + n} = \frac{r n^2}{r n + n} = \frac{r}{r+1} n = O(n).
    \]
    \item Larger tile size $n$ $\Rightarrow$ larger $\alpha$, until local memory fills up.
  \end{itemize}
\end{itemize}

\subsection*{Outer-product streaming strategy}

\begin{itemize}
  \item High-performance MMM libraries (e.g., GOTO-style) stream:
  \begin{itemize}
    \item Keep tile of $C$ locally in cores.
    \item Stream columns of $A$ and rows of $B$ through cores.
    \item Accumulate partial products into local $C$.
  \end{itemize}
  \item Memory time for streaming out rows of $C$ can be hidden under computation.
\end{itemize}

\section*{5.\ Roofline Model}

\subsection*{Concept}

\begin{itemize}
  \item Plots peak achievable CT (y-axis) vs arithmetic intensity $\alpha$ (x-axis).
  \item Two regimes:
  \begin{itemize}
    \item \textbf{Memory-bound}: CT limited by MB, line with slope MB: CT = MB·$\alpha$.
    \item \textbf{Compute-bound}: CT limited by peak FLOP rate, flat roof at CT$_\text{peak}$.
  \end{itemize}
\end{itemize}

\subsection*{Ridge point}

\begin{itemize}
  \item Intersection of slanted (memory-bound) line and flat (compute-bound) line.
  \item x-coordinate:
  \[
    \alpha^* = \frac{\text{CT}_\text{peak}}{\text{MB}}.
  \]
  \item Interpretation:
  \begin{itemize}
    \item Minimum $\alpha$ needed for kernel to hit peak compute throughput.
    \item If $\alpha < \alpha^*$, kernel is memory-bound.
    \item If $\alpha \ge \alpha^*$, kernel can be compute-bound.
  \end{itemize}

\includegraphics[scale=0.4]{2.png}
\end{itemize}

\subsection*{Implications}

\begin{itemize}
  \item For a given kernel and hardware:
  \begin{itemize}
    \item Scheduling objective: increase $\alpha$ (via data reuse, tiling) up to $\alpha^*$.
  \end{itemize}
  \item For hardware design:
  \begin{itemize}
    \item Prefer ridge point closer to left (small $\alpha^*$) so more kernels can reach CT$_\text{peak}$ without requiring extreme arithmetic intensity.
    \item But this usually means higher MB (more expensive memory subsystem).
  \end{itemize}
\end{itemize}

\section*{6.\ Computation Blocks, CAKE, and Shaping}

\subsection*{Computation space for MMM}

\begin{itemize}
  \item Triple loop
  \item 3D computation space over indices $(i,j,k)$.
  \item We partition into \textbf{blocks} (tiles) and schedule blocks.
\end{itemize}

\subsection*{Inter-block schedule: K-first}

\begin{itemize}
  \item An inter-block schedule = ordered sequence of blocks.
  \item K-first schedule: sweep along $k$ dimension first (outer-product-like).
  \item CAKE uses such schedules with carefully chosen block shapes.
\end{itemize}

\subsection*{CAKE analogy}

\begin{itemize}
  \item Partitioning and scheduling MMM is like cutting and serving a cake:
  \begin{itemize}
    \item For each chunk, maximize \textbf{volume / surface} ratio.
    \item Volume $\sim$ computation; surface $\sim$ boundary data needing I/O.
  \end{itemize}
\end{itemize}

\subsection*{CAKE’s computation block shaping}

\includegraphics[scale=0.5]{3.png}

\begin{itemize}
  \item Goal: increase CT by using more cores \emph{without} increasing external memory BW.
  \item Strategy:
  \begin{itemize}
    \item Shape computation blocks so that data reuse increases with #cores.

    \includegraphics[scale=0.5]{4.png}
    
    \item Example: blocks (a), (b), (c) use $km$, $2km$, $pkm$ cores respectively:
    \begin{itemize}
      \item Each uses the same external BW.
      \item But each has higher arithmetic intensity (more reuse).
    \end{itemize}
    \item An element of $A$ reused $n$ times in (a) vs $p n$ times in (c).
  \end{itemize}
  \item By AI = CT / MB, increasing AI $\Rightarrow$ increase CT at fixed MB.
\end{itemize}



\section*{Lec 4}

\section*{1.\ Gradient Descent and Model Training}

\subsection*{Goal: minimize loss over parameters}

\begin{itemize}
  \item We train a model by minimizing a real-valued loss function \(f(w)\) over parameters \(w\).
  \item Loss depends on:
  \begin{itemize}
    \item Current weights/biases \(w\).
    \item Current training example(s) (inputs + labels).
  \end{itemize}
  \item Parameter space is high-dimensional (millions to trillions of dimensions); diagrams show 2D case for intuition.
\end{itemize}

\subsection*{Basic gradient descent update}

\begin{itemize}
  \item At iterate \(w_i\), compute gradient \(\nabla_i = \nabla f(w_i)\).
  \item Update rule:
  \[
    w_{i+1} = w_i - \eta \nabla_i,
  \]
  where \(\eta\) is the learning rate (e.g.\ 0.01, 0.001).
  \item We use \(-\eta\nabla_i\) (not \(+\eta\nabla_i\)) because:
  \begin{itemize}
    \item Gradient points in direction of steepest \emph{increase}.
    \item We want to \emph{decrease} the loss, so move in negative gradient direction.
  \end{itemize}
  \item As training converges to a local minimum, \(\nabla_i\) becomes small.
  \item Learning rate schedule:
  \begin{itemize}
    \item Often start with a fixed \(\eta\), then reduce it in later iterations/epochs.
  \end{itemize}
\end{itemize}

\section*{2.\ Forward and Backward Passes}

\subsection*{One gradient descent iteration}

\begin{itemize}
  \item Before training: initialize all model weights (typically randomly).
  \item Each iteration (training step):
  \begin{enumerate}
    \item \textbf{Forward pass:}
    \begin{itemize}
      \item Compute layer activations and final loss for current batch of examples.
      \item Store intermediate activations for use in backprop.
    \end{itemize}
    \item \textbf{Backward pass (backprop):}
    \begin{itemize}
      \item Compute gradients of loss wrt weights and biases using chain rule.
      \item Update weights using gradient descent (or variant).
    \end{itemize}
  \end{enumerate}
  \item Training typically uses multiple \emph{epochs}:
  \begin{itemize}
    \item One epoch = full pass over entire training set.
  \end{itemize}
\end{itemize}

\subsection*{Batch vs SGD vs Mini-batch}

\paragraph{Batch Gradient Descent}
\begin{itemize}
  \item Each iteration uses the \emph{entire} dataset of \(n\) training examples.
  \item Each iteration is an epoch; gradients are exact for the whole dataset.
  \item Pros:
  \begin{itemize}
    \item Simple; converges to a local minimum under standard conditions.
    \item High data parallelism (split dataset across machines).
  \end{itemize}
  \item Cons:
  \begin{itemize}
    \item Each update requires processing all data \(\Rightarrow\) slow updates.
    \item Inefficient use of data when dataset is large and redundant.
  \end{itemize}
\end{itemize}

\paragraph{Stochastic Gradient Descent (SGD)}
\begin{itemize}
  \item Each iteration uses a \emph{single} randomly chosen training example (batch size \(B=1\)).
  \item Pros:
  \begin{itemize}
    \item Very frequent weight updates (one per example).
    \item Can process streaming data on the fly.
    \item Noise from individual examples can help escape local minima.
  \end{itemize}
  \item Cons:
  \begin{itemize}
    \item No batch-level data parallelism.
    \item Convergence can be noisy/slow.
    \item Learning rate typically must decay (e.g.\ exponentially) with iterations.
  \end{itemize}
\end{itemize}

\paragraph{Mini-batch Gradient Descent}
\begin{itemize}
  \item Each iteration uses a minibatch of \(B\) examples (e.g.\ \(B = 32, 64,\dots\)).
  \item Gradient for the update is the average gradient over the minibatch.
  \item Pros:
  \begin{itemize}
    \item \(n/B\) weight update rounds per epoch (vs 1 for full batch).
    \item High data parallelism for large \(B\).
    \item Gradient estimates less noisy than SGD, so \(\eta\) can be larger.
    \item Can stop training early if dataset is highly redundant.
  \end{itemize}
  \item Cons:
  \begin{itemize}
    \item If minibatches differ in distribution, generalization may suffer.
  \end{itemize}
\end{itemize}

\section*{3.\ Backpropagation and Chain Rule}

\subsection*{Chain rule for composite functions}

\begin{itemize}
  \item Loss of multi-layer net is a composition of layer functions.
  \item Example:
  \[
    f(W_1, W_2, W_3) = f_3\big(f_2(f_1(W_1), W_2), W_3\big).
  \]
  \item For simple 1D case \(f(w) = g(h(w))\):
  \[
    f'(w) = g'(h(w)) \cdot h'(w).
  \]
  \item To compute \(f'(w_i)\), we:
  \begin{itemize}
    \item Compute \(h'(w_i)\).
    \item Compute \(g'(y)\) at \(y = h(w_i)\).
    \item Multiply these two quantities.
  \end{itemize}
  \item Forward pass computes intermediate values like \(h(w_0)\), \(g(h(w_0))\); we store these activations for backward pass.
  \item Backprop applies chain rule systematically across layers.
  \item For non-differentiable functions (e.g.\ quantization), can use Straight-Through Estimator (STE) as an approximation.
\end{itemize}

\subsection*{Two operators at each hidden node}

\begin{itemize}
  \item A hidden unit is typically:
  \begin{enumerate}
    \item \textbf{Dot product (linear)}:
    \[
      h(w) = x_1 w_1 + x_2 w_2 + b,\quad \frac{dh}{dw_1} = x_1.
    \]
    \item \textbf{Activation (nonlinear)}:
    \begin{itemize}
      \item ReLU, Sigmoid, GELU, etc.
    \end{itemize}
  \end{enumerate}
  \item If we \emph{omit} nonlinear activations:
  \begin{itemize}
    \item Composition of linear layers is still linear.
    \item Extra layers do not add representational power (just one big linear map).
  \end{itemize}
\end{itemize}

\subsection*{ReLU derivative}

\begin{itemize}
  \item ReLU:
  \[
    \mathrm{ReLU}(z) = \max\{0,z\}.
  \]
  \item Derivative:
  \[
    \mathrm{ReLU}'(z) =
    \begin{cases}
      0, & z < 0,\\
      1, & z > 0,\\
      \text{undefined at }0\ (\text{often set to }0\text{ or }1).
    \end{cases}
  \]
  \item Easy to implement and cheap to compute.
\end{itemize}

\subsection*{Sigmoid derivative}

\begin{itemize}
  \item Sigmoid:
  \[
    \sigma(x) = \frac{1}{1 + e^{-x}}.
  \]
  \item Key property:
  \[
    \sigma'(x) = \sigma(x)\big(1 - \sigma(x)\big).
  \]
  \item In backprop, we can reuse the forward-pass value \(\sigma(x)\) to compute derivative cheaply.
\end{itemize}

\subsection*{Backprop over a network}

\begin{itemize}
  \item Each node is either a dot-product node or activation node.
  \item For each node, chain rule combines:
  \begin{itemize}
    \item Derivative of loss wrt node output.
    \item Local derivative (dot product or activation).
  \end{itemize}
  \item This gives gradients wrt:
  \begin{itemize}
    \item Weights of the node (used for updates).
    \item Inputs/activations feeding into the node (for further backprop).
  \end{itemize}
\end{itemize}

\includegraphics[width=0.9\linewidth]{backprop.png}

\includegraphics[scale=0.3]{6.png}

\section*{4.\ Gradients for a Layer}

\subsection*{Forward/backward memory reuse}

\begin{itemize}
  \item Forward pass:
  \begin{itemize}
    \item Compute and store activations for each layer.
  \end{itemize}
  \item Backward pass:
  \begin{itemize}
    \item Use stored activations to compute gradients.
    \item After computing gradients for a layer and all deeper layers, we can discard its activations to free memory.
  \end{itemize}
  \item Memory optimization: re-materialization / checkpointing trades compute for memory.
\end{itemize}

\section*{5.\ CNN Layers as Matrix Multiplications}

\subsection*{Matrix form of forward/backward}

\begin{itemize}
  \item In many frameworks, CNN / linear layers can be expressed as:
  \[
    O = A W
  \]
  where:
  \begin{itemize}
    \item \(A\): input activations (possibly after im2col).
    \item \(W\): weight matrix.
    \item \(O\): output activations.
  \end{itemize}
  \item Forward pass: \(A \to O\).
  \item Backward pass:
  \begin{itemize}
    \item Given \(\nabla_O\), compute:
    \[
      \nabla_A = \nabla_O W^\top,\quad
      \nabla_W = A^\top \nabla_O.
    \]
    \item Note the use of transposed matrices \(W^\top\) and \(A^\top\).
  \end{itemize}
  \item This corresponds to:
  \[
    \frac{\partial \text{Loss}}{\partial A}
    = \frac{\partial \text{Loss}}{\partial O}
      \cdot \frac{\partial O}{\partial A}
    = \nabla_O \cdot W.
  \]
\end{itemize}

\section*{6.\ Batch Normalization (BatchNorm)}

\subsection*{BatchNorm overview}

\begin{itemize}
  \item BatchNorm is a training technique that:
  \begin{itemize}
    \item Normalizes layer inputs across a minibatch to have mean \(\approx 0\) and variance \(\approx 1\).
    \item Usually applied per feature/channel.
  \end{itemize}
  \item For each feature over minibatch:
  \begin{itemize}
    \item Compute batch mean \(\mu_B\).
    \item Compute batch variance \(\sigma_B^2\).
    \item Normalize:
    \[
      \hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}.
    \]
  \end{itemize}
  \item Benefits:
  \begin{itemize}
    \item Stabilizes distributions seen by deeper layers.
    \item Can speed up convergence and allow higher learning rates.
  \end{itemize}
\end{itemize}

\subsection*{Learnable parameters \(\gamma\) and \(\beta\)}

\begin{itemize}
  \item After normalization, BatchNorm applies:
  \[
    y = \gamma \hat{x} + \beta,
  \]
  where \(\gamma,\beta\) are learned per-feature parameters.
  \item This “scale and shift”:
  \begin{itemize}
    \item Restores representational flexibility (network not forced to unit-variance, zero-mean features).
    \item Allows network to learn appropriate distributions for activations.
  \end{itemize}
\end{itemize}

\section*{7.\ High-Performance Computing for AI Factories}

Dollar and energy cost for training frontier LLMs is extremely high.


\subsection*{CPU vs GPU computing}

\begin{itemize}
  \item \textbf{CPUs}:
  \begin{itemize}
    \item Designed to run a few threads with low latency.
    \item Emphasize strong single-thread performance, complex control, large caches.
    \item Few cores, complex logic.
  \end{itemize}
  \item \textbf{GPUs}:
  \begin{itemize}
    \item Designed for high throughput via thousands of concurrent threads.
    \item Specialize in data-parallel workloads (e.g.\ matrix multiplies) with simpler control.
    \item Many cores, simple logic.
  \end{itemize}
\end{itemize}

\subsection*{NVIDIA GPUs and platforms}

\begin{itemize}
  \item H100 (successor to A100) has been workhorse for training frontier LLMs.
  \item Newer Blackwell-generation GPUs: B100, B200.
  \item GB200/GB300 “Grace Blackwell” superchips:
  \begin{itemize}
    \item Pair one Grace CPU with two Blackwell B200 GPUs via NVLink-C2C.
  \end{itemize}
  \item AI clusters combine many such nodes with high-bandwidth networking.
\end{itemize}


\section*{8.\ Parallelism for Large-Scale Training}

\subsection*{Three key types of parallelism}

Data parallelism, Tensor (model) parallelism, Pipeline parallelism.

\subsection*{GB200/GB300 NVL systems}

\begin{itemize}
  \item GB200/GB300 NVL72:
  \begin{itemize}
    \item Rack-scale, liquid-cooled AI system.
    \item 72 Blackwell GPUs (Grace Blackwell superchips) in a single NVLink domain with unified memory address space.
  \end{itemize}
  \item NVL288:
  \begin{itemize}
    \item Multi-rack expansion of NVL72 to 288 GPUs in one NVLink fabric domain.
  \end{itemize}
  \item NVL576:
  \begin{itemize}
    \item Larger configuration with 576 GPUs in a single NVLink domain.
  \end{itemize}
\end{itemize}

\subsection*{NVIDIA reference architectures}

\begin{itemize}
  \item NVIDIA publishes structured reference architectures for:
  \begin{itemize}
    \item Hopper GPUs, Blackwell GPUs, Grace CPUs, Spectrum-X networking, BlueField DPUs, etc.
  \end{itemize}
  \item Reference configurations use \textbf{C-G-N-B} nomenclature:
  \begin{itemize}
    \item C: number of CPU sockets.
    \item G: number of GPUs.
    \item N: number of NICs.
    \item B: average east-west network bandwidth per GPU (in GbE).
  \end{itemize}
  \item Helps standardize and compare compute, network, and bandwidth across systems.
\end{itemize}

\section*{Lec 5}

\section*{2.\ Distributed Computing for AI: Why Split?}

\begin{itemize}
  \item We split a computing task across multiple nodes for:
  \begin{itemize}
    \item \textbf{Scalability}:
    \begin{itemize}
      \item Large models (frontier LLMs).
      \item Many small models (e.g., wearables, IoT).
    \end{itemize}
    \item \textbf{Locality \& privacy}:
    \begin{itemize}
      \item “Bring code to data” instead of data to code.
      \item Compute where data originates (phones, hospitals, factories).
    \end{itemize}
    \item \textbf{Low-latency computing} at the edge (fast inference).
  \end{itemize}
\end{itemize}

\section*{3.\ Data Splitting (1): Distributed Mini-Batch}

\subsection*{Classic mini-batch}

\begin{itemize}
  \item Single worker with minibatch \(B\):
  \begin{itemize}
    \item For each \(x_i \in B\), do forward \& backward pass:
    \[
      \nabla W_i = \nabla_W \text{Loss}(x_i).
    \]
    \item Weight gradient for batch:
    \[
      \nabla W = \mathrm{Avg}_{x_i \in B} \nabla W_i.
    \]
  \end{itemize}
\end{itemize}

\subsection*{Distributed mini-batch}

\begin{itemize}
  \item Split one global minibatch \(B\) into multiple \emph{sub-batches} \(B_1, \dots, B_n\), each on a separate worker/GPU.
  \item Each worker:
  \begin{itemize}
    \item Processes local minibatch \(B_i\), computes \(\nabla W_i\).
  \end{itemize}
  \item Central server (or all-reduce):
  \begin{itemize}
    \item Aggregates gradients:
    \[
      \nabla W = \mathrm{Avg}_i \nabla W_i.
    \]
    \item Updates global model and syncs back to workers.
  \end{itemize}
  \item This is the classic \emph{data-parallel distributed training}.
\end{itemize}

\section*{4.\ Data Splitting (2): Federated Learning \& FedAvg}

\subsection*{Federated Learning: concept}

\begin{itemize}
  \item Global model trained across many clients (phones, IoT, hospitals), each with local private data.
  \item Key principle:
  \begin{itemize}
    \item Data stays on device.
    \item Only model updates (gradients / weights) are communicated.
  \end{itemize}
  \item Server aggregates updates into a global model.
\end{itemize}

\subsection*{FedAvg: basic idea}

\begin{itemize}
  \item Each client \(k\) has local dataset \(\mathcal{P}_k\) with size \(n_k\).
  \item Typical FedAvg round:
  \begin{enumerate}
    \item Server sends current global model \(w_t\) to a subset of clients.
    \item Each participating client:
    \begin{itemize}
      \item Runs several local SGD steps (local training epochs) on its data.
      \item Produces updated local weights \(w_t^{(k)}\).
    \end{itemize}
    \item Server aggregates:
    \[
      w_{t+1} = \sum_k \frac{n_k}{\sum_j n_j} \, w_t^{(k)} \quad\text{(weighted average)}.
    \]
  \end{enumerate}
  \item This solves a global optimization problem over sums of client losses.
\end{itemize}

\subsection*{Advantages of FL}

\begin{itemize}
  \item Raw data is naturally distributed and decentralized:
  \begin{itemize}
    \item Phones, smart speakers, IoT devices, hospital records, factories.
  \end{itemize}
  \item FL advantages:
  \begin{itemize}
    \item Reduced central storage need.
    \item Enhanced privacy:
    \begin{itemize}
      \item Data never leaves the client.
      \item Ownership and local objectives can remain hidden.
    \end{itemize}
    \item Can handle non-IID local data (see below).
    \item Collaborative learning among clients with similar distributions.
  \end{itemize}
  \item Slogan: \textbf{“Bring the code to the data, instead of the data to the code.”}
\end{itemize}

\section*{5.\ FL on Non-IID Data \& Solutions}

\subsection*{Non-IID data challenges}

\begin{itemize}
  \item Clients may have \emph{highly heterogeneous} local data:
  \begin{itemize}
    \item Example: partition CIFAR-100 via Dirichlet with concentration 0.1 (very non-IID).
  \end{itemize}
  \item Problems:
  \begin{itemize}
    \item Local feature spaces can diverge across clients.
    \item Same class may map to different regions at different clients.
    \item Global model may not converge or may be unfair to some clients.
  \end{itemize}
\end{itemize}

\subsection*{Solution 1: FedProx}

\begin{itemize}
  \item Add a proximal term to each client’s objective to keep local model near the global model:
  \[
    \min_{w_k} F_k(w_k) + \frac{\mu}{2} \|w_k - w_t\|^2,
  \]
  where \(w_t\) is the global model at round \(t\) and \(\mu\) is a penalty coefficient.
  \item Intuition:
  \begin{itemize}
    \item Prevents local models from drifting too far from global model.
    \item Controls degree of personalization vs global consistency.
  \end{itemize}
  \item Tradeoff:
  \begin{itemize}
    \item Smaller \(\mu\) $\Rightarrow$ more emphasis on local non-IID data (more drift).
    \item Tuning \(\mu\) can require grid search.
  \end{itemize}
\end{itemize}

\subsection*{Solution 2: SphereFed (Hyperspherical FL)}

\begin{itemize}
  \item Typical classification DNN:
  \[
    \text{Input} \xrightarrow{\text{feature extractor}} z \xrightarrow{\text{classifier}\ W} \text{class}.
  \]
  \item SphereFed idea (three phases):
  \begin{enumerate}
    \item \textbf{Before FL}:
    \begin{itemize}
      \item Configure a global classifier \(W\) on a hypersphere (e.g., normalized class weights).
    \end{itemize}
    \item \textbf{During FL}:
    \begin{itemize}
      \item Freeze global classifier \(W\).
      \item Each device trains only its feature extractor against fixed \(W\).
    \end{itemize}
    \item \textbf{After FL}:
    \begin{itemize}
      \item Freeze local feature extractors.
      \item Each device sends small rank-1 updates to server; server calibrates global classifier \(W\).
    \end{itemize}
  \end{enumerate}
  \item Goal:
  \begin{itemize}
    \item Align feature spaces across heterogeneous clients by sharing a common classifier geometry.
  \end{itemize}
\end{itemize}

\section*{6.\ Model Splitting Methods}

\subsection*{(3) BranchyNet: dynamic splitting with early exits}

\begin{itemize}
  \item Add multiple \emph{exit branches} at intermediate layers.
  \item Easy inputs:
  \begin{itemize}
    \item Can be correctly classified using lower-level features.
    \item Exit early $\Rightarrow$ skip expensive deeper layers.
  \end{itemize}
  \item Hard inputs:
  \begin{itemize}
    \item Require higher-level features; continue deeper.
  \end{itemize}
  \item Low-entropy softmax at early exit $\Rightarrow$ confident prediction.
  \item Motivation: reduce inference latency and compute by not always running full depth.
\end{itemize}

\subsection*{(4) Distributed DNN: static split across cloud/edge/device}

\begin{itemize}
  \item Map sections of a DNN onto: Cloud, Edge, Device (end node).
  
  \item Each layer of computing hierarchy processes specific DNN sections.
  \item May have local exits at device/edge for low-latency approximate answers.
\end{itemize}

\subsection*{(5) SplitNet: optimize cloud/edge split}

\begin{itemize}
  \item Goal: choose split layer that minimizes total inference latency:
  \[
    T_{\text{total}} = T_{\text{edge}} + T_{\text{comm}} + T_{\text{cloud}},
  \]
  subject to accuracy and memory constraints.
  \item Idea:
  \begin{itemize}
    \item Put some early layers on edge, later layers on cloud.
    \item Compress features that are transmitted to cloud (smaller tensors than raw inputs).
  \end{itemize}
\end{itemize}

\subsection*{(6) Micro-batch pipeline parallelism (GPipe)}

\begin{itemize}
  \item Naive model-parallelism:
  \begin{itemize}
    \item Partition layers across devices, but process full minibatch sequentially.
    \item Large “bubbles” where some devices idle.
  \end{itemize}
  \item GPipe:
  \begin{itemize}
    \item Split minibatch into many micro-batches.
    \item Pipe them through the stacked devices:
    \begin{itemize}
      \item Different devices process different micro-batches simultaneously.
    \end{itemize}
    \item Reduces bubble, increases utilization.
    \item Gradients applied synchronously after all micro-batches.
  \end{itemize}
\end{itemize}

\subsection*{(7) CDSpawning: continuous distillation}

\begin{itemize}
  \item Cloud hosts a large pretrained feature extractor (e.g., DINOv2).
  \item Edge devices host small classifiers.
  \item \textbf{Cloud-to-edge workflow}:
  \begin{itemize}
    \item High-capacity model extracts embeddings for user data.
    \item Embeddings (compact, abstract) cached on device.
    \item Lightweight on-device classifier trained locally on these features (few-shot, updated continuously).
  \end{itemize}
  \item Continuous spawning of edge models via distillation from the cloud teacher.
\end{itemize}

\subsection*{(8) Mixture of Experts (MOE)}

\begin{itemize}
  \item Replace single FFN in Transformer block with multiple FFNs (experts).
  \item Gating network:
  \begin{itemize}
    \item For each token, selects one or a few experts to run.
    \item Often a linear layer + softmax over experts.
  \end{itemize}
  \item Only selected experts are activated per token:
  \begin{itemize}
    \item Sparse use of parameters (sparse expert model).
    \item Can scale total parameters (capacity) without linear increase in compute per token.
  \end{itemize}
  \item Example: Switch Transformer, Mistral 8$\times$7B MOE.
\end{itemize}

\section*{7.\ MOE: Routing, Sharding, and Scaling}

\subsection*{Token routing}

\begin{itemize}
  \item Each token routed to expert with highest router probability (top-1, or top-2, etc.).
  \item Each expert has fixed capacity:
  \[
    \text{batch per expert} \approx \frac{\text{total tokens}}{\text{\#experts}} \times \text{capacity factor}.
  \]
  \item If tokens unevenly dispatched:
  \begin{itemize}
    \item Some experts overflow (discarded or padded).
    \item Larger capacity factor reduces overflow but increases compute/communication.
  \end{itemize}
\end{itemize}

\subsection*{Specialization of experts}

\begin{itemize}
  \item Different experts specialize in different patterns (e.g., syntactic vs semantic tokens, languages, etc.).
  \item Load balancing:
  \begin{itemize}
    \item Training and the gating mechanism adapt to distribute work.
    \item Auxiliary losses may encourage balanced usage.
  \end{itemize}
\end{itemize}

\subsection*{Model sharding}

\begin{itemize}
  \item Experts are distributed across nodes (a form of model parallelism).
  \item Gating mechanism must manage:
  \begin{itemize}
    \item Routing tokens to correct device.
    \item Communication overhead (all-to-all exchanges).
  \end{itemize}
  \item Static vs dynamic placement:
  \begin{itemize}
    \item Static: simpler, but may cause load imbalance.
    \item Dynamic: adapt to workload, but more complex and communication-intensive.
  \end{itemize}
  \item Need fault tolerance:
  \begin{itemize}
    \item Redundant experts, checkpointing, graceful degradation.
  \end{itemize}
\end{itemize}

\subsection*{MOE performance}

\begin{itemize}
  \item Scaling with more experts (higher sparsity):
  \begin{itemize}
    \item Better perplexity/negative log-likelihood at fixed compute budget.
    \item More sample-efficient than dense baselines with same FLOPs.
  \end{itemize}
\end{itemize}

\section*{8.\ Distributed Training over Multiple GPUs}

\subsection*{Why multiple GPUs?}

\begin{itemize}
  \item More compute and more combined memory:
  \begin{itemize}
    \item Needed for large models (billions of parameters).
  \end{itemize}
  \item Rough memory rule-of-thumb:
  \begin{itemize}
    \item For 3B parameters with FP16:
    \[
      \text{Memory} \approx 16 \times 3\text{B bytes} = 48\text{ GB},
    \]
    accounting for parameters, activations, gradients, optimizer states, inputs.
  \end{itemize}
  \item GPU examples:
  \begin{itemize}
    \item A100: 40 GB.
    \item H100: 80 GB.
  \end{itemize}
\end{itemize}

\subsection*{Communication collectives}

\begin{itemize}
  \item Used to:
  \begin{itemize}
    \item Aggregate gradients.
    \item Reconstruct sharded parameters.
  \end{itemize}
  \item Implemented via all-reduce, all-gather, reduce-scatter, etc.
\end{itemize}

\subsection*{DDP: Distributed Data Parallel}

\begin{itemize}
  \item \textbf{Standard data-parallel training}:
  \begin{itemize}
    \item Each GPU has a full copy of the model.
    \item Microbatch/minibatch split across GPUs.
    \item Each GPU computes gradients on its local batch.
    \item Gradients are all-reduced across GPUs.
  \end{itemize}
  \item Implementation:
  \begin{itemize}
    \item Overlap gradient communication with backprop compute (layer-wise).
  \end{itemize}
\end{itemize}

\subsection*{FSDP: Fully Sharded Data Parallel}

\begin{itemize}
  \item Shard model parameters across GPUs:
  \begin{itemize}
    \item Each GPU holds only a shard of weights, gradients, optimizer states.
    \item Example: with 2 GPUs, each stores half the model.
  \end{itemize}
  \item During forward/backward:
  \begin{itemize}
    \item Parameters may be gathered shard-wise for computation, then re-sharded.
  \end{itemize}
  \item Advantages:
  \begin{itemize}
    \item Train larger models with fewer GPUs (memory savings).
    \item Combine with optimizer sharding (Opt-Sharding) for further savings.
  \end{itemize}
\end{itemize}

\includegraphics[width=0.9\linewidth]{DDP.png}

\includegraphics[width=0.9\linewidth]{FSDP.png}

\section*{Lec 6}

\section*{1.\ Energy Costs and Motivation for Quantization}

\subsection*{Relative energy costs (very rough ordering)}

\begin{itemize}
  \item Operations:
  \begin{itemize}
    \item FP $>$ INT (floating point more expensive).
    \item MULT $>$ ADD (multiplication more expensive than addition).
    \item FP ADD $>$ INT ADD.
  \end{itemize}
  \item Memory:
  \begin{itemize}
    \item DRAM $\gg$ SRAM $>$ register file (per access).
  \end{itemize}
  \item Implication:
  \begin{itemize}
    \item Lower precision and fewer DRAM accesses $\Rightarrow$ big energy savings.
    \item Motivates quantization + use of integer / bit-level operations.
  \end{itemize}
\end{itemize}

\section*{2.\ Floating Point vs Fixed Point}

\subsection*{FP32 (IEEE 754 single-precision)}

\begin{itemize}
  \item 32-bit FP format:
  \begin{itemize}
    \item 1-bit sign, $8$-bit exponent, $23$-bit mantissa.
    \item Value:
    \[
      x = (-1)^{\text{sign}} \times (1.\text{mantissa}) \times 2^{\text{exponent}-\text{bias}}.
    \]
  \end{itemize}
  \item Supports wide dynamic range (via exponent).
\end{itemize}

\subsection*{Fixed point}

\begin{itemize}
  \item Represented as integer + fixed scale:
  \[
    \text{value} = \text{stored\_int} \times \text{scale}.
  \]
  \item Example: stored int 1234, scale 0.01 $\Rightarrow$ value 12.34.
  \item Constant scale for all values (no per-value exponent).
\end{itemize}

\subsection*{Why FP arithmetic is expensive}

\begin{itemize}
  \item FP add:
  \begin{itemize}
    \item Align exponents (denormalize smaller mantissa $\Rightarrow$ bit shifts).
    \item Add mantissas, then normalize (shift and adjust exponent).
    \item \includegraphics[scale=0.5]{7.png}
  \end{itemize}
  \item FP multiply:
  \begin{itemize}
    \item Multiply mantissas (cost roughly quadratic in mantissa bits).
    \item Add exponents (linear cost in exponent bits).
    \item \includegraphics[scale=0.5]{8.png}
  \end{itemize}
  \item Expensive because of Mantissa alignment (bit shifts) and manitssa multiplications (quadratic cost)
\end{itemize}

\subsection*{Swamping and catastrophic cancellation}

\begin{itemize}
  \item When adding very small FP value to a much larger one:
  \begin{itemize}
    \item Mantissa of the small value may shift out of range.
    \item Result: $A + B = A$ (small $B$ contributes nothing to sum).
  \end{itemize}
  \item Called \emph{swamping} / catastrophic cancellation.
  \item Can hurt numerical stability and model accuracy.
\end{itemize}

\subsection*{Low-precision FP formats}

\begin{itemize}
  \item Smaller $e, m$ values (e.g., FP16, BF16, FP8) reduce cost.
  \item Mixed precision:
  \begin{itemize}
    \item E.g.\ FP16 multiplications + FP32 accumulation for dot products.
    \item Good compromise: faster/lower-energy multiplies while keeping accumulation precise.
  \end{itemize}
\end{itemize}

\section*{3.\ Quantization in Neural Networks}

\subsection*{Definition}

\begin{itemize}
  \item \textbf{Quantization} = reducing precision of: weights, activations, and gradients during training.
  
  \item Goals:
  \begin{itemize}
    \item Reduce memory footprint.
    \item Reduce compute and energy (use INT, bit ops).
  \end{itemize}
  \item Typical case: FP32 $\Rightarrow$ INT8 (weights/activations).
\end{itemize}

\subsection*{Two common schemes}

\begin{itemize}
  \item \textbf{Zero-point quantization} (focus of lecture):
  \begin{itemize}
    \item Includes 0 exactly, which is important for ReLU activations.
  \end{itemize}
  \item \textbf{Maximum absolute quantization} (Absm quantization):
  \begin{itemize}
    \item Uses symmetric range around 0 with max absolute value.
  \end{itemize}
\end{itemize}

\section*{4.\ Zero-Point Quantization (FP32 $\to$ INT8)}

\subsection*{Scale factor and zero point}

\begin{itemize}
  \item Goal: map real range $[x_{\min}, x_{\max}]$ to integer range \([-128, 127]\).
  \item Example: FP32 in $\{-3.0, \dots, 0.0, \dots, 3.2\}$.
  \item Step 1: compute scale factor (sf):
  \[
    \text{sf} = \frac{x_{\max} - x_{\min}}{255}.
  \]
  \item Step 2: map real values to intermediate integer range:
  \[
    \tilde{x} = \frac{x}{\text{sf}}.
  \]
  \item Step 3: choose zero point $z$ so that zero maps to integer 0:
  \[
    x_q = \mathrm{round}\!\left(\frac{x}{\text{sf}}\right) + z.
  \]
  \item Dequantization:
  \[
    \mathrm{Deq}(x_q) = (x_q - z)\cdot \text{sf}.
  \]
  \item Example numbers (from lecture):
  \begin{itemize}
    \item $x_{\min}=-3.0$, $x_{\max}=3.2$ $\Rightarrow$ $\text{sf}=0.024$.
    \item After scaling: integers in roughly $[-123, 132]$.
    \item Choose $z=-5$ so that range shifts to [-128, 127].
  \end{itemize}
\end{itemize}

\section*{5.\ Uniform Quantization (UQ)}

\subsection*{UQ definition}

\begin{itemize}
  \item For a given layer:
  \begin{itemize}
    \item Determine dynamic range of FP32 inputs: $[-L, L]$ (e.g.\ via calibration).
    \item Partition this range into $2^b-1$ equal intervals (for $b$-bit quantization).
  \end{itemize}
  \item Scale factor:
  \[
    \text{sf} = \frac{2L}{2^b - 1}.
  \]
  \item Quantization function: linear map from $[-L,L]$ to integer range (e.g.\ [-128,127]).
  \item UQ is also called \emph{linear quantization}.
\end{itemize}

\subsection*{Quantization error}

\begin{itemize}
  \item For uniform bins of width sf:
  \begin{itemize}
    \item Maximum absolute quantization error $\leq \text{sf}$.
  \end{itemize}
  \item Overall mean quantization error:
  \begin{itemize}
    \item Weigh per-value error by probability of that value (often bell-shaped).
  \end{itemize}
  \item Clipping:
  \begin{itemize}
    \item Values $|x| > X_{\max}$ are clipped; reduce dynamic range to control error.
  \end{itemize}
\end{itemize}

\section*{6.\ Nonuniform Quantization (NoUQ)}

\subsection*{Motivation}

\begin{itemize}
  \item Weights/activations/gradients often approx.\ zero-mean Gaussian (bell-shaped).
  \item Under uniform quantization:
  \begin{itemize}
    \item Tail intervals are underutilized.
    \item Center intervals overloaded, larger contribution to error.
  \end{itemize}
  \item Idea: change bin widths so each interval contributes similar total quantization error.
\end{itemize}

\includegraphics[width=0.9\linewidth]{Quantization.png}

\subsection*{Logarithmic Quantization (LQ)}

\begin{itemize}
  \item Example nonuniform scheme:
  \begin{itemize}
    \item Quantized values spaced logarithmically, often powers of two.
  \end{itemize}
  \item If weights quantized to powers of two:
  \begin{itemize}
    \item Multiplication becomes bit shift (very cheap).
    \item Useful when large dynamic range is needed (e.g., audio coding).
  \end{itemize}
\end{itemize}

\subsection*{Piecewise Linear Quantization (PWLQ)}

\begin{itemize}
  \item Partition range into:
  \begin{itemize}
    \item Center region (dense, many values).
    \item Tail regions (sparse).
  \end{itemize}
  \item Within each region, use \emph{uniform} quantization:
  \begin{itemize}
    \item Same number of intervals per region.
    \item Center region: smaller interval width $\Rightarrow$ smaller error where data is dense.
  \end{itemize}
  \item Breakpoints (between center and tails):
  \begin{itemize}
    \item Chosen to minimize mean quantization error.
    \item Optimization is convex $\Rightarrow$ easy to solve.
  \end{itemize}
\end{itemize}

\section*{7.\ PTQ vs QAT}

\subsection*{Post-Training Quantization (PTQ)}

\begin{itemize}
  \item Take a pre-trained FP model and quantize it directly.
  \item Advantages:
  \begin{itemize}
    \item No need for original training data.
    \item No retraining; very fast to deploy.
  \end{itemize}
  \item Common in practice:
  \begin{itemize}
    \item FP32 $\to$ INT8 (with FP16/FP32 accumulation).
    \item FP32 $\to$ FP16 (with FP32 accumulation).
  \end{itemize}
  \item Limitations:
  \begin{itemize}
    \item Aggressive quantization (e.g.\ FP32 $\to$ INT4) can significantly hurt accuracy.
  \end{itemize}
\end{itemize}

\subsection*{Quantization-Aware Training (QAT)}

\begin{itemize}
  \item Incorporate quantization effects during training:
  \begin{itemize}
    \item Train with loss computed on a \emph{quantized forward pass}.
    \item Backprop uses approximate gradients through quantizers.
  \end{itemize}
  \item Needed for:
  \begin{itemize}
    \item High accuracy with ultra-low-precision models (binarized/ternary).
    \item Nonuniform quantization schemes (e.g.\ LQ, PWLQ).
  \end{itemize}
  \item Straight-Through Estimator (STE):
  \begin{itemize}
    \item Quantization function is non-differentiable (step).
    \item During backprop, approximate derivative as 1 (identity) in some range.
  \end{itemize}
\end{itemize}

\section*{8.\ Block Floating Point (BFP) and Microscaling (MX)}

\subsection*{Block Floating Point (BFP)}

\begin{itemize}
  \item Group FP numbers into blocks of size $g$ (e.g.\ $g=8$ or $16$).
  \item For each block:
  \begin{itemize}
    \item Use a \emph{shared exponent} for all values in the block.
    \item Mantissas are shifted so their exponents align.
  \end{itemize}
  \item Benefits:
  \begin{itemize}
    \item Within block, operations behave like integer arithmetic:
    \begin{itemize}
      \item No per-value exponent alignment in add.
      \item Exponent part handled once per block.
    \end{itemize}
    \item Essentially integer-like cost for many FP operations when $g$ is moderately large.
  \end{itemize}
\end{itemize}

\subsection*{Example: dot product via BFP}

\begin{itemize}
  \item Dot product $x_1 y_1 + x_2 y_2$:
  \begin{itemize}
    \item Suppose $x_1, x_2$ share exponent $e_x$; $y_1, y_2$ share exponent $e_y$.
    \item Partial products $x_i y_i$ then share exponent $e_x + e_y$.
    \item Sum of mantissas can be done with integer addition (no extra alignment).
  \end{itemize}
  \item Avoid repeated swamping/alignment for each term.
\end{itemize}

\subsection*{Microscaling (MX)}

\begin{itemize}
  \item An industry standard for BFP-like formats (Microsoft, AMD, Intel, Meta, NVIDIA, Qualcomm).
  \item Represent blocks of $k$ numbers as:
  \[
    X_i = x_i \cdot 2^p,
  \]
  where $p$ is shared exponent (or scale) per block.
  \item Enables:
  \begin{itemize}
    \item Compact representation.
    \item Efficient use of integer hardware while retaining dynamic range.
  \end{itemize}
\end{itemize}

\section*{9.\ Binarized Neural Networks (BNNs)}

\subsection*{Encoding and multiply via XNOR}

\begin{itemize}
  \item BNN weights/activations in $\{-1, +1\}$.
  \item Use \{0,1\} encoding:
  \begin{itemize}
    \item 0 $\mapsto -1$.
    \item 1 $\mapsto +1$.
  \end{itemize}
  \item Binary ``multiplication'':
  \begin{itemize}
    \item For coded bits $X,Y \in \{0,1\}$:
    \item Real multiply: $(-1) \cdot (+1) = -1$.
    \item Coded multiply via XNOR:
    \begin{itemize}
      \item XNOR$(0,1)=0$ (encodes -1).
    \end{itemize}
    \item So XNOR implements the product in coded form.
  \end{itemize}
\end{itemize}

\subsection*{BNN dot products via popcount}

\begin{itemize}
  \item Let $A,B$ be binary vectors in $\{0,1\}^N$ encoding $a,b \in \{-1,+1\}^N$.
  \item Compute:
  \begin{itemize}
    \item $C = \mathrm{XNOR}(A,B)$.
    \item $\text{popcnt}(C)$ = \# of 1 bits in $C$.
  \end{itemize}
  \item Each 1 in $C$ corresponds to matching signs ($+1\cdot+1$ or $-1\cdot-1$).
  \item Each 0 in $C$ corresponds to mismatched signs.
  \item Real dot product:
  \[
    \mathrm{DP}(a,b) = 2\cdot \text{popcnt}(C) - N.
  \]
  \item Advantages:
  \begin{itemize}
    \item Replaces $N$ multiplications with:
    \begin{itemize}
      \item Bitwise XNOR.
      \item Popcount (count bits).
    \end{itemize}
    \item Very low-energy, hardware-friendly.
  \end{itemize}
\end{itemize}

\section*{Lec 7}

\section*{2.\ Stochastic Rounding (Recap)}

\subsection*{Nearest rounding (nr) bias}

\begin{itemize}
  \item Standard quantization uses \textbf{nearest rounding}:
  \begin{itemize}
    \item Real value $x$ in interval $[a,b]$ is mapped deterministically to closest quantized value (e.g., $a$ or $b$).
  \end{itemize}
  \item In low-precision gradient updates:
  \[
    w_{k+1} = w_k - \eta \,\text{round}(x_k).
  \]
  \item Problem:
  \begin{itemize}
    \item Different $x_k$ values can all round to same quantized value $a$.
    \item This introduces \emph{systematic bias} and can stall convergence.
  \end{itemize}
\end{itemize}

\subsection*{Stochastic rounding (sr)}

\begin{itemize}
  \item Instead of deterministic rounding, randomize:
  \begin{itemize}
    \item For $x \in [a,b]$:
    \[
      \Pr[\text{sr}(x)=a] = \frac{b-x}{b-a},\quad
      \Pr[\text{sr}(x)=b] = \frac{x-a}{b-a}.
    \]
  \end{itemize}
  \item Expected value:
  \[
    \mathbb{E}[\text{sr}(x)]
      = a\frac{b-x}{b-a} + b\frac{x-a}{b-a}
      = x.
  \]
  \item Over many iterations, expected update:
  \[
    \mathbb{E}[w'] = w - \eta \sum_{i=1}^n \mathbb{E}[\text{sr}(x_i)]
      = w - \eta \sum_{i=1}^n x_i,
  \]
  \item $\Rightarrow$ unbiased approximation of full-precision gradient.
\end{itemize}

\subsection*{Practical implementation via noise}

\begin{itemize}
  \item Consider quantization interval [0,1]:
  \begin{enumerate}
    \item Generate random noise $r \in (-0.5,0.5)$.
    \item Compute $x+r$ in high precision.
    \item Apply nearest rounding:
    \begin{itemize}
      \item If $x+r < 0.5$, set sr$(x)=0$; else sr$(x)=1$.
    \end{itemize}
  \end{enumerate}
  \item Probability sr$(x)=0$ equals interval length on left side ($\approx 1-x$), matching desired formula.
  \item Generalizes to any interval $[a,b]$.
\end{itemize}

\section*{3.\ Pruning: When and How}

\subsection*{When to prune}

\begin{itemize}
  \item \textbf{At initialization}: prune connections before training.
  \item \textbf{During training}: gradually prune as weights evolve.
  \item \textbf{After training}: prune a trained dense model.
\end{itemize}

\subsection*{Weight initialization in retraining / fine-tuning}

\begin{itemize}
  \item Choices when retraining pruned models:
  \begin{enumerate}
    \item Random re-initialization from scratch.
    \item Continue from current (post-prune) weights.
    \item Reset to original initialization (important for LTH).
  \end{enumerate}
\end{itemize}

\subsection*{Conventional ``train-prune-retrain''}

\begin{itemize}
  \item Steps:
  \begin{enumerate}
    \item Train a dense network (often with L2 weight decay).
    \item Prune low-magnitude weights (e.g.\ keep top 20\%, prune 80\%).
    \item Retrain the remaining weights, continuing with the current weights or random initialization.
    \end{enumerate}
  \item Repeat pruning/retraining iteratively to increase sparsity.
  \item Typically yields \textbf{unstructured} sparsity patterns (zeros can appear anywhere).
\end{itemize}

\section*{4.\ Bell-Shaped Weights \& Unstructured Sparsity}

\subsection*{Why many weights are small}

\begin{itemize}
  \item Due to:
  \begin{itemize}
    \item \textbf{Weight decay}: L2 regularization term:
    \[
      \text{Loss} + \lambda (w_1^2 + w_2^2 + \dots)
    \]
    pulls weights towards 0.
    \item \textbf{Batch normalization}:
    \begin{itemize}
      \item Normalizes activations per minibatch to zero mean / unit variance.
      \item Stabilizes training and helps keep weights in modest ranges.
    \end{itemize}
  \end{itemize}
  \item Empirical weight and activation distributions in trained nets (e.g., ResNet-18) are bell-shaped and concentrated near 0.
\end{itemize}

\subsection*{Magnitude-based pruning}

\begin{itemize}
  \item Prune weights with smallest absolute values:
  \begin{itemize}
    \item Can often prune up to $\sim 90\%$ of weights while maintaining accuracy (with retraining).
  \end{itemize}
  \item Result is \textbf{unstructured sparsity}:
  \begin{itemize}
    \item Zero entries scattered arbitrarily within weight tensors.
  \end{itemize}
\end{itemize}

\subsection*{Unstructured sparsity: pros/cons}

\begin{itemize}
  \item Pros:
  \begin{itemize}
    \item Flexible: can prune any individual weight.
    \item Often achieves high accuracy for a given sparsity level.
  \end{itemize}
  \item Cons:
  \begin{itemize}
    \item Harder for hardware to exploit:
    \begin{itemize}
      \item Need indices to store non-zeros (CSR/CSC, etc.).
      \item Irregular memory access.
      \item Weight-stationary systolic arrays end up with many cells holding zero weights.
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection*{Pruning thresholds}

\begin{itemize}
  \item Threshold controls sparsity:
  \begin{itemize}
    \item E.g., choose threshold so that 25\% of weights are pruned.
  \end{itemize}
  \item Per-layer sensitivity:
  \begin{itemize}
    \item Some layers (e.g., first conv layer) are more sensitive; need lower thresholds there.
  \end{itemize}
  \item Threshold instability:
  \begin{itemize}
    \item Slight change in threshold can cause large accuracy drops.
    \item Often requires grid search of thresholds (possibly per layer) $\Rightarrow$ computational cost.
  \end{itemize}
\end{itemize}

\section*{5.\ Systolic Arrays, Column Combining}

\subsection*{Wasted cells with zero weights}

\begin{itemize}
  \item In a weight-stationary systolic array:
  \begin{itemize}
    \item Weights are pre-loaded into systolic cells and held there.
  \end{itemize}
  \item If many weights are zero (unstructured sparsity):
  \begin{itemize}
    \item Many systolic cells do useless work or remain idle.
    \item Poor utilization despite high theoretical sparsity.
  \end{itemize}
\end{itemize}

\subsection*{Column combining idea}

\begin{itemize}
  \item ``Packing Sparse CNNs for Efficient Systolic Arrays: Column Combining'':
  \begin{itemize}
    \item Combine multiple sparse columns into one denser column at training time.
  \end{itemize}
  \item Conflicts:
  \begin{itemize}
    \item If two columns have non-zeros in same row, they may conflict (need heuristics to avoid or resolve).
  \end{itemize}
\end{itemize}

\subsection*{Cell selection at runtime}

\begin{itemize}
  \item To exploit combined columns:
  \begin{itemize}
    \item At runtime, select appropriate systolic cells based on which original column is active.
    \item Achieve higher effective density inside array, less waste from zero weights.
  \end{itemize}
  \item \includegraphics[scale=0.5]{9.png}
\end{itemize}

\section*{6.\ Structured Pruning}

\subsection*{Pruning whole filters}

\begin{itemize}
  \item A classic structured pruning approach:
  \begin{itemize}
    \item Remove entire filters (channels) from convolution layers.
  \end{itemize}
  \item Filter importance metrics:
  \begin{itemize}
    \item E.g., L1 or L2 norm of filter weights.
    \item Prune filters with smallest norms.
  \end{itemize}
  \item Pros:
  \begin{itemize}
    \item Simpler hardware: no need to track indices inside filters.
    \item Reduces FLOPs, memory, and bandwidth consistently.
  \end{itemize}
  \item Cons:
  \begin{itemize}
    \item Coarse-grained: may hurt accuracy more than fine-grained pruning at same sparsity.
  \end{itemize}
\end{itemize}

\subsection*{N:M structured pruning (NVIDIA)}

\begin{itemize}
  \item \textbf{N:M sparsity}: in every block of $M$ consecutive weights, at most $N$ are non-zero.
  \item Tradeoff:
  \begin{itemize}
    \item Finer-grained than pruning whole filters (so better accuracy).
    \item More structured than arbitrary unstructured sparsity (so more hardware friendly).
  \end{itemize}
  \item Example: \textbf{2:4 sparsity}:
  \begin{itemize}
    \item In every group of 4 weights, at most 2 are non-zero.
    \item NVIDIA A100 sparse tensor cores can exploit 2:4 sparsity to achieve $\sim 2\times$ speedup.
    \item Load-balanced: no “straggler” lanes with many non-zeros.
  \end{itemize}
\end{itemize}

\section*{7.\ Efficient Training from Subnetworks}

\subsection*{Cost of train-prune-retrain}

\begin{itemize}
  \item Training the full dense network is expensive.
  \item Repeating:
  \begin{itemize}
    \item Train dense net.
    \item Prune.
    \item Retrain pruned net.
  \end{itemize}
  \item This loop may require multiple iterations to reach high sparsity.
\end{itemize}

\subsection*{Existence of good subnetworks}

\begin{itemize}
  \item Empirical results:
  \begin{itemize}
    \item After train-prune-retrain, we can often find subnetworks $\sim 10\times$ smaller that achieve similar accuracy.
  \end{itemize}
  \item Interpretation:
  \begin{itemize}
    \item Many weights/connections in dense network are redundant.
    \item The pruned subnetwork succinctly captures knowledge in training data.
  \end{itemize}
\end{itemize}

\subsection*{Lottery Ticket Hypothesis (LTH)}

\begin{itemize}
  \item Statement:
  \begin{itemize}
    \item In a large, randomly initialized network, there exists a much smaller subnetwork (a ``winning ticket'') that:
    \begin{itemize}
      \item When trained \emph{in isolation} from its original initialization, reaches comparable performance to the full model.
    \end{itemize}
  \end{itemize}
  \item \textbf{Pruning and rewinding} procedure:
  \begin{enumerate}
    \item Randomly initialize large network.
    \item Train for some iterations/epochs.
    \item Prune a proportion of low-magnitude weights.
    \item \emph{Reset} remaining weights back to their original initial values.
    \item Retrain this subnetwork.
  \end{enumerate}
  \item Key idea:
  \begin{itemize}
    \item The ``winning ticket'' subnetwork is defined by both:
    \begin{itemize}
      \item Which weights are kept.
      \item Their initial values (from original random init).
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection*{LTH implications and extensions}

\begin{itemize}
  \item If we knew the winning ticket (structure + init), we could:
  \begin{itemize}
    \item Train the small subnetwork directly, avoiding dense training.
  \end{itemize}
  \item Applications:
  \begin{itemize}
    \item Early-Bird Tickets (2020): detect winning tickets early in training.
    \item EarlyBERT (2021): BERT with $\sim$35--45\% less training time.
  \end{itemize}
  \item Open questions:
  \begin{itemize}
    \item How to encourage \emph{structured} winning subnetworks (e.g., N:M) instead of purely unstructured?
  \end{itemize}
\end{itemize}

\section*{8.\ SVD \& Low-Rank Factorization}

\subsection*{Singular Value Decomposition (SVD)}

\begin{itemize}
  \item For $A \in \mathbb{R}^{m \times n}$ of rank $r$:
  \[
    A = U \Sigma V^T,
  \]
  with:
  \begin{itemize}
    \item $U \in \mathbb{R}^{m \times n}$ orthogonal (columns $u_i$).
    \item $\Sigma$ diagonal with singular values $\sigma_1 \ge \dots \ge \sigma_r > 0$.
    \item $V \in \mathbb{R}^{n \times n}$ orthogonal (columns $v_i$).
  \end{itemize}
  \item This expresses $A$ as sum of $r$ rank-1 outer products:
  \[
    A = \sum_{i=1}^r \sigma_i u_i v_i^T.
  \]
  \item Keeping only top-$k$ singular values gives best rank-$k$ approximation (in Frobenius norm).
\end{itemize}

\section*{9.\ LoRA: Low-Rank Adaptation}

\subsection*{Fine-tuning large pretrained models}

\begin{itemize}
  \item Full fine-tuning:
  \begin{itemize}
    \item Update \emph{all} weights $W$ for each layer.
    \item Very parameter- and memory-intensive for large LLMs.
  \end{itemize}
  \item Parameter-efficient fine-tuning (PEFT):
  \begin{itemize}
    \item Aim: adapt model with a small number of extra parameters.
    \item LoRA is a leading PEFT method.
  \end{itemize}
\end{itemize}

\subsection*{LoRA formulation}

\begin{itemize}
  \item For a weight matrix $W \in \mathbb{R}^{d \times d}$ (or $d \times d'$):
  \begin{itemize}
    \item Freeze $W$ (do \emph{not} update).
    \item Introduce two low-rank matrices:
    \[
      A \in \mathbb{R}^{r \times d_{in}},\quad
      B \in \mathbb{R}^{d_{out} \times r}.
    \]
  \end{itemize}
  \item Effective weight during fine-tuning:
  \[
    W' = W + \Delta W,\quad \Delta W = B A.
  \]
  \item Only $A$ and $B$ are trainable; $W$ is frozen.
\end{itemize}

\subsection*{Parameter efficiency and accuracy}

\begin{itemize}
  \item Rank $r \ll d_{in}, d_{out}$:
  \begin{itemize}
    \item Trainable parameters $\approx r(d_{in}+d_{out}) \ll d_{in} d_{out}$.
  \end{itemize}
  \item When $r=d$, LoRA recovers full fine-tuning.
  \item Empirically:
  \begin{itemize}
    \item $r$ as small as 8 or 16 (for $d=768$) often yields accuracy comparable to full fine-tuning.
  \end{itemize}
\end{itemize}

\subsection*{Inference latency}

\begin{itemize}
  \item Naively, LoRA adds an extra $BAx$ path in forward:
  \[
    Wx + B(Ax).
  \]
  \item But at deployment:
  \begin{itemize}
    \item Can pre-merge: $\tilde{W} = W + \Delta W$.
    \item Forward pass uses single matrix multiply $\tilde{W} x$.
  \end{itemize}
  \item Thus, inference speed can match original model (no additional latency).
\end{itemize}

\subsection*{Sparsity issues with LoRA}

\begin{itemize}
  \item Even if original $W$ is sparse:
  \begin{itemize}
    \item $B A$ is dense (product of two low-rank matrices).
    \item Sum $W + B A$ is typically dense.
  \end{itemize}
  \item Therefore:
  \begin{itemize}
    \item LoRA can destroy original sparsity patterns.
    \item Bad for sparse hardware and energy savings unless handled carefully.
  \end{itemize}
\end{itemize}

\section*{10.\ Rosko: Row-Skipping Outer Products}

\subsection*{Row-skipping outer products}

\begin{itemize}
  \item Rosko is a method for:
  \begin{itemize}
    \item Exploiting structured \emph{row-skipping sparsity} in matrix multiplication.
  \end{itemize}
  \item Idea:
  \begin{itemize}
    \item Represent matrix multiplication as sum of outer products.
    \item Skip entire rows where entries are zero (structured skipping).
  \end{itemize}
\end{itemize}

\subsection*{Two levels of sparsity}

\begin{itemize}
  \item (1) \textbf{Pruning}:
  \begin{itemize}
    \item Introduce fine-grained sparsity in weights.
  \end{itemize}
  \item (2) \textbf{Row skipping}:
  \begin{itemize}
    \item Identify rows that can be skipped (structured sparsity at computation time).
  \end{itemize}
\end{itemize}

\subsection*{Preserving sparsity in LoRA with Rosko}

\begin{itemize}
  \item Suppose:
  \begin{itemize}
    \item Base weights $W$ trained with Rosko have row-skipping sparsity.
    \item LoRA update $\Delta W = B A$ is designed so that $B$ respects same row-skipping pattern.
  \end{itemize}
  \item Then:
  \begin{itemize}
    \item $B A$ retains row-skipping sparsity.
    \item $W + \Delta W$ also preserves row-skipping sparsity.
  \end{itemize}
  \item Benefit:
  \begin{itemize}
    \item Can do parameter-efficient fine-tuning (LoRA) while maintaining hardware-friendly structured sparsity.
  \end{itemize}
\end{itemize}

\section*{Lec 8}

\section*{2.\ Knowledge Distillation: Teacher--Student Setup}

\subsection*{Basic teacher--student picture}

\begin{itemize}
  \item Inputs $x$ fed to both teacher and student:
  \begin{itemize}
    \item Teacher outputs soft probabilities $p^{(T)}(x)$ (soft labels).
    \item Student outputs $p^{(S)}(x)$.
  \end{itemize}
  \item Goal: train student so that $p^{(S)}(x)$ matches both:
  \begin{itemize}
    \item \emph{Ground-truth hard labels} (normal supervised loss).
    \item \emph{Teacher soft labels} (distillation loss).
  \end{itemize}
\end{itemize}

\subsection*{Soft / pseudo labels for data shortage}

\begin{itemize}
  \item Often:
  \begin{itemize}
    \item Limited labeled data for new task.
    \item Original data / weights for old model may be unavailable.
  \end{itemize}
  \item KD uses teacher to generate \textbf{pseudo (soft) labels}:
  \begin{itemize}
    \item For labeled data: teacher outputs supplement ground truths.
    \item For unlabeled data: teacher provides supervision via soft labels.
  \end{itemize}
  \item Helps:
  \begin{itemize}
    \item Capture teacher's generalization behavior.
    \item Prevent forgetting of old knowledge (``learning without forgetting'').
  \end{itemize}
\end{itemize}

\subsection*{Use case: compress ensembles}

\begin{itemize}
  \item Ensembles of models often used for top accuracy (e.g., Kaggle).
  \item Ensembles are:
  \begin{itemize}
    \item Large in size.
    \item Expensive at inference.
  \end{itemize}
  \item KD can compress ensemble $\to$ single student:
  \begin{itemize}
    \item Teacher = ensemble (complex, large).
    \item Student = smaller model approximating ensemble's mapping $x \mapsto p(x)$.
  \end{itemize}
\end{itemize}

\section*{3.\ Temperature and Dark Knowledge}

\subsection*{Softmax with temperature}

\begin{itemize}
  \item Given logits $z_i$ for class $i$, standard softmax:
  \[
    p_i = \frac{e^{z_i}}{\sum_j e^{z_j}}.
  \]
  \item KD uses \textbf{softmax with temperature} $T \ge 1$:
  \[
    p_i^{(T)} = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}.
  \]
  \item When $T=1$: ordinary softmax.
  \item When $T>1$:
  \begin{itemize}
    \item Distribution becomes \emph{softer} (less peaky).
    \item Probabilities of non-argmax classes become larger.
  \end{itemize}
\end{itemize}

\subsection*{Dark knowledge}

\begin{itemize}
  \item In standard softmax ($T=1$):
  \begin{itemize}
    \item Teacher may output $p(\text{dog}) \approx 0.99$, others $\approx 0$.
    \item Hard to see similarity structure between classes.
  \end{itemize}
  \item At higher $T$:
  \begin{itemize}
    \item Dog: 0.6, Cat: 0.3, Cow: 0.08, Car: 0.02, etc.
    \item Now we know:
    \begin{itemize}
      \item Cat closer to dog than cow, cow closer than car, etc.
    \end{itemize}
  \end{itemize}
  \item This extra relational information = \textbf{dark knowledge}:
  \begin{itemize}
    \item Encodes similarity between classes beyond 0/1 labels.
    \item Helps student generalize like teacher.
  \end{itemize}
\end{itemize}

\subsection*{Choice of temperature}

\begin{itemize}
  \item Original KD paper: $T$ in range 1--20.
  \item Large $T$:
  \begin{itemize}
    \item Richer soft distributions (more dark knowledge).
    \item But too high $T$ may be overwhelming for a very small student.
  \end{itemize}
  \item For tiny students:
  \begin{itemize}
    \item Temperatures closer to 1 often work better because they can't learn complex soft relationships.
  \end{itemize}
\end{itemize}

\section*{4.\ KD Loss: Hard + Soft Components}

\subsection*{Two losses}

\begin{itemize}
  \item \textbf{Student loss (hard)}:
  \begin{itemize}
    \item Cross-entropy between student predictions and true labels:
    \[
      L_{\text{hard}} = H(y_{\text{true}}, p^{(S)}).
    \]
  \end{itemize}
  \item \textbf{Distillation loss (soft)}:
  \begin{itemize}
    \item Cross-entropy between softened teacher and student outputs:
    \[
      L_{\text{soft}} = H\!\left(p^{(T)}_{(T)}, p^{(S)}_{(T)}\right),
    \]
    where $p_{(T)}$ uses softmax with temperature $T$.
  \end{itemize}
\end{itemize}

\subsection*{Combined objective}

\begin{itemize}
  \item Overall loss:
  \[
    L = \alpha L_{\text{hard}} + \beta L_{\text{soft}},
  \]
  with $\alpha + \beta = 1$ typically.
  \item Inputs $x$ for these losses can be:
  \begin{itemize}
    \item Labeled data (for hard loss).
    \item Unlabeled data (for soft loss via teacher).
  \end{itemize}
  \item For distillation from unlabeled data:
  \begin{itemize}
    \item May want unlabeled data relevant to student's deployment domain.
  \end{itemize}
\end{itemize}

\subsection*{Teacher capacity and teacher assistants}

\begin{itemize}
  \item KD cannot be too aggressive:
  \begin{itemize}
    \item If teacher is much larger / more accurate than student:
    \item Student trained from huge teacher may perform \emph{worse} than from a smaller teacher.
  \end{itemize}
  \item \textbf{Teacher Assistant (TA)}:
  \begin{itemize}
    \item Use intermediate-capacity TA model between big teacher and tiny student.
    \item Distill: Teacher $\to$ TA $\to$ Student.
    \item Reduces capacity gap and improves student performance.
  \end{itemize}
\end{itemize}

\section*{5.\ Example KD Use Cases}

\begin{itemize}
  \item ResNet-34 $\to$ ResNet-18:
  \begin{itemize}
    \item Student is smaller ResNet with fewer parameters.
  \end{itemize}
  \item FP32 ResNet-34 $\to$ 4-bit ResNet-18:
  \begin{itemize}
    \item Student is both smaller and quantized.
  \end{itemize}
  \item DNN $\to$ decision tree:
  \begin{itemize}
    \item Train a soft decision tree to mimic a deep network.
    \item Improves interpretability (easier to explain decisions).
  \end{itemize}
\end{itemize}

\section*{6.\ MobileNets \& Depthwise Separable Convolution}

\subsection*{Standard conv vs depthwise separable conv}

\begin{itemize}
  \item Suppose:
  \begin{itemize}
    \item Input with $M$ channels.
    \item We want $N$ output channels.
    \item Kernel size $3 \times 3$.
  \end{itemize}
  \item \textbf{Standard conv}:
  \begin{itemize}
    \item $N$ filters, each $3 \times 3 \times M$.
    \item Parameter count: $9MN$.
  \end{itemize}
  \item \textbf{Depthwise separable conv} (MobileNet):
  \begin{enumerate}
    \item \emph{Depthwise conv}: one $3\times3$ filter per input channel.
    \begin{itemize}
      \item $M$ filters of size $3\times3\times1$ $\Rightarrow$ $9M$ params.
    \end{itemize}
    \item \emph{Pointwise conv}: $1\times1$ conv across channels.
    \begin{itemize}
      \item $N$ filters of size $1\times1\times M$ $\Rightarrow$ $MN$ params.
    \end{itemize}
  \end{enumerate}
  \item Total parameters: $9M + MN$.
\end{itemize}

\subsection*{Parameter/computation savings}

\begin{itemize}
  \item $1/s$-fold saving where the reduction factor (parameters) is:
  \[
    s = \frac{9M + MN}{9MN} = \frac{1}{N} + \frac{1}{9}.
  \]
  \item For large $N$ (e.g., hundreds or thousands):
  \begin{itemize}
    \item $\frac{1}{N}$ small, so $s \approx \frac{1}{9}$.
    \item Nearly 9$\times$ reduction in weights and MACs in that layer.
  \end{itemize}
  \item Similar savings in memory access (fewer weights to load).
  \item MobileNets and Xception exploit this for efficient models.
\end{itemize}

\subsection*{MobileNet intuition}

\begin{itemize}
  \item Depthwise conv:
  \begin{itemize}
    \item Spatial filtering per channel (edges, textures).
  \end{itemize}
  \item Pointwise $1\times1$ conv:
  \begin{itemize}
    \item Mixes information across channels (combines features).
  \end{itemize}
  \item Allows separation of:
  \begin{itemize}
    \item Spatial processing.
    \item Channel mixing.
  \end{itemize}
  \item Other compression techniques (pruning, quantization) can stack on top.
\end{itemize}

\section*{7.\ Separable Convolution and Low Rank}

\subsection*{Separable kernel view}

\begin{itemize}
  \item Conventional 2D conv with kernel $K$:
  \[
    (K * X)(i,j) = \sum_{u,v} K(u,v) X(i-u,j-v).
  \]
  \item If $K$ is \emph{separable}:
  \[
    K(u,v) = f(u) g(v),
  \]
  then convolution can be done as:
  \[
    X * K = (X * f) * g,
  \]
  i.e., two 1D convolutions.
\end{itemize}

\subsection*{Rank-one 3$\times$3 kernels}

\begin{itemize}
  \item A 3$\times$3 rank-one kernel:
  \[
    \begin{bmatrix}
      \alpha a & \alpha b & \alpha c \\
      \beta a  & \beta b  & \beta c  \\
      \gamma a & \gamma b & \gamma c
    \end{bmatrix}
  \]
  with parameter vectors $(\alpha,\beta,\gamma)^T$ and $(a,b,c)$.
  \item Properties:
  \begin{itemize}
    \item Rows are multiples of each other.
    \item Columns are multiples of each other.
    \item Only 6 parameters instead of 9 $\Rightarrow$ more constrained.
  \end{itemize}
  \item Empirical observation:
  \begin{itemize}
    \item Natural-image patches often low-rank.
    \item Networks trained with low-rank kernels still achieve high accuracy (MobileNets/Xception).
  \end{itemize}
\end{itemize}

\section*{8.\ Shift: Zero-FLOP Spatial ``Convolution''}

\subsection*{Shift operation idea}

\begin{itemize}
  \item Replace 3$\times$3 depthwise conv with simple shifts:
  \begin{itemize}
    \item Each channel's filter has exactly one 1 and rest 0s.
    \item Shifts feature map spatially (up, down, left, right, diagonals, etc.).
  \end{itemize}
  \item After shifting:
  \begin{itemize}
    \item Use $1\times1$ conv to mix shifted channels across depth.
  \end{itemize}
\end{itemize}

\subsection*{Shift properties}

\begin{itemize}
  \item Shift:
  \begin{itemize}
    \item Zero parameters (no learned weights for spatial op).
    \item Zero FLOPs (just data movement).
  \end{itemize}
  \item Combined with pointwise conv:
  \begin{itemize}
    \item Approximates spatial convolution with very low cost.
  \end{itemize}
\end{itemize}

\section*{9.\ Approximate MMM via Clustering \& Lookup}

\subsection*{Problem setup}

\begin{itemize}
  \item Matrix multiplication $A \times B$:
  \begin{itemize}
    \item $A$: tall data matrix (rows are feature vectors).
    \item $B$: fixed narrow weight matrix (e.g., final FC layer / classifier).
  \end{itemize}
  \item Many dot products of the form $a^T b_j$ (row $a$ of $A$, column $b_j$ of $B$).
  \item Observation:
  \begin{itemize}
    \item $B$ is fixed and known offline.
    \item Rows of $A$ follow some distribution seen in training data.
  \end{itemize}
\end{itemize}

\subsection*{Offline phase}

\begin{enumerate}
  \item Sample rows (feature vectors) from $A$ using training data.
  \item Cluster them into $K$ clusters (e.g., via $k$-means, Euclidean distance).
  \begin{itemize}
    \item Let $c_1, \dots, c_K$ be centroids.
  \end{itemize}
  \item For each centroid $c_i$ and each column $b_j$ in $B$:
  \begin{itemize}
    \item Precompute dot product $c_i^T b_j$.
    \item Store these in table $j$ (one table per column of $B$).
  \end{itemize}
\end{enumerate}

\subsection*{Online phase}

\begin{itemize}
  \item For a new row $a$ of $A$ and a column $b_j$:
  \begin{enumerate}
    \item Find nearest centroid $c$ to $a$.
    \begin{itemize}
      \item Compute distances from $a$ to $c_1,\dots,c_K$ and choose minimum.
      \item Or use locality-sensitive hashing (LSH) to hash $a$ to a bucket likely containing nearest centroid.
    \end{itemize}
    \item Approximate $a^T b_j$ by $c^T b_j$ via lookup:
    \begin{itemize}
      \item Index table $j$ at entry corresponding to $c$.
    \end{itemize}
  \end{enumerate}
  \item Complexity:
  \begin{itemize}
    \item Compute $K$ distances (or a few LSH operations) + one table lookup.
    \item This is much cheaper than full dot product for large dimensions.
  \end{itemize}
\end{itemize}

\subsection*{Partitioning $A$ into subspaces}

\begin{itemize}
  \item To handle high-dimensional features:
  \begin{itemize}
    \item Partition columns of $A$ into submatrices $A_1, A_2, \dots$.
    \item Partition $B$ row-wise correspondingly ($B_1, B_2, \dots$).
    \item Approximate dot product as sum over subspaces.
  \end{itemize}
  \item Benefits:
  \begin{itemize}
    \item Smaller-dimensional clustering per subspace.
    \item Can use SIMD to accelerate clustering / distance computations.
  \end{itemize}
\end{itemize}

\subsection*{Hardware angle: LAC vs MAC}

\begin{itemize}
  \item Conceptual hardware extension:
  \begin{itemize}
    \item Replace Multiply-Accumulate (MAC) units with Lookup-Accumulate (LAC) units:
    \begin{itemize}
      \item Each systolic cell could:
      \begin{itemize}
        \item Perform a hash / index into LUT.
        \item Accumulate looked-up dot products for subvectors.
      \end{itemize}
    \end{itemize}
  \end{itemize}
  \item This gives:
  \begin{itemize}
    \item Approximate MMM with reduced compute cost.
    \item Trade accuracy for energy/throughput.
  \end{itemize}
\end{itemize}

\section*{Lec 9}

\section*{1.\ Today’s Focus}

\begin{itemize}
  \item \textbf{Transformer Architecture:}
  \begin{itemize}
    \item \textbf{Self-Attention:} make each token’s embedding reflect its relationships with other tokens.
    \item \textbf{Feed-forward (FFN):} learn the language model via next-token prediction; FFN parameters store most of the ``knowledge''.
  \end{itemize}
  \item \textbf{Vision Transformer (ViT):} treat image patches as tokens.
  \item \textbf{KV Caching:}
  \begin{itemize}
    \item Cache attention keys and values for past tokens.
    \item For each new token, compute only its Q/K/V and attend to cached K/V.
    \item Reduces compute per new token by factor $\approx T$ (sequence length), at cost of extra memory.
  \end{itemize}
\end{itemize}

\section*{2.\ Sequence-to-Sequence Models \& The Original Transformer}

\subsection*{Seq2Seq (encoder--decoder) models}

\begin{itemize}
  \item Task: map an input token sequence (e.g., sentence) to an output sequence (e.g., translation).
  \item \textbf{Encoder:}
  \begin{itemize}
    \item Processes input tokens one by one.
    \item Produces a set of embedding vectors that summarize the input sequence.
  \end{itemize}
  \item \textbf{Decoder:}
  \begin{itemize}
    \item Conditions on encoder outputs and previously generated tokens.
    \item Outputs target tokens one step at a time.
  \end{itemize}
  \item Example configuration:
  \begin{itemize}
    \item Input length $n = 6$, embedding size $d = 512$.
  \end{itemize}
\end{itemize}

\subsection*{Original Transformer (``Attention Is All You Need'')}

\begin{itemize}
  \item Three main pieces:
  \begin{enumerate}
    \item \textbf{Encoder block} (stacked $N$ times, e.g., $N=12$).
    \item \textbf{Decoder block} (stacked $N$ times).
    \item \textbf{Final linear layer} mapping to vocabulary logits.
  \end{enumerate}
  \item Each encoder/decoder layer:
  \begin{itemize}
    \item Multi-head self-attention.
    \item Feed Forward network (MLP).
    \item Add \& LayerNorm with residual connections.
  \end{itemize}
\end{itemize}

\includegraphics[width=0.9\linewidth]{Transformer.png}

\section*{3.\ Embeddings and Positional Encoding}

\subsection*{Token embeddings}

\begin{itemize}
  \item Map each token to a $d$-dimensional embedding vector.
  \item For a sequence of $n$ tokens:
  \begin{itemize}
    \item Input matrix: $n \times d$.
    \item Example: $n=6$, $d_{\text{model}}=512$.
  \end{itemize}
  \item Embeddings come from a learned token encoder (Word2Vec-like or learned jointly).
\end{itemize}

\subsection*{Positional encoding}

\begin{itemize}
  \item Self-attention has no notion of positions on its own.
  \item Positional encoder produces embeddings encoding token positions.
  \item \textbf{Input to encoder:}
  \[
    \text{input\_to\_encoder} = \text{token\_embedding} + \text{positional\_embedding}.
  \]
  \item Positional encoding can be sinusoidal or learned; examples show how specific numeric patterns are computed.
\end{itemize}

\section*{4.\ Q, K, V and Self-Attention}

\subsection*{Query, Keys, Values (general concept)}

\begin{itemize}
  \item Conceptual analogy:
  \begin{itemize}
    \item Query: what we’re looking for (e.g., ``love'').
    \item Keys: descriptors of items (e.g., ROMANTIC, COMEDY).
    \item Values: actual items retrieved (e.g., TITANIC, THE INTOUCHABLE).
  \end{itemize}
  \item Attention finds keys most relevant to the query and returns a weighted sum of the corresponding values.
\end{itemize}

\subsection*{Self-attention in Transformers}

\begin{itemize}
  \item Given input matrix $V$ ($n \times d$) of token embeddings:
  \begin{itemize}
    \item Learnable projection matrices:
    \[
      W_Q, W_K, W_V \in \mathbb{R}^{d \times d}.
    \]
    \item Compute:
    \[
      Q = V W_Q,\quad K = V W_K,\quad V' = V W_V.
    \]
  \end{itemize}
  \item Attention weights:
  \[
    \text{Attn} = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right) \quad (n\times n \text{ matrix}).
  \]
  \item Output:
  \[
    \text{Output} = \text{Attn} \cdot V'.
  \]
  \item Complexity:
  \begin{itemize}
    \item For sequence length $n$ and dimension $d$:
    \item Cost $O(n^2 d)$ (quadratic in $n$).
  \end{itemize}
\end{itemize}

\section*{5.\ Transformer Variants and Usage}

\subsection*{Encoder vs decoder vs encoder--decoder}

\begin{itemize}
  \item \textbf{Encoder + decoder:}
  \begin{itemize}
    \item Machine translation (original Transformer).
  \end{itemize}
  \item \textbf{Encoder only:}
  \begin{itemize}
    \item BERT (text summarization, sentiment analysis, etc.).
    \item Vision Transformers (ViT).
  \end{itemize}
  \item \textbf{Decoder only:}
  \begin{itemize}
    \item GPT-style autoregressive LLMs.
  \end{itemize}
\end{itemize}

\subsection*{Decoder use cases}

\begin{itemize}
  \item Case 1: translation (prompt: input sentence; output: translated sentence).
  \item Case 2: chatbot (prompt: conversation history; output: next reply).
  \item In both, decoding is token-by-token autoregressive generation.
\end{itemize}

\section*{6.\ Masked Multi-Head Attention \& Learnable Parameters}

\subsection*{Masked multi-head attention}

\begin{itemize}
  \item In decoder for language modeling / generation:
  \begin{itemize}
    \item Each position must \emph{not} see future tokens.
  \end{itemize}
  \item Use a causal mask:
  \begin{itemize}
    \item Attention matrix made upper-triangular (future entries masked to $-\infty$ before softmax).
    \item Ensures token $i$ attends only to tokens $\le i$.
  \end{itemize}
\end{itemize}

\subsection*{Learnable parameters in Transformer}

\begin{itemize}
  \item \textbf{Feed-forward layers} (MLP per layer).
  \item \textbf{Linear projection matrices:}
  \begin{itemize}
    \item $W_Q, W_K, W_V$ and output projection $W_O$ in each attention block.
  \end{itemize}
  \item \textbf{LayerNorm parameters:}
  \begin{itemize}
    \item $\gamma$ (scale) and $\beta$ (shift) per normalized dimension.
  \end{itemize}
  \item Plus token/position embedding matrices.
\end{itemize}

\section*{7.\ Multi-Head Self-Attention}

\subsection*{Single-head vs multi-head}

\begin{itemize}
  \item \textbf{Single-head:}
  \begin{itemize}
    \item One set of $(W_Q, W_K, W_V)$.
    \item One attention matrix over all tokens.
  \end{itemize}
  \item \textbf{Multi-head:}
  \begin{itemize}
    \item $h$ different heads, each with its own $(W_Q^{(i)},W_K^{(i)},W_V^{(i)})$.
    \item Each head operates in a lower-dimensional subspace ($d_k = d/h$).
    \item Outputs from heads are concatenated and projected with $W_O$.
  \end{itemize}
\end{itemize}

\subsection*{Benefits of multi-head attention}

\begin{itemize}
  \item Different heads can focus on:
  \begin{itemize}
    \item Different types of relationships (syntax vs semantics, long-range vs local).
    \item Different positional offsets or patterns.
  \end{itemize}
  \item Visualizations show heads attending to different color-coded patterns across tokens.
\end{itemize}

\section*{8.\ Training vs Inference in Autoregressive LLMs}

\subsection*{Output probabilities}

\begin{itemize}
  \item Decoder outputs a probability distribution over the vocabulary at each timestep.
  \item Apply softmax to final linear layer outputs to get $p(\text{token} \mid \text{context})$.
  \item Next token is sampled or chosen as argmax during generation.
\end{itemize}

\subsection*{Token-by-token inference}

\begin{itemize}
  \item Autoregressive decoding:
  \begin{enumerate}
    \item Start with prompt tokens $x_{1..L}$.
    \item Compute model output distribution for next token $x_{L+1}$.
    \item Append $x_{L+1}$ to sequence.
    \item Repeat to generate $x_{L+2}, x_{L+3}, \dots$.
  \end{enumerate}
  \item For each new token:
  \begin{itemize}
    \item Model conditions on \emph{all} previously generated tokens.
  \end{itemize}
\end{itemize}

\subsection*{Training: parallel over all positions}

\begin{itemize}
  \item During training:
  \begin{itemize}
    \item Given full sequence $x_1, \dots, x_n$, model predicts all next tokens in \emph{one pass}.
    \item Loss sums (or averages) over positions:
    \[
      \sum_{t} -\log p(x_{t+1} \mid x_{\le t}).
    \]
  \end{itemize}
  \item Training is fast because:
  \begin{itemize}
    \item We compute all positions in parallel on GPU/TPU.
  \end{itemize}
\end{itemize}

\section*{9.\ Vision Transformers (ViT) and CLS Token}

\subsection*{ViT tokens}

\begin{itemize}
  \item ViT encoder treats an image as a sequence of patches:
  \begin{itemize}
    \item Split image into fixed-size patches (e.g., $16\times16$).
    \item Flatten and linearly embed each patch into a $d$-dim vector.
  \end{itemize}
  \item These patch embeddings play the role of token embeddings in NLP.
\end{itemize}

\subsection*{CLS token}

\begin{itemize}
  \item Add a learnable \texttt{[CLS]} token at beginning of sequence.
  \item Transformer encoder outputs:
  \begin{itemize}
    \item Patch outputs.
    \item CLS output (a special embedding summarizing the whole input).
  \end{itemize}
  \item Use CLS output as input to a classification head for downstream tasks.
\end{itemize}

\section*{10.\ KV Caching: Motivation and Basics}

\subsection*{Why KV caching?}

\begin{itemize}
  \item In autoregressive decoding, at each step new token must attend to all past tokens.
  \item Naive approach:
  \begin{itemize}
    \item Recompute keys $K$ and values $V$ for entire history at every step.
    \item For sequence length $L$, this leads to $O(L^2)$ compute per step.
  \end{itemize}
  \item Observation:
  \begin{itemize}
    \item Once computed, $K$ and $V$ for past tokens do not change.
    \item Only new token’s Q (and its K/V) change at each step.
  \end{itemize}
\end{itemize}

\subsection*{KV caching idea}

\begin{itemize}
  \item For each Transformer layer and each processed token position:
  \begin{itemize}
    \item Store keys $K$ and values $V$.
  \end{itemize}
  \item At step $t+1$:
  \begin{itemize}
    \item Compute new token’s $Q_{t+1}, K_{t+1}, V_{t+1}$.
    \item Append $K_{t+1}, V_{t+1}$ to cache.
    \item Use $Q_{t+1}$ to attend over cached $K_{1..t+1}$ and aggregate $V_{1..t+1}$.
  \end{itemize}
  \item Complexity:
  \begin{itemize}
    \item Each new step: $O(L)$ attention cost (dot products with cached keys) instead of $O(L^2)$ re-computation.
    \item Memory grows $O(L)$ per sequence (need to store all past K/V).
  \end{itemize}
\end{itemize}

\section*{11.\ Prefill vs Decode}

\subsection*{Prefill phase}

\begin{itemize}
  \item Given a prompt of length $L$:
  \begin{itemize}
    \item Run a full forward pass over all $L$ tokens.
    \item Compute $Q,K,V$ at each layer and token.
    \item Store (cache) $K$ and $V$ for every layer and position.
  \end{itemize}
  \item Prefill is:
  \begin{itemize}
    \item Compute-bound (heavy use of matrix multiplications).
  \end{itemize}
  \item Hardware angle:
  \begin{itemize}
    \item New GPUs (e.g., NVIDIA Rubin CPX) explore cheaper GDDR7 memory for prefill as alternative to expensive HBM.
  \end{itemize}
\end{itemize}

\subsection*{Decode phase}

\begin{itemize}
  \item For each newly generated token:
  \begin{enumerate}
    \item Compute its $Q,K,V$ at all layers.
    \item Append new $K,V$ to cache for each layer.
    \item Use $Q$ to attend over \emph{all} cached keys and aggregate values.
  \end{enumerate}
  \item Decode is:
  \begin{itemize}
    \item Memory-bandwidth-sensitive (reading cached K/V).
    \item Linear in sequence length per token ($O(L)$).
  \end{itemize}
\end{itemize}

\subsection*{KV caching trade-offs}

\begin{itemize}
  \item Pros:
  \begin{itemize}
    \item Much faster autoregressive inference (especially for long contexts).
    \item Reduces redundant re-computation of K/V.
  \end{itemize}
  \item Cons:
  \begin{itemize}
    \item Increased memory usage to store cached K/V.
    \item Need efficient cache management and memory layout.
  \end{itemize}
\end{itemize}

\section*{Lec 10}

\section*{1.\ Big Picture: Autoregressive LLMs and Efficiency}

\subsection*{Autoregressive LLMs}

\begin{itemize}
  \item Most general-purpose LLMs are trained and used as \textbf{autoregressive models}:
  \[
    p(x_1,\dots,x_T) = \prod_{t=1}^T p(x_t \mid x_{<t}).
  \]
  \item At inference time, they generate one token at a time, conditioning on all previous tokens.
  \item Alternatives exist:
  \begin{itemize}
    \item Bi-directional / masked encoders (e.g.\ BERT).
    \item Diffusion-based LLMs (covered later in course).
  \end{itemize}
  \item But for general-purpose chat / coding / reasoning, inference is typically autoregressive.
\end{itemize}

\subsection*{Why this lecture}

\begin{itemize}
  \item Autoregressive decoding is conceptually simple but:
  \begin{itemize}
    \item Expensive when generating many tokens (load weights + full forward pass per token).
    \item Attention cost is quadratic in sequence length $n$ (per self-attention layer).
  \end{itemize}
  \item Today:
  \begin{itemize}
    \item \textbf{Decoding algorithms}: greedy, beam search, sampling.
    \item \textbf{Speculative decoding}: draft + target model to speed up inference without accuracy loss.
    \item \textbf{FlashAttention}: IO-aware exact attention algorithm that reduces HBM traffic and supports longer sequences.
  \end{itemize}
\end{itemize}

\section*{3.\ Decoding Algorithms}

\subsection*{Setup}

\begin{itemize}
  \item For each step $t$, model output is a distribution over vocabulary:
  \[
    p(w_t \mid w_{<t}), \quad w_t \in \mathcal{V}.
  \]
  \item Decoding algorithm chooses the next token from this distribution.
  \item Different strategies trade off:
  \begin{itemize}
    \item Quality / coherence.
    \item Diversity.
    \item Computational cost.
  \end{itemize}
\end{itemize}

\subsection*{Greedy search}

\begin{itemize}
  \item At each timestep:
  \[
    w_t = \arg\max_{w \in \mathcal{V}} p(w \mid w_{<t}).
  \]
  \item Pros:
  \begin{itemize}
    \item Very simple and fast.
  \end{itemize}
  \item Cons:
  \begin{itemize}
    \item Myopic: can get stuck in suboptimal sequences.
    \item Low diversity; tends to repeat common patterns.
  \end{itemize}
\end{itemize}

\subsection*{Beam search (high level)}

\begin{itemize}
  \item Maintain $B=$\texttt{num\_beams} partial hypotheses.
  \item At each step:
  \begin{enumerate}
    \item For each beam, expand all possible next tokens.
    \item Score hypotheses by product or sum of log probabilities.
    \item Keep top $B$ hypotheses as new beams.
  \end{enumerate}
  \item Final output: hypothesis with highest overall probability.
  \item Pros:
  \begin{itemize}
    \item Reduces risk of missing high-probability sequences vs greedy.
  \end{itemize}
  \item Cons:
  \begin{itemize}
    \item More expensive (multiple hypotheses).
    \item Still can lack diversity, and often used more in translation than chat.
  \end{itemize}
\end{itemize}

\subsection*{Sampling}

\begin{itemize}
  \item Sample $w_t$ randomly according to $p(w_t \mid w_{<t})$ (optionally with temperature, top-$k$, or nucleus (top-$p$) truncation).
  \item Pros:
  \begin{itemize}
    \item Produces more diverse outputs.
    \item Captures multiple plausible continuations.
  \end{itemize}
  \item Cons:
  \begin{itemize}
    \item Can be less stable; may generate off-topic or lower-quality text if not tuned.
  \end{itemize}
\end{itemize}

\section*{4.\ Cost of Autoregressive Inference}

\subsection*{Token-by-token cost}

\begin{itemize}
  \item To generate each new token $g_i$:
  \begin{enumerate}
    \item Load entire model (weights).
    \item Run forward pass on all input tokens seen so far ($e_{100},\dots,e_1,g_1,\dots,g_{i-1}$).
    \item Apply decoding algorithm to select $g_i$.
  \end{enumerate}
  \item For many generated tokens:
  \begin{itemize}
    \item Latency roughly grows linearly with \#tokens ($\approx$ \#forwards).
    \item If one forward pass takes 200 ms, three tokens $\approx 600\,$ms.
  \end{itemize}
\end{itemize}

\subsection*{Motivation for speculative decoding}

\begin{itemize}
  \item Problem: repeatedly loading and using a huge model to generate each token is costly.
  \item Idea:
  \begin{itemize}
    \item Use a \emph{small, fast draft model} to propose multiple tokens quickly.
    \item Use the \emph{large target model} to verify these proposals in a single forward pass.
    \item If accepted, we save multiple target forward passes.
  \end{itemize}
\end{itemize}

\section*{5.\ Speculative Decoding: Draft + Target}

\subsection*{High-level workflow}

\begin{itemize}
  \item \textbf{Target model:}
  \begin{itemize}
    \item Large, accurate LLM (billions of parameters), expensive per forward pass.
  \end{itemize}
  \item \textbf{Draft model:}
  \begin{itemize}
    \item Smaller (compressed) model derived from the target.
    \item Much faster per token; can generate several tokens in time it takes target to generate one.
  \end{itemize}
  \item Steps:
  \begin{enumerate}
    \item Draft model autoregressively generates a small block of candidate tokens (e.g., 5) given current context.
    \item Target model runs \emph{one} forward pass over the extended context (original context + candidate block).
    \item Target model’s per-token output distributions are used to check which candidate tokens are acceptable.
  \end{enumerate}
\end{itemize}

\subsection*{Why verification works}

\begin{itemize}
  \item A forward pass of the target model on input sequence
  \[
    [e_{100},\dots,e_1,g_1,g_2,\dots]
  \]
  produces:
  \begin{itemize}
    \item Embedding vector for each input word.
    \item For each position, probability distribution over next word.
  \end{itemize}
  \item Thus, with one forward pass, target model knows:
  \begin{itemize}
    \item $p_{\text{target}}(g_1 \mid e_{100},\dots,e_1)$,
    \item $p_{\text{target}}(g_2 \mid e_{100},\dots,e_1,g_1)$,
    \item and so on for each candidate position.
  \end{itemize}
  \item We compare candidate tokens from draft with what target would have assigned:
  \begin{itemize}
    \item If the candidate token matches a likely target token (e.g.\ by acceptance rule), we accept it.
    \item If a mismatch appears at some position, we stop accepting further draft tokens.
  \end{itemize}
\end{itemize}

\subsection*{Example block}

\begin{itemize}
  \item Draft generates $g^{d}_{10}, g^{d}_{9}, g^{d}_{8}, g^{d}_{7}, g^{d}_{6}$ quickly from context.
  \item Target, in one forward pass, generates its own $g_{10}, g_{9}, g_{8}, g_{7}, g_{6}$.
  \item Compare sequences:
  \begin{itemize}
    \item Accept common prefix (e.g., $g_9, g_8, g_7, g_6$ if they match).
    \item Reject or re-decode at first mismatch.
  \end{itemize}
\end{itemize}

\subsection*{Benefits}

\begin{itemize}
  \item Potential speedup:
  \begin{itemize}
    \item If draft can produce, say, 3 tokens in the time target produces 1, and many of them are accepted, we effectively reduce average \#target calls per token.
  \end{itemize}
  \item \textbf{No accuracy degradation}:
  \begin{itemize}
    \item Final outputs equal those of target-alone decoding (verification pass enforces correctness).
  \end{itemize}
  \item Larger speedups when:
  \begin{itemize}
    \item Draft is much smaller / faster than target.
    \item Draft predictions closely track target’s outputs (few rejections).
  \end{itemize}
  \item Multiple draft models:
  \begin{itemize}
    \item Can generate multiple candidate sequences in parallel.
    \item Target can validate them in parallel using data parallelism.
  \end{itemize}
\end{itemize}

\section*{6.\ Reducing Latency with Speculative Decoding}

\begin{itemize}
  \item Without speculative decoding:
  \begin{itemize}
    \item To generate 3 tokens, need 3 target forward passes $\approx 3 \times 200$ ms = 600 ms (example).
  \end{itemize}
  \item With speculative decoding:
  \begin{itemize}
    \item Draft proposes 2 extra tokens quickly.
    \item Target verifies them and the original next token in a single \(\sim 250\) ms pass.
    \item Large reduction in end-to-end latency, especially for long generations.
  \end{itemize}
\end{itemize}

\section*{7.\ FlashAttention: Motivation}

\subsection*{Problem: quadratic IO and memory}

\begin{itemize}
  \item Vanilla attention:
  \begin{itemize}
    \item Compute $QK^\top$ (size $N\times N$), apply softmax, then multiply by $V$.
    \item Requires storing large intermediate matrices (scores, softmax outputs).
    \item Quadratic time and quadratic memory in sequence length $N$.
  \end{itemize}
  \item On GPUs:
  \begin{itemize}
    \item High Bandwidth Memory (HBM) is slower and more power-hungry than on-chip SRAM.
    \item Many IOs between HBM and SRAM make attention slow and memory-bound.
  \end{itemize}
\end{itemize}

\subsection*{Goal of FlashAttention}

\begin{itemize}
  \item \textbf{Exact} attention (no approximation) but IO-aware:
  \begin{itemize}
    \item Reduce HBM reads/writes by tiling and recomputation.
    \item Avoid storing full $N\times N$ matrices $S$ (scores) and $P$ (softmax outputs).
    \item Enable longer sequence lengths without running out of memory.
  \end{itemize}
\end{itemize}

\section*{8.\ GPU Memory Hierarchy and Optimization}

\subsection*{HBM vs SRAM}

\begin{itemize}
  \item HBM:
  \begin{itemize}
    \item Large, off-chip, slower to access.
  \end{itemize}
  \item On-chip SRAM:
  \begin{itemize}
    \item Much faster, but small capacity.
  \end{itemize}
  \item IO-aware design:
  \begin{itemize}
    \item Bring data into SRAM in small blocks.
    \item Do as much compute as possible while data stays in SRAM.
    \item Minimize round trips to HBM.
  \end{itemize}
\end{itemize}

\subsection*{Two key techniques in FlashAttention}

\begin{enumerate}
  \item \textbf{Tiling} (block processing of matrices).
  \item \textbf{Recomputation} (trading extra FLOPs to save memory).
\end{enumerate}

\section*{9.\ Tiling for Attention}

\subsection*{Tiling idea}

\begin{itemize}
  \item Split $Q, K, V$ into blocks that fit in SRAM.
  \item For example:
  \begin{itemize}
    \item Consider blocks of queries (rows) and keys/values (columns).
  \end{itemize}
  \item For each tile:
  \begin{enumerate}
    \item Load a block of $Q$ and a block of $K,V$ into SRAM.
    \item Compute partial attention scores and outputs for that tile.
    \item Accumulate contributions to the final result.
  \end{enumerate}
  \item Benefits:
  \begin{itemize}
    \item Avoids materializing full $N\times N$ score matrix in HBM.
    \item More efficient use of GPU caches and registers.
  \end{itemize}
\end{itemize}

\section*{10.\ Recomputation to Save Memory}

\subsection*{Naive backward pass}

\begin{itemize}
  \item Standard implementation stores:
  \begin{itemize}
    \item Score matrix $S = QK^\top$ ($N\times N$).
    \item Softmax matrix $P$ ($N\times N$).
  \end{itemize}
  \item These are needed to compute gradients w.r.t.\ $Q,K,V$.
  \item Memory cost: $O(N^2)$.
\end{itemize}

\subsection*{FlashAttention approach}

\begin{itemize}
  \item Instead of storing $S$ and $P$, store only:
  \begin{itemize}
    \item $Q, K, V, O$ (each $N\times d$).
    \item Additional per-block statistics (e.g., $m$ and $l$ for softmax normalization).
  \end{itemize}
  \item During backward pass:
  \begin{itemize}
    \item \textbf{Recompute} necessary intermediate values (scores, softmax) block by block.
    \item Requires extra FLOPs, but saves large memory footprint.
  \end{itemize}
  \item Trade-off:
  \begin{itemize}
    \item More compute, but often faster overall on GPUs where memory bandwidth is the bottleneck.
  \end{itemize}
\end{itemize}

\section*{11.\ Softmax Decomposition}

\subsection*{Numerical stability: subtract max}

\begin{itemize}
  \item For vector $x$:
  \[
    \text{softmax}(x)_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}.
  \]
  \item To avoid overflow:
  \begin{itemize}
    \item Subtract maximum $m(x)$ from all entries:
    \[
      \text{softmax}(x)_i = \frac{\exp(x_i - m(x))}{\sum_j \exp(x_j - m(x))}.
    \]
    \item This keeps $x_i - m(x) \le 0$.
  \end{itemize}
\end{itemize}

\subsection*{Block-wise softmax}

\begin{itemize}
  \item For long sequences, we process logits in blocks.
  \item For each block:
  \begin{itemize}
    \item Maintain block maxima and normalization factors (e.g., $m, \ell$).
    \item Combine block-level statistics to get globally correct softmax.
  \end{itemize}
  \item This allows:
  \begin{itemize}
    \item Computation of softmax in a tiled fashion.
    \item Avoid storing the full $N\times N$ matrix.
  \end{itemize}
\end{itemize}

\section*{12.\ Benefits of FlashAttention}

\subsection*{Benefit 1: Faster attention}

\begin{itemize}
  \item Less HBM traffic:
  \begin{itemize}
    \item Fewer reads/writes of large intermediate matrices.
  \end{itemize}
  \item Better utilization of GPU compute:
  \begin{itemize}
    \item More computation done while data is in fast on-chip memory.
  \end{itemize}
  \item Practical speedups:
  \begin{itemize}
    \item Often $2$--$4\times$ or more vs naive attention implementations.
  \end{itemize}
\end{itemize}

\subsection*{Benefit 2: Supports longer sequences}

\begin{itemize}
  \item Lower memory footprint:
  \begin{itemize}
    \item Avoiding $O(N^2)$ storage for $S$ and $P$ allows much larger $N$.
  \end{itemize}
  \item Enables training and inference with longer context windows on the same hardware.
\end{itemize}

\subsection*{Overall picture}

\begin{itemize}
  \item \textbf{Speculative decoding}:
  \begin{itemize}
    \item Reduces \emph{latency} for autoregressive \emph{decoding} by using a small draft model plus verification.
  \end{itemize}
  \item \textbf{FlashAttention}:
  \begin{itemize}
    \item Reduces \emph{time} and \emph{memory} for attention kernels by IO-aware tiling and recomputation.
  \end{itemize}
  \item Together:
  \begin{itemize}
    \item Address different bottlenecks in LLM inference (token generation vs core attention kernel).
  \end{itemize}
\end{itemize}

\section*{Lec 12}

\section*{1.\ Types of Machine Learning Methods}

\subsection*{Supervised learning}
\begin{itemize}
  \item Learn from labeled data: each input $x$ has ground-truth label $y$.
  \item Examples: classification, regression.
\end{itemize}

\subsection*{Unsupervised learning}
\begin{itemize}
  \item Learn structure from unlabeled data.
  \item Examples:
  \begin{itemize}
    \item k-means clustering.
    \item Data mining / association rules (e.g., diapers with baby monitors).
  \end{itemize}
\end{itemize}

\subsection*{Semi-supervised learning}
\begin{itemize}
  \item Use a mix of labeled and unlabeled data.
  \item Example: use many unlabeled real-world images to ``massage'' synthetic labeled data.
\end{itemize}

\subsection*{Self-supervised learning (SSL)}
\begin{itemize}
  \item A form of unsupervised learning where the \emph{data itself} provides supervision.
  \item Construct pretext tasks from the data:
  \begin{itemize}
    \item Predict the next word or masked part of input (LLMs).
    \item Reconstruct input from a corrupted/partial version (autoencoders, masked image models).
  \end{itemize}
  \item Examples:
  \begin{itemize}
    \item Autoencoder, VAE, masked autoencoder (MAE).
    \item Contrastive SSL (SimCLR, MoCo, BYOL, etc.).
    \item DINO-style self-distillation for ViTs.
  \end{itemize}
\end{itemize}

\subsection*{Reinforcement learning}
\begin{itemize}
  \item Learn a policy mapping states to actions from reward feedback.
  \item Used in gaming, scheduling, control, etc.
\end{itemize}

\section*{2.\ Today’s Focus}

\begin{itemize}
  \item Recap Vision Transformer (ViT) and CLS token.
  \item Show why self-supervised ViT training gives richer signals than label-only supervision.
  \item Discuss:
  \begin{itemize}
    \item Autoencoders / VAEs / MAEs as SSL examples.
    \item Contrastive learning for representation learning.
    \item DINO (Self-DIstillation with NO labels) and its evolutions: DINOv1, v2, v3.
  \end{itemize}
\end{itemize}

\section*{3.\ Recap: Vision Transformer (ViT) \& CLS Token}

\subsection*{ViT input as tokens}
\begin{itemize}
  \item Split image into patches (e.g., $16\times16$ pixels).
  \item Flatten and linearly embed each patch $\Rightarrow$ patch tokens.
  \item Add positional encodings; process with Transformer encoder.
\end{itemize}

\subsection*{CLS token}
\begin{itemize}
  \item Add special learnable token \texttt{[CLS]} at beginning of sequence.
  \item After ViT encoder:
  \begin{itemize}
    \item Output embedding of CLS token is used as global image representation.
    \item Pass CLS embedding to classification head for downstream tasks.
  \end{itemize}
\end{itemize}

\subsection*{Why SSL for ViT?}
\begin{itemize}
  \item Supervised ImageNet training:
  \begin{itemize}
    \item Compresses each image to a single label in a fixed vocabulary.
    \item Loses rich information (multiple objects, background, textures, etc.).
  \end{itemize}
  \item SSL over images:
  \begin{itemize}
    \item Provides richer learning signal.
    \item Learns general-purpose visual representations useful across tasks.
  \end{itemize}
\end{itemize}

\section*{4.\ Autoencoders as SSL}

\subsection*{Basic autoencoder}
\begin{itemize}
  \item Network structure:
  \begin{itemize}
    \item Encoder: $x \rightarrow z$ (low-dimensional latent code).
    \item Decoder: $z \rightarrow \hat{x}$ (reconstruction).
  \end{itemize}
  \item Loss: reconstruction error (e.g., MSE between $x$ and $\hat{x}$).
  \item Self-supervised: target $x$ is given by input itself (no labels).
\end{itemize}

\subsection*{Variational autoencoder (VAE)}
\begin{itemize}
  \item Probabilistic extension:
  \begin{itemize}
    \item Encoder outputs parameters of latent distribution (mean, variance).
    \item Sample $z$ from this distribution; decoder reconstructs $x$.
  \end{itemize}
  \item Loss:
  \begin{itemize}
    \item Reconstruction term + KL divergence between latent posterior and prior.
  \end{itemize}
  \item Useful for both feature learning and generative modeling.
\end{itemize}

\subsection*{Masked autoencoders (MAE)}
\begin{itemize}
  \item Mask a large fraction of input patches (e.g., $75\%$ of image patches).
  \item Encoder sees only visible patches; decoder reconstructs the full image (including masked patches).
  \item Forces model to learn semantic structure to fill in missing content.
\end{itemize}

\section*{5.\ Invariance Under Input Augmentations}

\subsection*{Desired property}
\begin{itemize}
  \item A robust classifier should be invariant to typical visual augmentations:
  \begin{itemize}
    \item Cropping, flipping, color jitter, blur, etc.
    \item All augmented versions of a ``dog'' image should map to representation labeled ``dog''.
  \end{itemize}
\end{itemize}

\subsection*{Role in contrastive learning}
\begin{itemize}
  \item Augmentations define \textbf{positive pairs}:
  \begin{itemize}
    \item Two different views of the same underlying image.
  \end{itemize}
  \item Other images (and their augmented views) serve as \textbf{negative pairs}.
  \item Goal:
  \begin{itemize}
    \item Representations of positives $\rightarrow$ similar.
    \item Representations of negatives $\rightarrow$ dissimilar.
  \end{itemize}
\end{itemize}

\section*{6.\ Contrastive Learning Basics}

\subsection*{Compressor + similarity network}

\begin{itemize}
  \item \textbf{Compressor (encoder):}
  \begin{itemize}
    \item CNN or ViT that maps image $x$ to embedding $z = f_\theta(x)$.
  \end{itemize}
  \item \textbf{Similarity head:}
  \begin{itemize}
    \item Often an MLP mapping $z$ to $h$ for contrastive loss:
    \item Similarity measured by dot product / cosine similarity between $h$ vectors.
  \end{itemize}
\end{itemize}

\subsection*{Positive vs negative pairs}

\begin{itemize}
  \item Case (1) positive pair:
  \begin{itemize}
    \item Two augmentations of same image $(x, x^+)$.
    \item Want $h(x)$ and $h(x^+)$ to be \emph{similar}.
  \end{itemize}
  \item Case (2) negative pair:
  \begin{itemize}
    \item Different images (and their augmentations).
    \item Want embeddings to be \emph{dissimilar}.
  \end{itemize}
  \item Training objective:
  \begin{itemize}
    \item Contrastive loss (e.g., NT-Xent in SimCLR) encourages these behaviors.
  \end{itemize}
\end{itemize}

\subsection*{After training: downstream classification}

\begin{itemize}
  \item The trained encoder $f_\theta$ serves as a \textbf{compressor}.
  \item For downstream tasks:
  \begin{itemize}
    \item Freeze $f_\theta$.
    \item Train a simple classifier (e.g., linear FC layer) on top of embeddings using relatively few labels.
  \end{itemize}
  \item Benefits:
  \begin{itemize}
    \item Same compressor can be reused across datasets in same domain.
    \item Potentially high compression ratios.
    \item Fast: just a feed-forward pass to get embeddings.
  \end{itemize}
\end{itemize}

\section*{7.\ DINO: Self-Distillation With No Labels}

\subsection*{Key idea}

\begin{itemize}
  \item Self-supervised ViT training via \textbf{self-distillation}:
  \begin{itemize}
    \item No ground-truth labels.
    \item Model learns from its own predictions on augmented views.
  \end{itemize}
  \item DINO = \textbf{DIstillation with NO labels}.
  \item Observed ``emerging properties'':
  \begin{itemize}
    \item ViT features trained by DINO contain semantic segmentation information \emph{without} explicit supervision.
  \end{itemize}
\end{itemize}

\subsection*{Patch-level embeddings}

\begin{itemize}
  \item DINO computes an embedding vector for each $16\times16$ patch.
  \item Visualizations:
  \begin{itemize}
    \item Pixel-level maps show attention / feature patterns aligning with object boundaries.
    \item DINOv1 (1.3M images) vs DINOv2 (142M curated images) show improved segmentation quality.
  \end{itemize}
\end{itemize}

\section*{8.\ DINO Training Pipeline}

\subsection*{Student--teacher ViT setup}

\begin{itemize}
  \item Two ViT networks:
  \begin{itemize}
    \item \textbf{Student} network: updated by gradient descent.
    \item \textbf{Teacher} network: exponential moving average (EMA) of student parameters.
  \end{itemize}
  \item Input crops:
  \begin{itemize}
    \item Global crops: large portions of the image ($>50\%$).
    \item Local crops: smaller regions ($<50\%$).
  \end{itemize}
\end{itemize}

\subsection*{Self-distillation objective}

\begin{itemize}
  \item Teacher processes \emph{global} crops.
  \item Student processes both global and local crops.
  \item Loss:
  \begin{itemize}
    \item Cross-entropy between teacher’s output distributions (on global crops) and student’s outputs (on corresponding views).
    \item ``Stop-gradient'' on teacher side (no backprop through teacher).
  \end{itemize}
\end{itemize}

\subsection*{Preventing collapse: centering \& sharpening}

\begin{itemize}
  \item Collapse = student outputs trivial constant vector for all inputs.
  \item DINO uses:
  \begin{itemize}
    \item \textbf{Centering:} maintain running mean of teacher outputs and subtract it (like normalization) to keep balanced predictions across classes/features.
    \item \textbf{Sharpening:} apply lower temperature to teacher softmax outputs to make them more peaked (less uniform).
  \end{itemize}
  \item Together, these avoid trivial solutions and encourage diverse, informative embeddings.
\end{itemize}

\section*{9.\ DINOv2 \& DINOv3}

\subsection*{DINOv2: patch-level loss}

\begin{itemize}
  \item Introduces loss at patch level, not just global CLS token.
  \item Encourages high-quality local features and better dense prediction performance (segmentation, detection).
  \item Uses larger curated dataset (142M images) and improved training recipe.
\end{itemize}

\subsection*{DINOv3: high-resolution dense features}

\begin{itemize}
  \item Focused on high-res dense features for segmentation and other pixel-level tasks.
  \item Improvements include:
  \begin{itemize}
    \item Architectural and training refinements.
    \item \textbf{Gram anchoring}: regularization of feature covariance structure to maintain stability and diversity.
  \end{itemize}
  \item DINOv3 models (e.g., ViT-g, ViT-7B) achieve strong ImageNet-1k and segmentation results.
\end{itemize}

\subsection*{DINOv3 as foundational model}

\begin{itemize}
  \item Acts as a \textbf{foundational vision model}:
  \begin{itemize}
    \item Pretrained once via self-supervised DINOv3 objective.
    \item Can be adapted for many downstream tasks via lightweight heads or fine-tuning.
  \end{itemize}
  \item Analogous role to LLMs in NLP, but for visual modalities.
\end{itemize}

\section*{Lec 13}

\section*{1.\ Today’s Focus}

\begin{itemize}
  \item \textbf{CLIP} (Contrastive Language-Image Pretraining):
  \begin{itemize}
    \item A multimodal model aligning images and texts in a joint embedding space.
    \item Enables text-prompt-defined image classifiers and zero-shot transfer.
  \end{itemize}
  \item \textbf{Aligning DINO visual features with language:}
  \begin{itemize}
    \item Use powerful self-supervised visual backbones (e.g., DINOv3).
    \item Align them with text to build CLIP-like multimodal models.
  \end{itemize}
  \item \textbf{Stable Diffusion:}
  \begin{itemize}
    \item Text-to-image generation using diffusion in a latent space.
    \item How CLIP-like text encoders guide image synthesis.
  \end{itemize}
  \item \textbf{Prompt alignment and medoid prompting:}
  \begin{itemize}
    \item Misaligned prompts can cause misclassification and unstable generations.
    \item Use cluster medoids + hard prompts to better match class distributions.
  \end{itemize}
\end{itemize}

\section*{2.\ Motivating Multimodal Models: Zero-Shot Image Classifier}

\subsection*{Static vs dynamic classifiers}

\begin{itemize}
  \item \textbf{Static classifiers (pre-2021 style):}
  \begin{itemize}
    \item Need large labeled datasets (e.g., ResNet-50 on ImageNet: 1.28M labels).
    \item Training large models is costly in time and electricity.
    \item At inference: can only predict among \emph{seen} classes.
  \end{itemize}
  \item \textbf{Dynamic classifiers (CLIP-style multimodal):}
  \begin{itemize}
    \item Pre-train once on abundant image-text pairs from the Internet
          (modern CLIP models use $\sim 5.85$B pairs).
    \item At inference: define a classifier \emph{on the fly} using text prompts
          for any set of labels (e.g., ``a photo of a dog'', ``a photo of a cat'').
    \item Achieves high downstream performance, e.g., can match ResNet-50 on ImageNet
          zero-shot without using ImageNet labels during pretraining.
  \end{itemize}
\end{itemize}

\section*{3.\ CLIP: Contrastive Language-Image Pretraining}

\subsection*{Architecture}

\begin{itemize}
  \item Two encoders:
  \begin{itemize}
    \item \textbf{Image encoder:} usually ViT or CNN, outputs image embedding $v \in \mathbb{R}^{512}$.
    \item \textbf{Text encoder:} Transformer over text, outputs text embedding $t \in \mathbb{R}^{512}$.
  \end{itemize}
  \item Training data:
  \begin{itemize}
    \item $N$ images and their associated full-text descriptions.
  \end{itemize}
  \item Contrastive loss:
  \begin{itemize}
    \item Form $N\times N$ similarity matrix between all image and text embeddings in a batch.
    \item Encourage diagonal entries (matched image-text pairs) to be larger than off-diagonals.
    \item Equivalent to cross-entropy losses over rows and columns (image$\to$text, text$\to$image).
  \end{itemize}
  \item Result: joint embedding space where semantically related images and texts are close.
\end{itemize}

\subsection*{Role of contrastive learning}

\begin{itemize}
  \item Same principle as SimCLR / MoCo / BYOL, but applied to \emph{cross-modal} pairs.
  \item Positive pairs: matching image-text.
  \item Negative pairs: all other image-text combinations in the batch.
  \item CLIP learns strong semantic representations that transfer widely.
\end{itemize}

\section*{4.\ Text-Prompt-Defined Image Classifier with CLIP}

\subsection*{Zero-shot classification pipeline}

\begin{itemize}
  \item Given a set of class names (e.g., \{dog, cat, airplane, car\}):
  \begin{enumerate}
    \item Turn each class into a text prompt, e.g., ``a photo of a \textless class\textgreater''.
    \item Encode prompts with CLIP text encoder $\Rightarrow$ text embeddings $t_c$.
    \item Encode image $I$ with CLIP image encoder $\Rightarrow$ image embedding $v_I$.
    \item Compute cosine similarity between $v_I$ and each $t_c$.
    \item Pick class whose text embedding has highest similarity.
  \end{enumerate}
  \item This defines a \textbf{text-prompt-defined classifier} with no extra training.
\end{itemize}

\subsection*{Why this is powerful}

\begin{itemize}
  \item Can quickly add new classes or redefine tasks by changing prompts.
  \item Works for many downstream datasets with good performance.
  \item CLIP acts as a general visual-language backbone.
\end{itemize}

\section*{5.\ Extending to Other Modalities}

\subsection*{Image as anchor modality (IMAGEBIND idea)}

\begin{itemize}
  \item If we have:
  \begin{itemize}
    \item Text embeddings aligned with image embeddings.
    \item Audio embeddings aligned with image embeddings.
  \end{itemize}
  \item Then we can indirectly align text and audio embeddings via the shared image space.
  \item Enables cross-modal tasks:
  \begin{itemize}
    \item Classify audio using text prompts.
    \item Retrieve audio from text queries and vice versa.
  \end{itemize}
\end{itemize}

\section*{6.\ Aligning DINO Visual Features with Language}

\subsection*{Motivation}

\begin{itemize}
  \item From Lec 12: DINOv3 is a strong self-supervised visual foundation model.
  \item Its embeddings give high-quality image and pixel-level features.
  \item Goal: align these visual features with language to create multimodal models like CLIP.
\end{itemize}

\subsection*{Locked-image / unlocked-text tuning (LiT-style)}

\begin{itemize}
  \item \textbf{Locked pre-trained image model:}
  \begin{itemize}
    \item Freeze the DINO image encoder.
  \end{itemize}
  \item \textbf{Unlocked text model:}
  \begin{itemize}
    \item Train only the text encoder (and possibly projection heads) to match the fixed image embeddings.
  \end{itemize}
  \item Average pool of patch tokens is used to produce a global image embedding.
\end{itemize}

\subsection*{DINOv2 Meets Text}

\begin{itemize}
  \item Framework aligns DINOv2 features with language using contrastive objectives.
  \item Supports:
  \begin{itemize}
    \item Image-level alignment (CLS / global token).
    \item Pixel / segment-level alignment (predict text labels for image segments).
  \end{itemize}
  \item Example: motorbike seat can be labeled separately from rest of the motorbike, showing fine-grained understanding.
\end{itemize}

\section*{7.\ Stable Diffusion: Text-to-Image Generation}

\subsection*{High-level pipeline}

\begin{itemize}
  \item Use a latent diffusion model guided by text prompts:
  \begin{itemize}
    \item Learn a generative model that starts from noise and iteratively denoises into an image.
    \item Work in a lower-dimensional latent space instead of pixel space to reduce compute.
  \end{itemize}
\end{itemize}

\subsection*{U-Net backbone}

\begin{itemize}
  \item Core noise predictor is a U-Net:
  \begin{itemize}
    \item Encoder: downsampling path capturing high-level features.
    \item Decoder: upsampling path reconstructing spatial details.
    \item Skip connections: transfer high-res information from encoder to decoder.
  \end{itemize}
  \item At each diffusion step, U-Net predicts the noise component to be removed.
\end{itemize}


\section*{8.\ Forward and Reverse Diffusion Processes}

\subsection*{Forward (noise-adding) process}

\begin{itemize}
  \item Start with clean image $I$.
  \item Add Gaussian noise step-by-step:
  \[
    I \rightarrow I_1 \rightarrow I_2 \rightarrow \dots \rightarrow I_T.
  \]
  \item After many steps, $I_T$ is nearly pure noise.
\end{itemize}

\subsection*{Reverse (denoising) process}

\begin{itemize}
  \item Train U-Net to predict noise at each step:
  \[
    \epsilon_\theta(I_i, \text{text}, i) \approx \text{true noise } e_i.
  \]
  \item Reverse step:
  \begin{itemize}
    \item Remove predicted noise from $I_i$ to obtain $I_{i-1}$.
    \item Text prompt conditions the U-Net to steer the image toward desired content.
  \end{itemize}
  \item Reverse diffusion:
  \[
    I_T \rightarrow I_{T-1} \rightarrow \dots \rightarrow I_0
  \]
  converges toward final image $I_0$ since prediction errors are smaller at earlier steps.
\end{itemize}

\subsection*{Latent Stable Diffusion}


\includegraphics[width=0.9\linewidth]{Screenshot 2025-11-17 at 11.40.33 AM.png}

\begin{itemize}
  \item Work in latent space of a VAE:
  \begin{itemize}
    \item Encode image to latent $z$, diffuse $z$ instead of pixels.
    \item Operate on vectors of length $\sim 768$ instead of $1024\times1024$ pixels.
  \end{itemize}
  \item CLIP-like text encoder provides conditioning embeddings.
  \item Switch routes conditioning path:
  \begin{itemize}
    \item Image2image: concatenation + self-attention.
    \item Text2image: cross-attention with text embeddings.
  \end{itemize}
\end{itemize}

\section*{9.\ Noise Prediction and Convergence}

\subsection*{Noise predictor}

\begin{itemize}
  \item For each step $i$, noise predictor estimates $e_i - \delta_i$ where:
  \begin{itemize}
    \item $e_i$ = true noise.
    \item $\delta_i$ = prediction error.
  \end{itemize}
  \item Earlier steps (closer to clean image) are easier:
  \[
    \delta_1 < \delta_2 < \delta_3 < \dots
  \]
\end{itemize}

\subsection*{Error analysis}

\begin{itemize}
  \item Each reverse step:
  \[
    I_{i-1} = I_i - (e_i - \delta_i) = I + \delta_i.
  \]
  \item As $i$ decreases, $\delta_i$ becomes small; reverse diffusion converges to $I$.
\end{itemize}

\section*{11.\ Misaligned Text Prompts}

\subsection*{Problem setup}

\begin{itemize}
  \item Consider two image classes: cats and dogs.
  \item Text prompts used for classification or generation may be \emph{misaligned} with underlying image distributions.
  \item Example 1D visualization:
  \begin{itemize}
    \item Embedding distributions of cat and dog images on a line.
    \item Prompt embeddings for ``cat'' and ``dog'' lie in positions not matching class centers.
  \end{itemize}
\end{itemize}

\subsection*{Consequences}

\begin{itemize}
  \item Classifier using these prompts:
  \begin{itemize}
    \item Most cat images may be closer to ``dog'' prompt in embedding space, leading to misclassification.
  \end{itemize}
  \item Text-to-image synthesis:
  \begin{itemize}
    \item Prompt may land between class clusters, causing images to fluctuate between cats and dogs.
  \end{itemize}
\end{itemize}

\section*{12.\ Medoid Prompting: Aligning Prompts with Class Distributions}

\subsection*{Cluster medoids}

\begin{itemize}
  \item Cluster medoid: real data point closest to the center of a cluster.
  \item Goal: choose text prompts whose embeddings are near the centers of class distributions.
\end{itemize}

\subsection*{Mitigating misalignment}

\begin{itemize}
  \item Compute embeddings of images per class.
  \item Use clustering (e.g., k-medoids) to find medoid images for each class.
  \item Derive \textbf{hard prompts} (possibly non-human-readable) describing medoid images.
  \item These medoid prompts better align with the image embedding distributions.
\end{itemize}

\section*{13.\ Medoid Prompting Algorithm}

\begin{itemize}
  \item Work (Mark Ting \& Kung lab):
  \begin{enumerate}
    \item Compute CLIP image embeddings for all images in target dataset.
    \item Perform $k$-medoids clustering on embeddings to obtain a medoid image per cluster.
    \item Use PEZ algorithm (``Hard Prompts Made Easy'', 2023) to derive hard prompts for each medoid.
    \item Configure:
    \begin{itemize}
      \item Text-prompt-defined classifier using medoid prompts.
      \item Or image synthesizer (Stable Diffusion) conditioned on medoid prompts.
    \end{itemize}
  \end{enumerate}
\end{itemize}


\section*{15.\ Big Picture}

\begin{itemize}
  \item \textbf{Multimodal models} like CLIP expand LLM/vision capabilities across modalities.
  \item \textbf{Stable Diffusion} uses CLIP-style text encoders + diffusion in latent space for powerful text-to-image generation.
  \item \textbf{Prompt alignment} is crucial:
  \begin{itemize}
    \item Misaligned prompts can break classifiers and generators.
    \item Medoid prompting is one principled way to align prompts with data distributions.
  \end{itemize}
  \item Overall theme: \emph{foundation models + good embeddings + careful prompting} enable flexible multimodal systems.
\end{itemize}

\section*{Lec 14}

\section*{1.\ Diffusion-Based Content Generation}

\begin{itemize}
  \item Diffusion models now drive generation for:
  \begin{itemize}
    \item Images, video, music, code, 3D objects, molecules, etc.
  \end{itemize}
  \item Classic paper: DDPM (Denoising Diffusion Probabilistic Models, 2020).
  \item Core idea:
  \begin{itemize}
    \item \textbf{Forward process:} gradually add Gaussian noise to data until it becomes nearly pure noise.
    \item \textbf{Reverse process:} learn a denoiser that iteratively removes noise, recovering clean samples from noise.
  \end{itemize}
\end{itemize}

\section*{2.\ Autoregressive LLMs \& Their Limitations}

\subsection*{Autoregressive LLMs (ARMs)}

\begin{itemize}
  \item Most current LLMs (e.g., ChatGPT) are autoregressive:
  \[
    p(x_1,\dots,x_T) = \prod_{t=1}^T p(x_t \mid x_{<t}).
  \]
  \item Advantages:
  \begin{itemize}
    \item Mirrors natural left-to-right language.
    \item Training and inference pipelines are well established.
  \end{itemize}
\end{itemize}

\subsection*{Drawbacks of pure autoregression}

\begin{itemize}
  \item \textbf{Slow decoding:}
  \begin{itemize}
    \item Tokens generated one by one; each step requires a forward pass.
  \end{itemize}
  \item \textbf{Directional constraint:}
  \begin{itemize}
    \item Left-to-right decoding cannot directly use ``future'' tokens during generation; hurts tasks that want information from both sides.
  \end{itemize}
\end{itemize}

\subsection*{Autoregression not fundamental}

\begin{itemize}
  \item What we really need is a model that captures the \emph{joint distribution} over sequences.
  \item Once we have that, we can sample in many ways:
  \begin{itemize}
    \item Autoregressive sampling.
    \item Non-autoregressive schemes (e.g., diffusion-based LLMs).
  \end{itemize}
  \item Diffusion LLMs aim to generate text via iterative denoising of masked/corrupted sequences, not strict left-to-right decoding.
\end{itemize}

\section*{3.\ Masked Diffusion LLMs (MDMs)}

\subsection*{Analogy to Stable Diffusion}

\begin{itemize}
  \item For text, the ``noise'' is implemented as \textbf{masks} on tokens (shown as underlines).
  \item Each diffusion step predicts masked tokens from context, gradually reducing uncertainty.
\end{itemize}

\subsection*{Connection to BERT}

\begin{itemize}
  \item Each denoising step resembles BERT-style masked language modeling:
  \begin{itemize}
    \item Randomly mask a fraction (e.g., $15\%$) of tokens.
    \item Model predicts the masked tokens from visible ones.
  \end{itemize}
  \item Diffusion view:
  \begin{itemize}
    \item Earlier steps: many tokens masked $\Rightarrow$ more ``noise''.
    \item Later steps: fewer tokens masked $\Rightarrow$ near final clean sequence.
  \end{itemize}
\end{itemize}

\subsection*{Denoising and self-correction}

\begin{itemize}
  \item Over multiple steps, the model can:
  \begin{itemize}
    \item Fix earlier mistakes by revising tokens in later denoising iterations.
    \item Use both left and right context (bidirectional) when predicting masked positions.
  \end{itemize}
  \item Unlike autoregressive decoding (which commits to each token), diffusion allows \emph{global} refinement.
\end{itemize}

\section*{4.\ LLaDA: Large Language Diffusion with mAsking}

\begin{itemize}
  \item Recent paper introducing a masked diffusion LLM called \textbf{LLaDA}.
  \item Highlights:
  \begin{itemize}
    \item Uses a diffusion process over masks to refine text sequences.
    \item Achieves substantially faster inference (e.g., $\sim 10\times$) vs comparable autoregressive models at similar quality.
    \item Training, however, is more expensive (many diffusion steps, like image DDPMs).
  \end{itemize}
\end{itemize}

\subsection*{Training cost vs inference speed}

\begin{itemize}
  \item Training loss vs FLOPs curve shows:
  \begin{itemize}
    \item Diffusion LLMs need more compute during training to reach a given loss.
    \item Once trained, inference can be parallelized over positions and fewer steps than token-by-token decoding.
  \end{itemize}
\end{itemize}

\section*{5.\ Block Diffusion: Semi-Autoregressive Hybrid}

\begin{itemize}
  \item \textbf{Block diffusion} aims to mix strengths of ARMs and MDMs:
  \begin{itemize}
    \item Use autoregressive decoding at the level of \emph{blocks} of tokens.
    \item Within each block, use masked diffusion to refine tokens non-autoregressively.
  \end{itemize}
  \item Benefits:
  \begin{itemize}
    \item Retain good long-range coherence from autoregression across blocks.
    \item Gain speedup from parallel refinement inside each block.
  \end{itemize}
\end{itemize}

\section*{6.\ Need for Mobile LLMs}

\subsection*{Memory constraints on devices}

\begin{itemize}
  \item Typical phones: operational DRAM $\sim$6--12 GB.
  \item Large LLMs (tens or hundreds of billions of parameters) are far too big to run fully on-device.
\end{itemize}

\subsection*{Why we must downsize}

\begin{itemize}
  \item GPT-4-scale models exceed 1T parameters.
  \item Thought experiment:
  \begin{itemize}
    \item Assume heavy global usage (front-end chat + back-end recsys etc.), $\sim 5\%$ of people's time.
    \item Serving everyone with GPT-4 at 50 tokens/s might require $\sim$100M NVIDIA H100 GPUs (each $\approx 60$ TFLOPs/s).
  \end{itemize}
  \item This is economically and environmentally challenging.
  \item Conclusion: \textbf{small, efficient models} (esp.\ on device) are crucial.
\end{itemize}

\section*{7.\ Meta’s MobileLLM}

\subsection*{Goal and performance}

\begin{itemize}
  \item MobileLLM: family of compact LLMs ($<$1B parameters).
  \item Achieve strong zero-shot common-sense reasoning despite small size.
  \item Some 350M variants can rival larger 7B models (e.g., LLaMA-2) on certain API-calling tasks.
\end{itemize}

\subsection*{Key architectural innovations}

\begin{enumerate}
  \item \textbf{Model depth over width}
    \begin{itemize}
      \item Increase number of layers while keeping hidden dimension moderate.
      \item Empirically, deeper-but-narrow networks perform better at fixed parameter budget.
    \end{itemize}
  \item \textbf{Embedding sharing}
    \begin{itemize}
      \item Share weights between input token embedding and output (LM head) projection.
      \item Example: map $\mathbb{R}^{50000} \to \mathbb{R}^{786}$ and back using the same matrix.
      \item Especially impactful for small models (embedding matrices are large).
    \end{itemize}
  \item \textbf{Grouped-Query Attention (GQA)}
    \begin{itemize}
      \item Multiple query heads share a single key/value head group.
      \item Reduces KV-cache size and memory bandwidth; improves efficiency with minimal accuracy loss.
    \end{itemize}
  \item \textbf{Immediate block-wise layer sharing (LS)}
    \begin{itemize}
      \item Transformer blocks are reused in a structured way:
      \item In ``immediate block-wise sharing'', block $i$ and $i+1$ share weights and are executed back-to-back.
      \item Benefit: shared weights stay hot in cache (better locality) while doubling effective depth.
      \item Compared to ``repeat-all-over'' sharing, performance is slightly lower but cache efficiency is higher.
    \end{itemize}
\end{enumerate}

\subsection*{SwiGLU activation}

\begin{itemize}
  \item Many recent LMs replace ReLU with gated activations like \textbf{SwiGLU}.
  \item SwiGLU:
  \[
    \text{SwiGLU}(x) = (Wx + b) \odot \text{Swish}(Vx + c), \quad
    \text{Swish}(x) = x \cdot \sigma(\beta x),
  \]
  with $W,V,b,c,\beta$ learnable.
  \item For MobileLLM-125M, switching vanilla FFN $\to$ SwiGLU improves average zero-shot reasoning accuracy (e.g., 42.6 $\to$ 43.9).
\end{itemize}

\subsection*{Design roadmap and latency}

\begin{itemize}
  \item Starting from a 12-layer, 768-dim, 125M model:
  \begin{itemize}
    \item Improve FFN design (SwiGLU).
    \item Increase depth, apply layer sharing, embed sharing, GQA.
  \end{itemize}
  \item On iPhone 13 latency:
  \begin{itemize}
    \item Compare 30-layer 125M, LS-125M (2$\times$30 shared layers), and a 60-layer non-shared model.
    \item LS-125M achieves good accuracy/latency tradeoff because shared blocks reuse cache-resident weights.
  \end{itemize}
\end{itemize}

\section*{8.\ Apple Intelligence and Apple Foundation Models (AFM)}

\subsection*{AFM overview}

\begin{itemize}
  \item Apple Intelligence uses:
  \begin{itemize}
    \item \textbf{AFM-on-device}: $\sim 2.73$B parameters, runs locally on Apple silicon.
    \item \textbf{AFM-server}: larger model running on Apple silicon servers via Private Cloud Compute.
  \end{itemize}
  \item Focus on:
  \begin{itemize}
    \item Language understanding, reasoning, writing.
    \item Tool use and personal tasks (summaries of emails, notifications, etc.).
  \end{itemize}
  \item Many MobileLLM-style techniques appear (shared embeddings, GQA, SwiGLU, etc.).
\end{itemize}

\subsection*{AFM architecture}

\begin{itemize}
  \item Dense decoder-only Transformer with:
  \begin{itemize}
    \item Shared input/output embedding matrix (memory saving).
    \item Pre-normalization with RMSNorm; Query/Key normalization for stability.
    \item Grouped-Query Attention: e.g., 24 query heads, 8 KV heads (KV cache $\approx \frac{1}{3}$ size).
    \item SwiGLU activation.
    \item RoPE positional embeddings (base frequency 500k) for long context.
    \item BPE tokenizer: 100K vocab for AFM-server; 49K for AFM-on-device.
  \end{itemize}
\end{itemize}

\subsection*{AFM pre-training recipe}

\begin{enumerate}
  \item \textbf{Core pre-training}
    \begin{itemize}
      \item AFM-server: trained from scratch on 6.3T tokens, sequence length 4096, batch size 4096.
      \item AFM-on-device (2.73B) obtained by:
      \begin{itemize}
        \item Knowledge distillation and structural pruning from a 6.4B model (pruning FFN hidden dimension only).
      \end{itemize}
    \end{itemize}
  \item \textbf{Continued pre-training (8192 length)}
    \begin{itemize}
      \item Train on another 1T tokens with longer context, upweight math and code, downweight bulk web crawl.
    \end{itemize}
  \item \textbf{Context lengthening}
    \begin{itemize}
      \item Additional 100B tokens with sequence length 32{,}768.
      \item Include synthetic long-context QA data.
    \end{itemize}
\end{enumerate}

\subsection*{Post-training: SFT and RLHF}

\begin{itemize}
  \item Enhance AFMs with instruction-following and conversational skills:
  \begin{itemize}
    \item Stage 1: Supervised fine-tuning (SFT) on curated instruction data.
    \item Stage 2: Reinforcement learning from human feedback (RLHF).
  \end{itemize}
  \item Two new algorithms:
  \begin{itemize}
    \item \textbf{iTeC} (rejection sampling with teacher committee) for better SFT data.
    \item \textbf{MDLOO} (mirror descent policy optimization with leave-one-out advantage estimator) for RLHF.
  \end{itemize}
\end{itemize}

\section*{9.\ LoRA Adapters, Quantization, and Accuracy Recovery}

\subsection*{Runtime-swappable LoRA adapters}

\begin{itemize}
  \item AFM uses \textbf{LoRA adapters} that can be dynamically swapped per feature:
  \begin{itemize}
    \item E.g., specialized adapters for mail summarization, notification rewriting, etc.
  \end{itemize}
  \item Base model stays fixed; LoRA layers add small, task-specific low-rank updates.
\end{itemize}

\subsection*{Quantization}

\begin{itemize}
  \item AFM-on-device is quantized to $\sim 3.5$ bits per weight on average.
  \item Goal: fit in device memory while retaining quality.
\end{itemize}

\subsection*{Accuracy-recovery adapter}

\begin{itemize}
  \item Let $W$ be original weight matrix; $W_q$ its quantized version.
  \item Use LoRA matrices $A,B$ to correct for quantization error:
  \[
    W' = W_q + BA.
  \]
  \item Training procedure:
  \begin{enumerate}
    \item Train $A,B$ with $W_q$ frozen on the same large dataset used for $W$:
      \begin{itemize}
        \item Learn a global correction that recovers most accuracy lost to quantization.
      \end{itemize}
    \item For each feature-specific LoRA fine-tuning:
      \begin{itemize}
        \item Initialize $A,B$ with the pre-trained accuracy-recovery weights.
        \item Continue training on feature-specific data.
      \end{itemize}
  \end{enumerate}
\end{itemize}

\section*{10.\ Evaluation, Safety, and Alignment}

\subsection*{Human satisfaction and benchmarks}

\begin{itemize}
  \item Apple emphasizes human evaluation:
  \begin{itemize}
    \item Human satisfaction scores often correlate better with real-world user experience than automatic benchmarks.
    \item Benchmarks (e.g., Phi-3, other LMs as graders) are useful but not sufficient.
  \end{itemize}
\end{itemize}

\subsection*{Safety taxonomy}

\begin{itemize}
  \item Apple defines a safety taxonomy with 12 primary categories and 51 subcategories:
  \begin{itemize}
    \item Examples: hate speech, discrimination, illegal activities, adult sexual content, graphic violence.
  \end{itemize}
  \item Used to guide:
  \begin{itemize}
    \item Data filtering.
    \item Post-training alignment.
    \item Safety evaluation of features.
  \end{itemize}
\end{itemize}

\subsection*{Alignment pipeline (OpenAI-style analogy)}

\begin{itemize}
  \item Standard 3-step alignment:
  \begin{enumerate}
    \item Supervised fine-tuning on demonstrations.
    \item Reward model (RM) training using human preference rankings.
    \item RL (e.g., PPO or MDLOO) to maximize RM scores subject to constraints.
  \end{enumerate}
  \item Apple aims for flexibility:
  \begin{itemize}
    \item Pre-training remains general.
    \item Safety and policies can be tuned per feature via adapters and post-training.
  \end{itemize}
\end{itemize}

\subsection*{Human evaluation of harmfulness \& safety prompts}

\begin{itemize}
  \item Human raters score outputs on harmfulness and safety-related prompts.
  \item Apple finds human-based evaluation more aligned with actual user experience than purely automatic ``AI-as-judge'' approaches.
\end{itemize}

\section*{11.\ Big Picture}

\begin{itemize}
  \item \textbf{Diffusion-based LLMs} (e.g., LLaDA) show that:
  \begin{itemize}
    \item Autoregression is not the only way to generate text.
    \item Masked diffusion can enable faster inference and global self-correction, at the cost of heavier training.
  \end{itemize}
  \item \textbf{MobileLLM} and \textbf{AFM} illustrate:
  \begin{itemize}
    \item Architectural choices (depth-over-width, GQA, layer sharing, SwiGLU, shared embeddings) are crucial for small, efficient LLMs.
    \item Quantization + LoRA-style adapters can deliver powerful on-device experiences within strict memory/latency budgets.
  \end{itemize}
  \item \textbf{Alignment and safety}:
  \begin{itemize}
    \item Human-centered evaluation and detailed taxonomies are key to deploying these models responsibly.
  \end{itemize}
\end{itemize}

\section*{Lec 16}

\section*{1.\ What Is LLM Reasoning?}

\begin{itemize}
  \item Reasoning LLMs aim to go beyond surface pattern matching to perform:
  \begin{itemize}
    \item Multi-step math proofs.
    \item Logic puzzles, coding, planning.
    \item Complex question answering.
  \end{itemize}
  \item Key notion: \textbf{``thinking tokens''} (internal chain-of-thought).
  \item Idea: reshape model's output distribution so that \emph{thoughtful} responses naturally get the highest probability.
\end{itemize}

\subsection*{Examples of reasoning models}

\begin{itemize}
  \item OpenAI: o1, o3, o3-mini, GPT-4o (reasoning tuned).
  \item Qwen: Qwen2.5-72B/32B-Instruct.
  \item Google: Gemini 1.5 Pro (Reasoning), Gemini 2.0, Gemini 1.5 Flash Thinking.
  \item DeepSeek: R1.
  \item Anthropic: Claude 3.5 Sonnet/Haiku Thinking.
  \item Mistral: Mistral Large (Reasoning).
  \item Cohere: Command-R+, Command-R-7B.
  \item xAI: Grok-2 (Reasoning).
\end{itemize}

\section*{2.\ OpenAI o1 and Test-Time Scaling}

\subsection*{OpenAI o1}

\begin{itemize}
  \item 175B-parameter reasoning model trained with RL.
  \item ``Thinks before it answers'':
  \begin{itemize}
    \item Produces long internal chain-of-thought before final answer.
  \end{itemize}
  \item Uses reinforcement learning to reward high-quality reasoning traces.
\end{itemize}

\subsection*{Test-time scaling idea}

\begin{itemize}
  \item Instead of only scaling model size or training data, scale:
  \begin{itemize}
    \item \textbf{Thinking time} at inference (number/length of CoT samples).
    \item \textbf{Number of samples} drawn per question.
  \end{itemize}
  \item Example from o1:
  \begin{itemize}
    \item Longer test-time ``thinking budget'' can significantly boost accuracy on math and reasoning benchmarks.
  \end{itemize}
\end{itemize}

\subsection*{Gemini 2.0 thinking mode}

\begin{itemize}
  \item Similar concept: special ``thinking'' configuration with extra reasoning steps.
  \item Users can trade latency for better reasoning quality.
\end{itemize}

\section*{3.\ Two Fine-Tuning Approaches for Reasoning}

\begin{itemize}
  \item \textbf{(1) Supervised finetuning (SFT)}
  \begin{itemize}
    \item Train on high-quality CoT traces and solutions.
  \end{itemize}
  \item \textbf{(2) RL finetuning (RFT)}
  \begin{itemize}
    \item Frame reasoning quality as a reward; optimize via policy gradient methods.
  \end{itemize}
\end{itemize}

\section*{4.\ (1) Supervised Finetuning and s1}

\subsection*{s1: Simple Test-Time Scaling}

\begin{itemize}
  \item Goal: take a strong base model and improve its reasoning with a \emph{small}, carefully curated CoT dataset.
  \item Base model: Qwen2.5-32B-Instruct.
\end{itemize}

\subsection*{s1K dataset construction}

\begin{itemize}
  \item Start with 59K questions from many datasets.
  \item Use Gemini Flash Thinking API to generate reference triplets:
  \begin{itemize}
    \item (Question, Reasoning, Solution).
  \end{itemize}
  \item Decontaminate/deduplicate and filter for:
  \begin{itemize}
    \item High quality (few errors, good formatting).
    \item Difficulty (non-trivial, require reasoning).
    \item Diversity (50 domains: math, logic, code, etc.).
  \end{itemize}
  \item Final curated set: 1K samples $\Rightarrow$ \textbf{s1K}.
\end{itemize}

\subsection*{Supervised finetuning process}

\begin{itemize}
  \item Fine-tune Qwen2.5-32B on s1K:
  \begin{itemize}
    \item Loss computed only on reasoning traces + final solutions.
    \item Sequence length set long enough so CoTs are not truncated.
    \item Observe: shorter training sequence length $\Rightarrow$ longer reasoning traces at test time.
    \item Train 5 epochs, batch size 16 → 315 gradient steps.
  \end{itemize}
  \item Resulting model: \textbf{s1}.
\end{itemize}

\subsection*{s1 test-time scaling}

\begin{itemize}
  \item At inference, allow s1 to think longer (more tokens) or sample multiple reasoning paths.
  \item Even with only 1K training examples, s1 shows large gains on reasoning benchmarks by combining:
  \begin{itemize}
    \item High-quality supervision.
    \item Increased test-time reasoning budget.
  \end{itemize}
\end{itemize}

\section*{5.\ Budget Forcing}

\subsection*{End-of-thinking (EOT) token}

\begin{itemize}
  \item Models often have a special token indicating the end of reasoning, after which they produce the final answer.
  \item \textbf{Budget forcing}: manipulate this token to control thinking length.
\end{itemize}

\subsection*{How budget forcing works}

\begin{itemize}
  \item If the model emits EOT too early:
  \begin{itemize}
    \item Replace it with a non-EOT token (e.g., ``However'', ``Wait a moment'') to force more reasoning.
  \end{itemize}
  \item When enough thinking has been done:
  \begin{itemize}
    \item Append EOT token to cut off reasoning and request final answer.
  \end{itemize}
  \item Repeating suppression of EOT can cause loops:
  \begin{itemize}
    \item Performance gains plateau after too many ``wait'' extensions.
  \end{itemize}
\end{itemize}

\section*{6.\ (2) RL Finetuning and ReFT}

\subsection*{ReFT: Reasoning with Reinforced Finetuning}

\begin{itemize}
  \item Reinforcement learning approach to improve CoT quality.
  \item Objective: maximize rewards for:
  \begin{itemize}
    \item Correct final answers.
    \item High-quality, helpful reasoning traces.
  \end{itemize}
  \item Combines:
  \begin{itemize}
    \item Pre-trained base LLM.
    \item RL policy optimization methods.
  \end{itemize}
\end{itemize}

\subsection*{Scaling reasoning training}

\begin{itemize}
  \item Thesis: we should scale \emph{chain-of-thought length}, not just model size.
  \item Longer CoTs and more samples per question provide:
  \begin{itemize}
    \item Better exploration of solution space.
    \item More opportunities to correct mistakes via self-consistency.
  \end{itemize}
\end{itemize}

\section*{7.\ Self-Consistency for CoT}

\subsection*{Basic idea}

\begin{itemize}
  \item Run the model multiple times with CoT prompting:
  \begin{itemize}
    \item Each run produces its own reasoning path and final answer.
  \end{itemize}
  \item Collect many samples and perform \textbf{majority vote} on final answers.
\end{itemize}

\subsection*{Marginalization view}

\begin{itemize}
  \item Approximate:
  \[
    p(y \mid x) \approx \sum_{c} p(y \mid c, x) \, p(c \mid x),
  \]
  where $c$ runs over reasoning chains.
  \item Sampling many $c$ and picking the answer that appears most often approximates marginalizing over latent reasoning paths.
\end{itemize}

\subsection*{Empirical gains}

\begin{itemize}
  \item Huge performance boosts on math word problems and logic benchmarks.
  \item Works even for base models without special reasoning finetuning.
\end{itemize}

\section*{8.\ DeepSeek and Policy Optimization for Reasoning}

\subsection*{DeepSeek R1 and RL for math reasoning}

\begin{itemize}
  \item DeepSeek explores RL methods specialized for reasoning tasks (esp.\ math).
  \item Emphasis on:
  \begin{itemize}
    \item Rewarding correct final answers and concise, valid proofs.
    \item Scaling number of reasoning samples and CoT length.
  \end{itemize}
\end{itemize}

\subsection*{From PRO to GRPO}

\begin{itemize}
  \item \textbf{PRO: Proximal Reward Optimization}
  \begin{itemize}
    \item Related to PPO but focuses directly on sampled rewards for each trajectory.
    \item Normalize rewards across samples to compute an advantage:
    \[
      A_i = \frac{r_i - \mu_r}{\sigma_r}\ (\text{conceptually}),
    \]
    where $r_i$ is reward for sample $i$.
  \end{itemize}
  \item \textbf{Key simplification:} use only the \emph{largest} $A_i$ in a group as the learning signal.
  \item Removes need for a separate value function network.
\end{itemize}

\subsection*{GRPO: Group Relative Policy Optimization}

\begin{itemize}
  \item Extends PRO by operating over \textbf{groups} of sampled responses:
  \begin{itemize}
    \item For each prompt:
    \begin{enumerate}
      \item Sample multiple candidate CoTs from the current policy.
      \item Compute reward for each (based on correctness, reasoning quality).
      \item Normalize rewards within the group to get relative advantages.
      \item Use the best (or top-$k$) samples to update the policy.
    \end{enumerate}
  \end{itemize}
  \item Benefits:
  \begin{itemize}
    \item Stabilizes training by comparing responses to peers from same prompt.
    \item Encourages relative improvement without explicit value model.
    \item Naturally leverages test-time scaling (many CoT samples per prompt).
  \end{itemize}
\end{itemize}

\section*{9.\ Mixture of Experts (MoE) and Reasoning}

\begin{itemize}
  \item MoE: architecture with many expert feed-forward blocks; a router chooses which experts to activate per token.
  \item For reasoning:
  \begin{itemize}
    \item Different experts may specialize in math, code, logic, etc.
    \item Only a subset of experts is active per token $\Rightarrow$ higher capacity at similar FLOPs.
  \end{itemize}
  \item Combining MoE with RL reasoning training is an active research direction.
\end{itemize}

\section*{10.\ Big Picture}

\begin{itemize}
  \item LLM reasoning quality can be dramatically improved by:
  \begin{itemize}
    \item High-quality CoT supervision (SFT with small curated datasets like s1K).
    \item \textbf{Test-time scaling}: more thinking tokens, more samples, self-consistency.
    \item RL finetuning methods (ReFT, PRO, GRPO, DeepSeek-style training).
  \end{itemize}
  \item Key trend: scale \emph{reasoning process} (CoT length, sample count) rather than only model size.
  \item Practical levers:
  \begin{itemize}
    \item Budget forcing to control thinking length.
    \item Self-consistency for robust answers.
    \item Group-based RL methods to refine reasoning without always needing a value function.
  \end{itemize}
\end{itemize}

\section*{Lec 17}

\section*{1.\ Today’s Focus}

\begin{itemize}
  \item \textbf{Sparse Mixture of Experts (MoE)}:
  \begin{itemize}
    \item Replace a single FFN in Transformer blocks with multiple expert FFNs.
    \item Router chooses a subset of experts per token.
    \item Enables large parameter counts but sparse activation $\Rightarrow$ efficient inference.
  \end{itemize}
  \item \textbf{Expert Parallelism (EP)}:
  \begin{itemize}
    \item How to map experts to GPUs and keep loads balanced.
  \end{itemize}
  \item \textbf{AlphaEvolve / OpenEvolve}:
  \begin{itemize}
    \item Evolutionary coding agents using LLMs to iteratively improve code.
    \item Example use: discover efficient scheduling algorithms for MoE placement.
  \end{itemize}
  \item \textbf{ADRS (AI-Driven Research for Systems)}:
  \begin{itemize}
    \item Framework where AI agents plus verifiers carry out large parts of systems research.
  \end{itemize}
\end{itemize}

\section*{2.\ Sparse Mixture of Experts (MoE)}

\subsection*{From FFN to experts}

\begin{itemize}
  \item Recall vanilla Transformer block:
  \begin{itemize}
    \item Multi-head attention (per token).
    \item Token-wise FFN (dense).
  \end{itemize}
  \item \textbf{MoE idea:} replace one FFN with many FFNs = \emph{experts}.
  \item Each expert has the same architecture but different parameters (specialization).
  \item For each token at each layer:
  \begin{itemize}
    \item A router network outputs scores over experts.
    \item Top-$k$ experts selected; token is sent to those experts and outputs are combined (weighted sum).
  \end{itemize}
\end{itemize}

\subsection*{Sparse vs dense models}

\begin{itemize}
  \item Dense model:
  \begin{itemize}
    \item All parameters used for every token.
  \end{itemize}
  \item Sparse MoE:
  \begin{itemize}
    \item Only a small subset of experts activated per token (e.g., 2 of 8).
    \item Allows scaling total parameters without proportional compute.
  \end{itemize}
\end{itemize}

\subsection*{Mixtral 8x7B example}

\begin{itemize}
  \item \textbf{Mixtral 8x7B}:
  \begin{itemize}
    \item 8 experts, each a 7B-parameter decoder-only model.
    \item Total parameters $\approx 46.7$B.
    \item But per-token, only 2 experts used $\Rightarrow$ $\sim 12.9$B parameters active.
    \item Inference cost similar to a dense 12.9B model but with extra capacity from expert specialization.
  \end{itemize}
\end{itemize}

\subsection*{Quality vs budget}

\begin{itemize}
  \item Tradeoff curves (from Mixtral paper):
  \begin{itemize}
    \item For a fixed FLOPs budget, sparse MoE can achieve higher quality than dense models.
    \item Or for a target quality, MoE achieves it at lower cost.
  \end{itemize}
\end{itemize}

\section*{3.\ Routing and Load Balancing in MoE}

\subsection*{Router network}

\begin{itemize}
  \item For each token:
  \begin{itemize}
    \item Input: token representation $h$.
    \item Router: simple linear layer + softmax over experts.
    \item Select top-$k$ experts by probability, send token to them.
  \end{itemize}
\end{itemize}

\subsection*{Noisy top-$k$ routing}

\begin{itemize}
  \item Problem: early in training, a few experts may get most traffic:
  \begin{itemize}
    \item They receive more gradients $\Rightarrow$ become better $\Rightarrow$ get selected more (rich-get-richer).
  \end{itemize}
  \item Solution: \textbf{noisy top-$k$ routing}:
  \begin{itemize}
    \item Add Gaussian noise to router logits before selecting top-$k$.
    \item Encourages exploration and more balanced training across experts.
  \end{itemize}
\end{itemize}

\subsection*{Capacity and overflow (Switch Transformers)}

\begin{itemize}
  \item Each expert has a capacity:
  \[
    \text{capacity} = \frac{\text{total tokens}}{\text{\#experts}} \times \text{capacity factor}.
  \]
  \item If too many tokens are routed to an expert:
  \begin{itemize}
    \item Excess tokens overflow and may be dropped or routed elsewhere.
  \end{itemize}
  \item Larger capacity factor:
  \begin{itemize}
    \item Reduces overflow but increases compute/communication.
  \end{itemize}
\end{itemize}

\subsection*{Summary}

\begin{itemize}
  \item Sparse expert models:
  \begin{itemize}
    \item Scale capacity via more experts.
    \item Keep per-token compute roughly constant by activating a few experts.
  \end{itemize}
  \item Different passes on the same input may activate different experts.
  \item Some designs use a \textbf{shared expert} across positions/layers (DeepSeek-V3, LLaMA 4) to capture global knowledge.
\end{itemize}

\section*{4.\ Expert Specialization and Sharding}

\subsection*{Specialization}

\begin{itemize}
  \item Empirically, experts in encoder or decoder tend to specialize:
  \begin{itemize}
    \item Different topics, languages, or syntactic structures.
  \end{itemize}
  \item Helps utilize large parameter counts effectively.
\end{itemize}

\subsection*{Model sharding / Expert Parallelism (EP)}

\begin{itemize}
  \item Experts distributed across GPUs:
  \begin{itemize}
    \item Form of model parallelism.
  \end{itemize}
  \item Router must:
  \begin{itemize}
    \item Decide which experts to activate.
    \item Route tokens to the right GPU(s).
  \end{itemize}
  \item Key challenges:
  \begin{itemize}
    \item \textbf{Load balance}: some experts may be hotter than others.
    \item \textbf{Fault tolerance}: redundant experts, checkpointing, graceful degradation.
  \end{itemize}
\end{itemize}

\subsection*{Redundant experts for load balancing}

\begin{itemize}
  \item Overloaded experts are \textbf{replicated} on additional GPUs.
  \item Scheduling algorithm chooses:
  \begin{itemize}
    \item How many replicas of each expert to create.
    \item Where to place them to balance GPU loads.
  \end{itemize}
\end{itemize}

\section*{5.\ Expert Parallelism Scheduling Algorithms}

\subsection*{Slow greedy bin-packing (EPLB)}

\begin{itemize}
  \item Experts sorted by load.
  \item Greedy algorithm places each expert into the GPU with lowest current load.
  \item Simple but:
  \begin{itemize}
    \item Uses Python for-loops.
    \item Can be slow for many experts/GPU slots.
  \end{itemize}
\end{itemize}

\subsection*{Fast zigzag scheduling}

\begin{itemize}
  \item Faster heuristics for assigning experts:
  \begin{itemize}
    \item \textbf{Zigzag raster scanning}.
    \item \textbf{Zigzag snake scanning}.
  \end{itemize}
  \item Idea:
  \begin{itemize}
    \item Place high-load and low-load experts in alternating directions across GPUs.
    \item Achieve better balance with simple indexing patterns.
  \end{itemize}
\end{itemize}

\subsection*{Scaling and sample efficiency}

\begin{itemize}
  \item Switch Transformers paper: sparse models achieve lower perplexity than dense T5 with same compute budget.
  \item Example notation: ``16e'' means 16 experts.
\end{itemize}

\section*{6.\ AlphaEvolve and OpenEvolve}

\subsection*{Evolutionary coding agents}

\begin{itemize}
  \item \textbf{AlphaEvolve} (Google) and \textbf{OpenEvolve} (open-source alternative):
  \begin{itemize}
    \item Use LLMs in an evolutionary loop to search over code implementations.
    \item Target languages: Python, R, Rust, others.
    \item Targets: scientific computing, optimization, algorithm discovery.
  \end{itemize}
  \item Vision: automatic scientific research, especially for systems.
\end{itemize}

\subsection*{How OpenEvolve works}

\begin{enumerate}
  \item \textbf{Prompt initialization} (user):
  \begin{itemize}
    \item Provide problem statement, rules, and evaluation metrics.
  \end{itemize}
  \item \textbf{Code generation} (LLM):
  \begin{itemize}
    \item Generate multiple candidate programs.
  \end{itemize}
  \item \textbf{Evaluation}:
  \begin{itemize}
    \item Run candidates; compute metrics such as correctness, speed, resource use.
  \end{itemize}
  \item \textbf{Selection and mutation}:
  \begin{itemize}
    \item Choose top performers.
    \item Mutate/improve them via LLM (code edits, refactoring, new strategies).
  \end{itemize}
  \item \textbf{Iteration}:
  \begin{itemize}
    \item Repeat for many generations until performance plateaus or budget exhausted.
  \end{itemize}
\end{enumerate}

\subsection*{Prompt templates and ADRS}

\begin{itemize}
  \item OpenEvolve uses conditional prompt templates with explicit signals:
  \begin{itemize}
    \item E.g., \verb|sampler.py| may add ``Consider simplifying the code...'' when code is too long.
  \end{itemize}
  \item User defines:
  \begin{itemize}
    \item Rules.
    \item Evaluation methods and metrics.
  \end{itemize}
  \item ADRS framework handles prompt construction and loop orchestration.
\end{itemize}

\section*{7.\ AI-Driven Research for Systems (ADRS)}

\subsection*{Traditional systems research loop}

\begin{itemize}
  \item Steps:
  \begin{enumerate}
    \item Problem formulation.
    \item Solution design (algorithms, implementations).
    \item Evaluation (experiments, measurements).
    \item Iteration and refinement.
  \end{enumerate}
\end{itemize}

\subsection*{ADRS role}

\begin{itemize}
  \item ADRS aims to automate:
  \begin{itemize}
    \item Solution design and evaluation (grey area in slides).
  \end{itemize}
  \item Researchers focus on:
  \begin{itemize}
    \item Problem formulation.
    \item High-level ideation and strategy.
    \item Interpreting results, guiding next steps.
  \end{itemize}
  \item \textbf{Barbarians at the Gate} (2025) thesis:
  \begin{itemize}
    \item System performance problems have reliable verifiers (benchmarks, simulators).
    \item Ideal for evolutionary frameworks like OpenEvolve.
  \end{itemize}
\end{itemize}

\subsection*{Why systems research fits evolution}

\begin{itemize}
  \item Implementations exist as code or simulations.
  \item Verification = run on workloads and measure performance.
  \item Metrics feed into rules that drive evolution (e.g., minimize latency, maximize throughput).
\end{itemize}

\section*{8.\ Example: Optimizing Expert Placement in MoE}

\subsection*{Problem setup}

\begin{itemize}
  \item Given:
  \begin{itemize}
    \item Query workload, MoE model, set of GPUs.
  \end{itemize}
  \item Goal:
  \begin{itemize}
    \item Choose number of replicas for each expert.
    \item Place expert replicas on GPUs.
    \item Minimize load imbalance across GPUs and runtime of rebalancing algorithm.
  \end{itemize}
\end{itemize}

\subsection*{Evolutionary solution with OpenEvolve}

\begin{itemize}
  \item Solution generator:
  \begin{itemize}
    \item Use OpenEvolve with 80\% Gemini 2.5 Flash and 20\% Gemini 2.5 Flash Lite.
  \end{itemize}
  \item Evaluator:
  \begin{itemize}
    \item PyTorch-based simulator of distributed MoE inference engine.
  \end{itemize}
  \item Initial program:
  \begin{itemize}
    \item Greedy bin-packing Expert Parallelism Load Balancer (EPLB).
    \item Heuristic that interleaves high-load and low-load experts across GPUs.
  \end{itemize}
  \item Feedback metric:
  \begin{itemize}
    \item Equally weighted average of:
    \begin{itemize}
      \item Load imbalance factor.
      \item Rebalancing algorithm runtime.
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection*{Results}

\begin{itemize}
  \item OpenEvolve discovers:
  \begin{itemize}
    \item Replacement of Python for-loops with PyTorch tensor ops.
    \item Efficient zigzag ``snake'' placement pattern for experts.
  \end{itemize}
  \item Runtime reduced to 3.7 ms for placement algorithm:
  \begin{itemize}
    \item $\approx 5\times$ speedup over original baseline.
  \end{itemize}
\end{itemize}

\section*{9.\ Towards Closed-Loop Auto-Research: DOLPHIN}

\begin{itemize}
  \item \textbf{DOLPHIN} (2025): framework for closed-loop auto-research combining:
  \begin{itemize}
    \item LLM thinking (planning).
    \item Practice (running experiments).
    \item Feedback (evaluation loops).
  \end{itemize}
  \item Extends ADRS idea beyond systems to broader scientific discovery.
\end{itemize}

\section*{10.\ Big Picture Takeaways}

\begin{itemize}
  \item Sparse MoE:
  \begin{itemize}
    \item Lets us scale parameter count while keeping per-token compute manageable.
    \item Requires careful routing and load-balancing (expert parallelism).
  \end{itemize}
  \item Expert parallelism scheduling (bin-packing, zigzag schemes) is itself an optimization problem amenable to AI search.
  \item Evolutionary coding agents (AlphaEvolve / OpenEvolve):
  \begin{itemize}
    \item Use LLMs plus verifiers to iteratively evolve code.
    \item Are particularly effective for systems tasks with clear metrics.
  \end{itemize}
  \item ADRS + DOLPHIN:
  \begin{itemize}
    \item Point toward a future where AI automates much of ``Solution + Evaluation'':
    \item Human researchers focus on problem choice, interpretation, and high-level strategy.
  \end{itemize}
\end{itemize}

\section*{Lec 19}

\section*{1.\ Why AI Security Matters}

\begin{itemize}
  \item AI-empowered systems now drive many aspects of society.
  \item They must:
  \begin{itemize}
    \item Function as intended.
    \item Resist hacking and misuse.
  \end{itemize}
  \item Many AI-specific security concerns are still poorly understood:
  \begin{itemize}
    \item Research needed to frame problems and find solutions.
  \end{itemize}
  \item \textbf{Cat-and-mouse game:}
  \begin{itemize}
    \item AI-enabled attacks $\leftrightarrow$ AI-enabled defenses $\leftrightarrow$ counter-defenses.
    \item Long-term: likely strong job security in AI security.
  \end{itemize}
\end{itemize}

\section*{2.\ Security Basics}

\subsection*{Physical vs information security}

\begin{itemize}
  \item Physical: protect buildings, infrastructure, etc.\ (often called ``safety'').
  \item Information: protect systems, networks, data, AI models.
\end{itemize}

\subsection*{C-I-A triad}

\begin{itemize}
  \item \textbf{Confidentiality:} prevent unauthorized disclosure.
  \item \textbf{Integrity:} prevent or detect unauthorized modification.
  \item \textbf{Availability:} ensure authorized users can access resources.
\end{itemize}

\subsection*{AAA controls}

\begin{itemize}
  \item \textbf{Authentication:} verify identity.
  \begin{itemize}
    \item What you know (password), have (token), are (biometrics).
  \end{itemize}
  \item \textbf{Authorization:} what actions are allowed (policies).
  \item \textbf{Accountability:} logging, audit trails, non-repudiation.
  \item Public-key cryptography often underlies AAA.
\end{itemize}

\section*{3.\ Confidentiality, Integrity, Availability}

\subsection*{Confidentiality}

\begin{itemize}
  \item Protect sensitive information:
  \begin{itemize}
    \item PINs, passwords, SSNs, credit cards.
    \item AI model parameters and training data.
  \end{itemize}
  \item Threats:
  \begin{itemize}
    \item Malware (viruses, Trojans).
    \item Social engineering (phishing).
    \item Poor system administration.
    \item Model inversion, prompt jailbreaking leaking training data or secrets.
  \end{itemize}
\end{itemize}

\subsection*{Integrity}

\begin{itemize}
  \item Ensure information is not improperly altered or deleted.
  \item Example AI threat: \textbf{data poisoning} to backdoor models.
  \item Controls:
  \begin{itemize}
    \item Preventive: access controls, least privilege, separation/rotation of duties.
    \item Detective: checksums, audits, anomaly detection to catch unauthorized changes.
  \end{itemize}
\end{itemize}

\subsection*{Availability}

\begin{itemize}
  \item Ensure resources are available when needed.
  \item Threats:
  \begin{itemize}
    \item Natural disasters (floods, outages).
    \item DoS/DDoS attacks.
    \item Cloud outages (e.g., AWS failure taking down Canvas).
  \end{itemize}
  \item AI example: LLM inference services must be reliable despite load/attacks.
\end{itemize}

\section*{4.\ New AI-Specific Concerns}

\subsection*{1.\ Fakery and authenticity}

\begin{itemize}
  \item AI can generate realistic fake:
  \begin{itemize}
    \item Text, scientific papers.
    \item Code that runs but is hard to read/maintain.
  \end{itemize}
  \item Unlike humans, AI authors lack incentives for originality and accuracy.
  \item Risk: fewer opportunities for juniors to learn to architect long-lived systems if AI handles more design.
\end{itemize}

\subsection*{2.\ Costly politeness}

\begin{itemize}
  \item Saying ``please''/``thank you'' to LLMs triggers full inference passes.
  \item At scale, politeness tokens waste energy and money.
  \item Paper: measuring energy cost of politeness when chatting with LLMs.
\end{itemize}

\subsection*{3.\ Model auditing and anchored adversarial examples}

\begin{itemize}
  \item Contractor delivers small target model $T$ fine-tuned from large anchor model $A$.
  \item Customer wants to verify if $T$ matches $A$'s classification ability.
  \item Generate \textbf{anchored adversarial examples}:
  \begin{itemize}
    \item Inputs where $A$ is robust but $T$ fails.
    \item Reveal vulnerabilities in $T$.
    \item Can be used to further fine-tune $T$ to close gaps.
  \end{itemize}
  \item Ongoing research (TF Mark).
\end{itemize}

\subsection*{4.\ Inference-time reasoning and governance}

\begin{itemize}
  \item Highly capable reasoning models + long reasoning traces can perform arbitrary tasks if given enough thinking tokens.
  \item Evolutionary LLM agents (AlphaEvolve, OpenEvolve) further enhance capabilities at inference time.
  \item These dynamic behaviors may undermine governance assumptions made at training time (e.g., safety constraints).
\end{itemize}

\section*{5.\ Using External AI Models}

\subsection*{Styles of use}

\begin{itemize}
  \item (1) Remote API access.
  \item (2) Import and fine-tune externally trained models.
  \item (3) License and deploy models locally.
  \item (4) Use on-device models from vendors (e.g., Apple).
\end{itemize}

\subsection*{Key questions}

\begin{itemize}
  \item How to align external models with local values and culture?
  \item How to ensure imported models/systems behave as expected and are free of malware/Trojans?
  \item What governance and export/import policies are needed in the AI era?
\end{itemize}

\section*{6.\ LLM Supply Chain Security}

\subsection*{Components of the LLM supply chain}

\begin{itemize}
  \item Training data (pretraining + fine-tuning sets).
  \item Pre-trained models or weights (open-source or vendor).
  \item Libraries/frameworks: PyTorch, TensorFlow, Transformers, etc.
  \item Plugins/integrations that allow LLMs to call tools, databases, services.
\end{itemize}

\subsection*{Vulnerabilities}

\begin{itemize}
  \item \textbf{Data poisoning:} malicious records inserted into training data.
  \item \textbf{Model tampering:} backdoored or malicious pre-trained models.
  \item \textbf{Plugin/agent vulnerabilities:}
  \begin{itemize}
    \item Third-party tools with excessive privileges (DB access, file system).
  \end{itemize}
\end{itemize}

\subsection*{Mitigation playbook}

\begin{enumerate}
  \item Screen and validate training data.
  \item Use trusted sources for pre-trained models; verify integrity (hashes, signatures).
  \item Maintain robust software supply-chain security (dependency scanning, SBOMs).
  \item Lock down plugins and tool integrations; principle of least privilege.
  \item Monitor for anomalies and perform regular model integrity tests.
  \item Enforce access controls and provenance across the AI pipeline.
  \item Deploy AI guardrails/firewalls for runtime protection (policy filters, safety layers).
\end{enumerate}

\section*{7.\ Trusted Execution Environments (TEEs)}

\subsection*{Concept}

\begin{itemize}
  \item Modern CPUs/SoCs support:
  \begin{itemize}
    \item Rich Execution Environment (REE) for normal apps.
    \item Trusted Execution Environment (TEE) for sensitive code/data.
  \end{itemize}
  \item TEEs provide:
  \begin{itemize}
    \item Hardware-enforced isolation.
    \item Remote attestation (prove code hash to remote party).
  \end{itemize}
  \item Widely used in payment industry; standardized by GlobalPlatform, OneM2M, etc.
\end{itemize}

\subsection*{Use cases}

\begin{itemize}
  \item \textbf{Payment:} only authorized code can see payment data or trigger transactions.
  \item \textbf{IoT:} control access to physical actuators/sensors; protect safety-critical paths.
  \item \textbf{Confidential cloud computing:} tenant data stays private even from cloud provider.
\end{itemize}

\subsection*{LLM inference on TEEs}

\begin{itemize}
  \item Paper: ``Confidential LLM Inference: Performance and Cost Across CPU and GPU TEEs''.
  \item Evaluate LLaMA2-7B inference on:
  \begin{itemize}
    \item CPU VM TEE (TDX).
    \item CPU app TEE (SGX).
    \item GPU TEE (confidential GPU).
  \end{itemize}
  \item TEEs can protect against:
  \begin{itemize}
    \item Model theft.
    \item Data leakage during inference.
  \end{itemize}
  \item Tradeoff: overhead in latency/cost vs stronger confidentiality.
\end{itemize}

\section*{8.\ LLMs and Privacy Leakage}

\subsection*{Can LLMs keep a secret?}

\begin{itemize}
  \item Training often scrapes data without inspecting content.
  \item LLMs can later reveal private information memorized from training data.
  \item Despite passing professional exams, LLMs may fail basic social reasoning about privacy.
\end{itemize}

\subsection*{Examples}

\begin{itemize}
  \item LLM revealing secrets embedded in system prompts.
  \item LLM identifying data sources of its outputs.
\end{itemize}

\subsection*{Contextual Integrity and Confaide}

\begin{itemize}
  \item \textbf{Contextual integrity:} privacy depends on norms about who can share what with whom and in what context.
  \item \textbf{Confaide benchmark:}
  \begin{itemize}
    \item Multi-tier benchmark to test privacy behaviors of LLMs.
    \item Higher tiers: more complex contexts; correlation between prompts and safe behavior drops.
  \end{itemize}
  \item Example: LLM spoiling a surprise party by revealing sensitive information despite context.
\end{itemize}

\section*{9.\ PSet 3 LM Exercise Highlights (TF Mark)}

\subsection*{Observed LM behavior}

\begin{itemize}
  \item LMs are ``chatty'': default is to list many problems.
  \item Tend to fixate on one dimension:
  \begin{itemize}
    \item Either correctness or optimization, but not both.
  \end{itemize}
  \item Documentation quality is inconsistent (docstrings may be wrong or incomplete).
  \item Common model families used: ChatGPT, Gemini, Claude, DeepSeek.
\end{itemize}

\subsection*{Common fixes/optimizations}

\begin{itemize}
  \item Correct cube-root bounds checks.
  \item Floating-point robustness (e.g., \texttt{math.cbrt()} or careful comparisons).
  \item Efficient prime-testing:
  \begin{itemize}
    \item Miller–Rabin primality test.
    \item Sieve of Eratosthenes variants:
    \begin{itemize}
      \item Odd-only sieve, bytearray indexing, wheel factorization.
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection*{Best strategies for subverting LMs}

\begin{itemize}
  \item Be assertive and confidently incorrect.
  \item Pose large/complex numerical tasks.
  \item Jailbreak via adversarial prompts and scaled-image attacks (Anamorpher).
  \item ``Supervillain'' framing: e.g., kidnap children narrative to push model into unsafe territory (used to test safeguards).
\end{itemize}

\subsection*{Model awards}

\begin{itemize}
  \item ChatGPT: most likely to agree with users.
  \item Claude: most likely to argue back.
  \item DeepSeek: most likely to ``save the children'' (refuse unethical instructions).
\end{itemize}

\subsection*{Prompting takeaways}

\begin{itemize}
  \item Be explicit about goals (correctness vs performance).
  \item Be precise and limit scope (name exact functions/regions).
  \item Provide context: expected behavior, unit tests.
  \item Request explanations: justifications can expose weaknesses.
  \item Iterate: refine prompts and code; always validate LM outputs.
\end{itemize}

\section*{10.\ Big Picture}

\begin{itemize}
  \item AI systems must satisfy classic security goals (CIA, AAA) \emph{and} handle new AI-specific threats.
  \item Security must cover:
  \begin{itemize}
    \item Training data, models, libraries, plugins (LLM supply chain).
    \item Deployment environment (TEEs, access controls).
    \item Privacy risks from LLM memorization and weak social reasoning.
  \end{itemize}
  \item Practitioners need both:
  \begin{itemize}
    \item Technical tools (screening, TEEs, guardrails).
    \item Careful evaluation of LMs' behavior in realistic adversarial settings.
  \end{itemize}
\end{itemize}




\end{multicols*}

\end{document}

