{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7ttBYeclcLI"
   },
   "source": [
    "# CS2420: Problem Set 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQGm6JtgehIq"
   },
   "source": [
    "> Harvard CS 2420: Computing at Scale (Fall 2025)\n",
    ">\n",
    "> Instructor: Professor HT Kung\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHcaXNxST1p2"
   },
   "source": [
    "### **Assignment Instructions**\n",
    "\n",
    "Read the following instructions carefully before starting the assignment and again before submitting your work:\n",
    "\n",
    "* This problem set must be completed in a team of 3-4 students.  **If you do not have a three or four person group, please email the TFs immediately.**\n",
    "* **We suggest you start right away.** While you will only need to write around **30 lines or fewer of code** per code cell, we expect this assignment to take **3-7 days** of significant effort for a group of three to four (training the networks takes a long time, so don't start at the last second).\n",
    "* The assignment consists of two files: **this Google Colab file** (an `.ipynb` file) and a **LaTeX answer template** (available on Canvas).\n",
    "* The Google Colab contains all assignment instructions and *Code Cells* that you will use to implement the programming components of the assignment (in Python).\n",
    "* We provide a significant amount of the code to make it easier to get started. In the *Code Cells*, please add comments to explain the purpose of each line of code in your implementation. **You will not receive credit for implementations that are not well documented.**\n",
    "* All output from the Colab Notebook **must** be saved. Do not delete code, comment it out if needed. TFs must be able to reproduce your results. **You will not receive credit for implementations that are not reproducible.**\n",
    "* <font color='red'>**Deliverables are highlighted in red**</font> in this Google Colab file. Use the LaTeX answer template to write down answers for these deliverables.\n",
    "* Each group will **submit** a PDF of your answers, any logs from generative AI, and your Google Colab file (.ipynb file) containing all completed *Code cells* to \"Problem Set 2\" on Canvas. Only one submission per group. Check your .ipynb file using this [tool](https://htmtopdf.herokuapp.com/ipynbviewer/) before submitting to ensure that you completed all *Code Cells* (including detailed comments).\n",
    "* The assignment is due on **10/6/2025 at 11:59 PM ET**.\n",
    "\n",
    "\n",
    "-----\n",
    "Outline of this assignment:\n",
    "\n",
    "1. **Getting Started** [20 points]: In this section, you will train a convolutional neural network in PyTorch on the CIFAR-10 image classification dataset. This section will introduce you to the PyTorch machine learning library and training neural networks.\n",
    "\n",
    "\n",
    "2. **Post-training Quantization** [60 points]: In this section, you will perform post-training quantization of weights on a small fully-connected neural network trained on the MNIST classification dataset. This section reinforces your understanding of quantization and will give you a concrete understanding of the limitations of quantization.\n",
    "\n",
    "3. **Conv Pruning** [60 points]:  In this section, you will implement a simplified version of structured filter pruning based on [Pruning Filters for Efficient ConvNets](https://openreview.net/pdf?id=rJqFGTslg) and non-structured pruning based on [Learning both Weights and Connections for Efficient Neural Networks\n",
    "](https://arxiv.org/abs/1506.02626).\n",
    "\n",
    "4. **Parameter Efficient Fine-tuning** [35 points]: In this section, you will fine-tune a ResNet-18 model using three methods for updating the model: 1) updating all of the models parameters 2) freezing earlier layers and updating the later layers 3) a combinations of frozen layers and low rank updates based on [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf).\n",
    "\n",
    "5. **Transformer Architecture** [35 points]: In this section, you will be implementing the individual components of the transformer model based on the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762). This section will provide further intuition on the attention mechanism and provide you a high-level overview of how data flows through the transformer model.\n",
    "\n",
    "6. **Depthwise Separable Convolution** [40 points]: In this section, you will implement depthwise separable convolutions and train a depthwise convolutional neural network on the CIFAR-10 image classification dataset. This section will reinforce your understanding of depthwise convolution operations and improve your PyTorch skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UvFA89jTuON"
   },
   "source": [
    "---\n",
    "\n",
    "### **1. Getting Started**\n",
    "\n",
    "---\n",
    "First, make a copy of this Google Colab by navigating to `File->Save a copy in Drive`. Share the copy with all other group members. You will modify the *Code Cells* in your new copy to complete the assignment.\n",
    "\n",
    "\n",
    "While you are reading through the material, please execute each code cell in order, by either selecting the code cell and pressing the play symbol (▶) on the left side of the code cell or by using the hotkey combination: Shift + Enter.\n",
    "\n",
    "It's important to note that simultaneously running the same Colab notebook on different computers can introduce version conflicts. One method to overcome this is to assign a person in your group to control the notebook while others work on another (copied) notebook to test out their code.\n",
    "\n",
    "\n",
    "*Code Cell 1.0* imports libraries used throughout the assignment and *Code Cell 1.1* defines model testing and training functions. We recommend reading through and understanding *Code Cell 1.1* before working through the rest of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRz6CSs4LwBK"
   },
   "outputs": [],
   "source": [
    "# nvidia-smi allows you to check if a GPU device has been enabled in Colab\n",
    "!nvidia-smi\n",
    "\n",
    "# If you did not see a GPU device after running the above command, go to:\n",
    "# 'Runtime --> Change runtime type --> Hardware accelerator' and select 'gpu'.\n",
    "# Then, click 'Runtime --> restart runtime'.\n",
    "\n",
    "# You should use a GPU device for training models as it will *significantly*\n",
    "# accelerate training compared to using a CPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9WL6HA_Lpe8"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 1.0: Imports\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFxb3jQlh1Dy"
   },
   "source": [
    "## Tested Environment\n",
    "\n",
    "We have tested Problem Set 2 under the following setup:\n",
    "\n",
    "| Library     | Package Version | CUDA Version | Display Version  |\n",
    "|-------------|-----------------|--------------|------------------|\n",
    "| PyTorch     | 2.8.0           | 12.6         | 2.8.0+cu126      |\n",
    "| Torchvision | 0.23.0          | 12.6         | 0.23.0+cu126     |\n",
    "\n",
    "You may use the same versions of PyTorch and Torchvision for your Problem Set 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeYR2zLEM1BH"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 1.1: Training/Test Functions\n",
    "\n",
    "# tracks the highest accuracy observed so far\n",
    "best_acc = 0\n",
    "\n",
    "def moving_average(a, n=100):\n",
    "    '''Helper function used for visualization'''\n",
    "    ret = torch.cumsum(torch.Tensor(a), 0)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def train(net, epoch, loader, criterion, optimizer, loss_tracker = [], acc_tracker = []):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        # update optimizer state\n",
    "        optimizer.step()\n",
    "        # compute average loss\n",
    "        train_loss += loss.item()\n",
    "        loss_tracker.append(loss.item())\n",
    "        loss = train_loss / (batch_idx + 1)\n",
    "        # compute accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        acc = 100. * correct / total\n",
    "        # Print status\n",
    "        sys.stdout.write(f'\\rEpoch {epoch}: Train Loss: {loss:.3f}' +\n",
    "                         f'| Train Acc: {acc:.3f}')\n",
    "        sys.stdout.flush()\n",
    "    acc_tracker.append(acc)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def test(net, epoch, loader, criterion, loss_tracker = [], acc_tracker = []):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            loss_tracker.append(loss.item())\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            loss = test_loss / (batch_idx + 1)\n",
    "            acc = 100.* correct / total\n",
    "    sys.stdout.write(f' | Test Loss: {loss:.3f} | Test Acc: {acc:.3f}\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    acc_tracker.append(acc)\n",
    "    if acc > best_acc:\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ovGy7tEjIWR"
   },
   "source": [
    "**Loading the Train and Test Datasets**\n",
    "\n",
    "For this assignment, we will use the CIFAR-10 dataset. It contains 10 object classes, where each sample is a color image (RGB channels) with a spatial resolution of 32 x 32 pixels. More details here: https://www.cs.toronto.edu/~kriz/cifar.html. *Code Cell 1.2* will take 1-2 minutes to execute as it downloads the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kgff5IPPMgFG"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 1.2\n",
    "\n",
    "# Load training data\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# Load testing data\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False,\n",
    "                                         num_workers=2)\n",
    "print('Finished loading datasets!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBa9meHqk3c4"
   },
   "source": [
    "**Constructing our Convolutional Neural Network (CNN)**\n",
    "\n",
    "For this assignment, we will use a 10-layer CNN which we call `ConvNet` that is provided in *Code Cell 1.3*. The CNN has 9 convolutional layers (`nn.Conv2d`) followed by 1 fully connected (`nn.Linear`) layer. The Batch Normalization layers (`nn.BatchNorm2d`) help make the training process more stable and the ReLU layers (`nn.ReLU`) are the non-linear activation functions required for learning when stacking multiple convolutional layers together.\n",
    "\n",
    "In this assignment, you will modify `ConvNet` to implement Depthwise Separable Convolutions (Section 2) and Pruning (Section 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOTGlfOG5IAi"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 1.3\n",
    "\n",
    "def conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n",
    "               padding=1):\n",
    "    '''\n",
    "    A nn.Sequential layer executes its arguments in sequential order. In\n",
    "    this case, it performs Conv2d -> BatchNorm2d -> ReLU. This is a typical\n",
    "    block of layers used in Convolutional Neural Networks (CNNs). The\n",
    "    ConvNet implementation below stacks multiple instances of this three layer\n",
    "    pattern in order to achieve over 90% classification accuracy on CIFAR-10.\n",
    "    '''\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n",
    "                  bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    '''\n",
    "    A 9 layer CNN using the conv_block function above. Again, we use a\n",
    "    nn.Sequential layer to build the entire model. The Conv2d layers get\n",
    "    progressively larger (more filters) as the model gets deeper. This\n",
    "    corresponds to spatial resolution getting smaller (via the stride=2 blocks),\n",
    "    going from 32x32 -> 16x16 -> 8x8. The nn.AdaptiveAvgPool2d layer at the end\n",
    "    of the model reduces the spatial resolution from 8x8 to 1x1 using a simple\n",
    "    average across all the pixels in each channel. This is then fed to the\n",
    "    single fully connected (linear) layer called classifier, which is the output\n",
    "    prediction of the model.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            conv_block(3, 32),\n",
    "            conv_block(32, 32),\n",
    "            conv_block(32, 64, stride=2),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 128, stride=2),\n",
    "            conv_block(128, 128),\n",
    "            conv_block(128, 256),\n",
    "            conv_block(256, 256),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "            )\n",
    "\n",
    "        self.classifier = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        The forward function is called automatically by the model when it is\n",
    "        given an input image. It first applies the 8 convolution layers, then\n",
    "        finally the single classifier layer.\n",
    "        '''\n",
    "        h = self.model(x)\n",
    "        B, C, _, _ = h.shape\n",
    "        h = h.view(B, C)\n",
    "        return self.classifier(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqFOd7_FUJsd"
   },
   "source": [
    "**Training ConvNet on the CIFAR-10 Dataset**\n",
    "\n",
    "Now that we have loaded our training and testing datasets, and created our model, it is time to train the model. This training process will take around 15 seconds per epoch (an epoch is an entire pass through the training dataset). Usually, we need to train models for many epochs in order to achieve good classification accuracy. For this assignment, we will train most models for 100 epochs. This means that the training process takes roughly 15 seconds * 100 = 25 minutes. **Please do not close or refresh the Colab instance during training and make sure to export results if you plan on using them in the future.** Note that the Google Colab instance is non-persistent, so if the session is left idle for a period of time (such as 30-minutes), the state of the machine will be lost. When this happens, you must execute all Code Cells, in order, up to your current point of progress. To avoid this, you may want to ensure that your session is kept alive by not leaving it idle.\n",
    "\n",
    "*Code Cell 1.4* provides a `train` function that will perform one epoch worth of training each time it is called. The `test` function will evaluate the performance of the model on the held-out test set.\n",
    "\n",
    "\n",
    "---\n",
    "<font color='red'>**PART 1.1:**</font> [5 points]\n",
    "\n",
    "<font color='red'>**Deliverables**</font>\n",
    "1. In *Code Cell 1.4*, set the number of epochs (`epochs`) to `5`. Train the model 3 separate times, with learning rate (`lr`) set to `0.0001`, `0.1`, and `1.0`.\n",
    "2. For each model run over 5 epochs, plot the training loss (`train_loss_tracker`) and test accuracy (`test_acc_tracker`). For the training loss, apply the provided `moving_average` function on `train_loss_tracker` before plotting to smooth out the loss curve.\n",
    "\n",
    "3. Plot the training loss and test accuracy for each run with the different learning rates `0.0001`, `0.1`, and `1.0`. (x-axis is epoch)\n",
    "4. Describe the difference in trends for each learning rate run. Which learning rate seemed to work best? Explain why you think it did best relative to the other learning rates. (100 words maximum)\n",
    "---\n",
    "\n",
    "\n",
    "<font color='red'>**PART 1.2:**</font> [10 points]\n",
    "* Set the learning rate (`lr`) to the best setting observed in <font color='red'>**PART 1.1**</font>.\n",
    "* Change the number of epochs (`epochs`) from `5` to `100`.\n",
    "\n",
    "Since you are training the model for 100 epochs, it will take roughly 25-30 minutes to complete. We use a deterministic random seed for weight initialization to remove variability between runs.\n",
    "\n",
    "Adjusting the learning rate during training is a common strategy used to improve the final accuracy of the CNN. We will compare two different methods (*MultiStep* and *CosineAnnealing*) to adjust the learning rate.  \n",
    "Please check the PyTorch documentation for [MultiStepLR](https://pytorch.org/docs/stable/optim.html?highlight=multistep#torch.optim.lr_scheduler.MultiStepLR) and [CosineAnnealingLR](https://pytorch.org/docs/stable/optim.html?highlight=cosineannealinglr#torch.optim.lr_scheduler.CosineAnnealingLR) to determine how to use them.\n",
    "\n",
    "<font color='red'>**Deliverables**</font>\n",
    "\n",
    "\n",
    "1. First try the [MultiStepLR](https://pytorch.org/docs/stable/optim.html?highlight=multistep#torch.optim.lr_scheduler.MultiStepLR) learning rate scheduler. Set `lr_scheduler` (*Line 96 in Code Cell 1.4*) as 'multistep'. In addition, you also need to set the milestones (`milestones`) used in the scheduler to decrease the learning rate by a factor of 10 every 25 epochs. Train the network for 100 epochs and plot the training loss and test accuracy.\n",
    "2. Then try the [CosineAnnealingLR](https://pytorch.org/docs/stable/optim.html?highlight=cosineannealinglr#torch.optim.lr_scheduler.CosineAnnealingLR) learning rate scheduler. Set `lr_scheduler` (*Line 96 in Code Cell 1.4*) as 'cosine_annealing'. Train the network for 100 epochs and plot the training loss and test accuracy (where the x-axis is the number of epochs).\n",
    "3. Describe the trends of the learning curves with *MultiStep* and *CosineAnnealing* respectively. (50 words maximum)\n",
    "4. Record and report the provided total running time.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKLF1PXMPg4p"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 1.4\n",
    "\n",
    "torch.manual_seed(43) # to give stable randomness\n",
    "\n",
    "device = 'cuda'\n",
    "net = ConvNet()\n",
    "net = net.to(device)\n",
    "\n",
    "# PART 1.1: set the learning rate (lr) used in the optimizer.\n",
    "lr =\n",
    "\n",
    "# PART 1.1: Modify this to train for a short 5 epochs\n",
    "# PART 1.2: Modify this to train a longer 100 epochs\n",
    "epochs =\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9,\n",
    "                            weight_decay=5e-4)\n",
    "\n",
    "# PART 1.2: try different learning rate scheduler\n",
    "scheduler_name=    # set this to 'multistep' or 'cosine_annealing'\n",
    "if scheduler_name=='multistep':\n",
    "    milestones =\n",
    "    gamma      =\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                    milestones=milestones,\n",
    "                                                    gamma=gamma)\n",
    "elif scheduler_name=='cosine_annealing':\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "else:\n",
    "    NotImplementedError\n",
    "\n",
    "# Records the training loss and training accuracy during training\n",
    "train_loss_tracker, train_acc_tracker = [], []\n",
    "\n",
    "# Records the test loss and test accuracy during training\n",
    "test_loss_tracker, test_acc_tracker = [], []\n",
    "\n",
    "print('Training for {} epochs, with learning rate {} and milestones {}'.format(\n",
    "      epochs, lr, milestones))\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(0, epochs):\n",
    "    # train(net, epoch, train_loss_tracker, train_acc_tracker, trainloader)\n",
    "    # test(net, epoch, test_loss_tracker, test_acc_tracker, testloader)\n",
    "    train(net=net, epoch=epoch, loader=trainloader, criterion=criterion, optimizer=optimizer, loss_tracker=train_loss_tracker, acc_tracker=train_acc_tracker)\n",
    "    test(net=net, epoch=epoch, loader=testloader, criterion=criterion, loss_tracker=test_loss_tracker, acc_tracker=test_acc_tracker)\n",
    "    scheduler.step()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('Total training time: {} seconds'.format(total_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iNceGsSDzHA"
   },
   "source": [
    "---\n",
    "\n",
    "### **2. Post-training Quantization**\n",
    "\n",
    "---\n",
    "\n",
    "In this section, you will perform post-training quantization of weights on a multi-layer perceptron with fully connected layers to reduce its memory requirements.\n",
    "We will start by training a small neural network on the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. Then, we will quantize its weights and verify that performance does not degrade significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHoj5ReoErRy"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 2.1\n",
    "\n",
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "# Run this code cell to train MNIST neural network. Do not modify!\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Train an MNIST neural network. Run this code cell.\n",
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self, hidden=128):\n",
    "      super(MNISTNet, self).__init__()\n",
    "\n",
    "      # First 2D convolutional layer, taking in 1 input channel (image),\n",
    "      # outputting 32 convolutional features, with a square kernel size of 3\n",
    "      self.hidden = hidden\n",
    "      self.fc1 = nn.Linear(28*28*1, self.hidden)\n",
    "      self.fc2 = nn.Linear(self.hidden, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = x.view(-1, 28*28)\n",
    "      x = self.fc1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.fc2(x)\n",
    "      return F.log_softmax(x, dim=1)\n",
    "\n",
    "def train_mnist(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test_mnist(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "model = MNISTNet()\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1_mnist = datasets.MNIST('./data_mnist', train=True, download=True,\n",
    "                    transform=transform)\n",
    "dataset2_mnist = datasets.MNIST('./data_mnist', train=False,\n",
    "                    transform=transform)\n",
    "train_loader_mnist = torch.utils.data.DataLoader(dataset1_mnist)\n",
    "test_loader_mnist = torch.utils.data.DataLoader(dataset2_mnist)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=.1)\n",
    "for epoch in range(1, 5):\n",
    "    train_mnist(model, device, train_loader_mnist, optimizer, epoch)\n",
    "    test_mnist(model, device, test_loader_mnist)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbcPLUGvlVEf"
   },
   "source": [
    "---\n",
    "<font color='red'>**PART 2.1:**</font> [20 points]\n",
    "\n",
    "We will perform post-training quantization on the weights of a neural network. Recall from lecture that quantization maps high-precision values to low-precision values to reduce the number of bits used to store each value. In this section you will implement two functions:\n",
    "* **quantize**: compresses a vector/matrix from 32-bit float to 8-bit integer\n",
    "* **dequantize**: takes the output of **quantize** and recovers the original values (possibly with some error).\n",
    "\n",
    "**Quantization Example**\n",
    "\n",
    "Given a vector of 32-bit floats containing the values `[400, 500, 600]`, our goal is to quantize these values so that they are representable in 8 bits. With 8 bits, we can only represent the values 0 through 255 -- but we have a problem: 400, 500 and 600 are outside of this range! To solve this, we want to map the values of the vector such that each value is *approximately* representable by values between 0 and 255. To do this we:\n",
    "1. Shift the vector so that the minimum value is 0\n",
    "2. Scale the vector so that every value is between 0 and 255\n",
    "3. Cast each value to an (8-bit) integer\n",
    "\n",
    "For example, suppose we are quantizing `v=[400,500,600]`.\n",
    "* After shifting would have `shift=400`, `v_shifted=[0,100,200]`.\n",
    "* After scaling, we would have `scale=255/200`, `shift=400`, `v_shifted_scaled=[0*255/200, 100 *255/200, 200 *255/200] = [0, 127.5, 255]`, giving us `v_q=[0, 128, 255]` after rounding.\n",
    "\n",
    "With this process we have successfully quantized the elements of `v` to be representable by 8 bits (with additional `scale` and shift `parameters`). This yields ~4x reduction in size for large vectors!\n",
    "\n",
    "Remember, casting should be done at the very last step (after rounding). Please keep this in mind when implementing your functions below!\n",
    "\n",
    "**Dequantization Example**\n",
    "\n",
    "To dequantize, we do the opposite:\n",
    "* Given: `v_q=[0,128,255]`, `shift=400`, `scale=255/200`\n",
    "* Reverse scale: `v_unscaled = v_q / scale = [0, 100.392, 200]`\n",
    "* Reverse shift: `v_unscaled + 400 = [400, 500.392, 600]`\n",
    "\n",
    "As you can see, our dequantized values are very close to our original values (`[400,500,600]`), but there is some quantization error due to loss of information during quantization. As you will see in the next section, neural networks may tolerate these errors and still attain high accuracy.\n",
    "\n",
    "<font color='red'>**Deliverables**</font>\n",
    "\n",
    "1. In *Code Cell 2.2*, implement the `quantize` function, which takes a 1D or 2D NumPy array and quantizes each element to be representable with 8 bits (i.e., with an integer value between 0 and 255). The function should return a NumPy array with `dtype=uint8`. Keep in mind that the quantized values will need to be dequantized (converted back to a non-quantized form), so you may also return any other value you may find useful for dequantization.\n",
    "\n",
    "2. In *Code Cell 2.2*, implement the `dequantize` function which takes in a NumPy array with `dtype=uint8` (and any other parameters you think necessary), and attempts to recover the values from before quantization. Because quantizing to 8 bits loses information, you may not necessarily obtain the exact weight values from before quantization. Ideally, the quantization error is small enough to maintain good inference accuracy.\n",
    "\n",
    "3. In *Code Cell 2.2*, verify that both your `quantize` and `dequantize` functions work by making sure it passes the test case. The test case checks that your quantization function achieves a 4x reduction in memory and asserts that the dequantized data is within some error threshold of the original matrix. You should see \"Success!\" if your method passes a test case.\n",
    "\n",
    "4. How much smaller (in bits) is the quantized data than the original? (Remember to account for the extra arguments you added!) Is it 4x? If not, why might this be? (50 words maximum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfs2zxo1nBgz"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 2.2\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def quantize(W):\n",
    "    # PART 2.1: Implement!\n",
    "    # Hint: you may return extra parameters\n",
    "    # Hint: our solution is 3-5 lines of code\n",
    "    # PART 2.2: Implement!\n",
    "    pass\n",
    "\n",
    "def dequantize(W, extra_args):\n",
    "    # PART 2.1 Implement!\n",
    "    # Hint: you may pass extra args through the extra_args parameter to assist dequantization\n",
    "    # Hint: our solution is 3-5 lines of code\n",
    "    pass\n",
    "\n",
    "def test_cases():\n",
    "    def count_bytes(x):\n",
    "        return sys.getsizeof(x)\n",
    "    def did_pass(x, thresh=2):\n",
    "        bytes_original = count_bytes(x.astype(np.float32))\n",
    "        quantized, args = quantize(x)\n",
    "        bytes_quantized = count_bytes(quantized)\n",
    "        dequantized = dequantize(quantized, args)\n",
    "\n",
    "        smaller = bytes_quantized <= bytes_original//2.3\n",
    "        mean_err = np.mean(np.abs(dequantized-x))\n",
    "        accurate = mean_err < thresh\n",
    "\n",
    "        if not smaller:\n",
    "            print(\"FAIL: Size original %d vs quantized %d\" % (bytes_original, bytes_quantized))\n",
    "            return False\n",
    "        if not accurate:\n",
    "            print(\"FAIL: Mean error %f above threshold %f\" % (mean_err, thresh))\n",
    "            return False\n",
    "        print(\"Success!\")\n",
    "        return True\n",
    "\n",
    "    did_pass(np.array([i for i in range(100)]))\n",
    "    did_pass(np.array([i*.5 for i in range(100)]))\n",
    "    did_pass(np.random.normal(0, 3, size=(100, 100)))\n",
    "    did_pass(np.random.uniform(0, 10, size=(100, 100)))\n",
    "    did_pass(np.random.uniform(5, 10, size=(100, 100)))\n",
    "    did_pass(np.random.uniform(-5, -10, size=(100, 100)))\n",
    "    did_pass(np.random.normal(-10, 1, size=(100, 100)))\n",
    "    did_pass(np.random.uniform(100, 100+255, size=(20, 40)))\n",
    "test_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4riSLFTjtKzg"
   },
   "source": [
    "---\n",
    "<font color='red'>**PART 2.2:**</font> [20 points]\n",
    "\n",
    "We will now evaluate the performance of the neural network with quantization.\n",
    "\n",
    "<font color='red'>**Deliverables**</font>\n",
    "\n",
    "1. Quantize each parameter of the model to 8 bits and dequantize (this injects simulated quantization error into the parameters). Then, create a new model from the dequantized parameters.\n",
    "2. Evaluate the above model and report the test accuracy. What was the drop/increase in accuracy versus the original full-precision model? (50 words maximum)\n",
    "3. Quantize to 4 bits instead of 8 and dequantize. What accuracy is achieved from these new 4-bit quantized-and-dequantized parameters? What was the drop/increase in accuracy versus the original full-precision model? (50 words maximum)\n",
    "4. Quantize to 2 bits instead of 4 and dequantize. What accuracy is achieved from these new 2-bit quantized-and-dequantized parameters? What was the drop/increase in accuracy versus the original full-precision model? (50 words maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySO8bbl3tJ7m"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 2.3\n",
    "\n",
    "print(\"Baseline Score\")\n",
    "test_mnist(model, device, test_loader_mnist)\n",
    "\n",
    "# PART 2.2: Quantize parameters!\n",
    "state_dict = model.state_dict()\n",
    "state_dict_q = {}\n",
    "pass\n",
    "\n",
    "print(\"Quantized performance\")\n",
    "test_mnist(model, device, test_loader_mnist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5Vyks9FnD4w"
   },
   "source": [
    "---\n",
    "<font color='red'>**PART 2.3:**</font> [20 points]\n",
    "\n",
    "We will now evaluate the performance of the neural network with Stochastic Rounding.\n",
    "\n",
    "<font color='red'>**Deliverables**</font>\n",
    "\n",
    "1. Modify the `quantize` function in *Code Cell 2.2* to implement **Stochastic Rounding** instead of standard Banker's Rounding. Quantize each parameter of the model to 2 bits and dequantize. Repeat the experiment 10 times in Code Cell 2.4 and answer the following questions. What accuracy is achieved from these new Stochastic Rounded parameters? What was the drop/increase in accuracy versus the original 2-bit model that uses standard Banker's Rounding? (50 words maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCeH6ejQwm_F"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 2.4\n",
    "\n",
    "# PART 2.3: Modify Code Cell 2.2 to implement Stochastic Rounding\n",
    "# TODO: Use Code Cell 2.3 as a reference to evaluate the performance of Stochastic Rounding\n",
    "\n",
    "# PART 2.3 Implement!\n",
    "\n",
    "print(\"Quantized performance\")\n",
    "test_mnist(model, device, test_loader_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myU7s113oeLS"
   },
   "source": [
    "---\n",
    "\n",
    "### **3. Structured and Non-structured Filter Pruning**\n",
    "\n",
    "---\n",
    "\n",
    "In this section, you will implement a simplified version of structured filter pruning proposed in [Pruning Filters for Efficient ConvNets](https://openreview.net/pdf?id=rJqFGTslg). Instead of pruning weights, this paper describes removing whole filters from each convolutional layer in a CNN. Compared to pruning weights across the network, filter pruning is a naturally structured pruning method that does not introduce irregular sparsity. Therefore, it does not require using sparse libraries or specialized hardware.\n",
    "For each convolutional layer, we measure each filter’s relative importance by its absolute weight sum $\\sum|\\mathcal{F}_{i,j}|$ (i.e., its $\\ell_1$-norm). When pruning a layer, $m$ filters with the smallest relative importance will be pruned, where $m$ = (prune percentage $\\times$ total number of filters in this layer).\n",
    "\n",
    "In *Code Cell 3.1*, we provide a `SparseConv2d` layer, which is similar to the standard `nn.Conv2d` layer, but adds a `mask` tensor. The `mask` is used to zero out filters in this layer before the convolution operation is performed. Initially, `mask` is set to all ones, meaning the `SparseConv2d` layer's behavior will be identical to the `nn.Conv2d` layer.\n",
    "\n",
    "Besides structured pruning, we will also implement non-structured pruning proposed in [Learning both Weights and Connections for Efficient Neural Networks\n",
    "](https://arxiv.org/abs/1506.02626) for comparsion. Non-structured pruning is more flexible than structured pruning, and allows irregular sparsity in the weight tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76ay4wFg7vWZ"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 3.1\n",
    "\n",
    "\n",
    "def _make_pair(x):\n",
    "    if hasattr(x, '__len__'):\n",
    "        return x\n",
    "    else:\n",
    "        return (x, x)\n",
    "\n",
    "class SparseConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                     padding=1):\n",
    "        super(SparseConv2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = _make_pair(stride)\n",
    "        self.padding = _make_pair(padding)\n",
    "\n",
    "        # initialize weights of this layer\n",
    "        self._weight = nn.Parameter(torch.randn([self.out_channels, self.in_channels,\n",
    "                                                        self.kernel_size, self.kernel_size]))\n",
    "        stdv = 1. / math.sqrt(in_channels)\n",
    "        self._weight.data.uniform_(-stdv, stdv)\n",
    "        # initialize mask\n",
    "        # Since we are going to zero out the whole filter, the number of\n",
    "        # elements in the mask is equal to the number of filters.\n",
    "        self.register_buffer('_mask', torch.ones(out_channels))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.conv2d(x, self.weight, stride=self.stride,\n",
    "                        padding=self.padding)\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        # check out https://pytorch.org/docs/stable/notes/broadcasting.html\n",
    "        # to better understand the following line\n",
    "        return self._mask[:,None,None,None] * self._weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1e8105P8zLd"
   },
   "source": [
    "---\n",
    "<font color='red'>**PART 3.1:**</font> [5 points]\n",
    "\n",
    "<font color='red'>**Deliverables**</font>\n",
    "\n",
    "1. In *Code Cell 3.2*, implement `SparseConvNet` using the `sparse_conv_block` in a similar fashion to *Code Cell 1.3*. Replace all the `nn.Conv2d` layers in `ConvNet` with the `SparseConv2d` layers provided in *Code Cell 3.1*. Use the following table to complete the network:\n",
    "\n",
    "|Layer #|in_channels|out_channels|stride|\n",
    "|-------|-----------|------------|------|\n",
    "|1|3|32|1|\n",
    "|2|32|32|1|\n",
    "|3|32|64|2|\n",
    "|4|64|64|1|\n",
    "|5|64|64|1|\n",
    "|6|64|128|2|\n",
    "|7|128|128|1|\n",
    "|8|128|256|1|\n",
    "|9|256|256|1|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvtFRYWY8z7O"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 3.2\n",
    "\n",
    "def sparse_conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n",
    "                      padding=1):\n",
    "    '''\n",
    "    Replaces 3x3 nn.Conv2d with 3x3 SparseConv2d\n",
    "    '''\n",
    "    return nn.Sequential(\n",
    "        SparseConv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "class SparseConvNet(nn.Module):\n",
    "    '''\n",
    "    A 9 layer CNN using the sparse_conv_block function above.\n",
    "    PART 3.1: Implement!\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(SparseConvNet, self).__init__()\n",
    "        # PART 3.1: Implement!\n",
    "        # self.model = nn.Sequential(...)\n",
    "\n",
    "        self.classifier = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        PART 3.1: Implement!\n",
    "        '''\n",
    "        h = self.model(x)\n",
    "        B, C, _, _ = h.shape\n",
    "        h = h.view(B, C)\n",
    "        return self.classifier(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVn51h0nCB4j"
   },
   "source": [
    "---\n",
    "<font color='red'>**PART 3.2:**</font> [5 points]\n",
    "\n",
    "<font color='red'>**Deliverables**</font>\n",
    "1. Using *Code Cell 3.3*, train `SparseConvNet` for 5 epochs with a learning rate of $0.1$. Confirm that this performance is approximately equal to what you observed in <font color='red'>**PART 1.1**</font>. (Note that this current model is not sparse, as no pruning has yet been performed. You will implement pruning in the next part. The purpose here is to validate that the performance is the same as the standard convolution.)\n",
    "2. Plot the training error and test accuracy of `SparseConvNet` trained over 5 epochs. (x-axis is epoch)\n",
    "---\n",
    "\n",
    "\n",
    "<font color='red'>**PART 3.3:**</font> [25 points]\n",
    "\n",
    "<font color='red'>**Deliverables**</font>\n",
    "1. Implement the `filter_l1_pruning` function in *Code Cell 3.3* and set the pruning schedule (using `prune_percentage` and `prune_epoch`) to prune an additional 10% filters every 10 epochs, starting at epoch 10, ending at epoch 50. By the end, you should achieve 50% sparsity for each convolution layer in the CNN. For simplicity, you do not need to prune the `nn.Linear` layer, which is the final layer in the model.\n",
    "2. Train `SparseConvNet` for 100 epochs using the same learning rate and `MultiStep` learning rate schedule as in <font color='red'>**PART 1.2**</font>.\n",
    "3. Compare the test accuracy curves against the baseline `ConvNet` model from <font color='red'>**PART 1.2**</font> in a single plot.\n",
    "4. Describe any observations in trends of test accuracy related to the pruning stages. (150 words maximum)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tstv8XasCR02"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 3.3\n",
    "\n",
    "torch.manual_seed(43) # to give stable randomness\n",
    "\n",
    "def get_sparse_conv2d_layers(net):\n",
    "    '''\n",
    "    Helper function which returns all SparseConv2d layers in the net.\n",
    "    Use this below to implement layerwise pruning.\n",
    "    '''\n",
    "    sparse_conv_layers = []\n",
    "    for layer in net.children():\n",
    "        if isinstance(layer, SparseConv2d):\n",
    "            sparse_conv_layers.append(layer)\n",
    "        else:\n",
    "            child_layers = get_sparse_conv2d_layers(layer)\n",
    "            sparse_conv_layers.extend(child_layers)\n",
    "\n",
    "    return sparse_conv_layers\n",
    "\n",
    "def filter_l1_pruning(net, prune_percent):\n",
    "    for i, layer in enumerate(get_sparse_conv2d_layers(net)):\n",
    "        num_nonzero = layer._mask.sum().item()\n",
    "        num_total = len(layer._mask)\n",
    "        num_prune = round(num_total * prune_percent)\n",
    "        sparsity = 100.0 * (1 - (num_nonzero / num_total))\n",
    "        print(num_prune, num_total, prune_percent)\n",
    "\n",
    "        # PART 3.3: Implement pruning by settings elements in layer._mask\n",
    "        #           to zero corresponding to the smallest l1-norm filters\n",
    "        #           in layer._weight. Note: to update variable such as\n",
    "        #           layer._mask and layer._weight, do the following:\n",
    "        #           layer._mask.data[idx] = 0\n",
    "        pass\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "net = SparseConvNet()\n",
    "net = net.to(device)\n",
    "\n",
    "# Set these parameters based on PART 1.2\n",
    "lr =\n",
    "milestones =\n",
    "\n",
    "# PART 3.3: Set this prune an additional 10% every 10 epochs, starting at\n",
    "#           epoch 10, ending at epoch 50. By the end, you should achieve\n",
    "#           50% sparsity for each convolution layer in the CNN. Current\n",
    "#           paramaters indicate 10% pruning at the end of epoch 0.\n",
    "prune_percentage =\n",
    "prune_epoch =\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9,\n",
    "                            weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                 milestones=milestones,\n",
    "                                                 gamma=0.1)\n",
    "\n",
    "train_loss_tracker, train_acc_tracker = [], []\n",
    "test_loss_tracker, test_acc_tracker = [], []\n",
    "\n",
    "print('Training for {} epochs, with learning rate {} and milestones {}'.format(\n",
    "      epochs, lr, milestones))\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(0, epochs):\n",
    "    train(net=net, epoch=epoch, loader=trainloader, criterion=criterion, optimizer=optimizer, loss_tracker=train_loss_tracker, acc_tracker=train_acc_tracker)\n",
    "\n",
    "    if epoch == prune_epoch:\n",
    "        print('Pruning at epoch {}'.format(epoch))\n",
    "        filter_l1_pruning(net, prune_percentage)\n",
    "        # unstructured_pruning(net, prune_percentage)\n",
    "\n",
    "    test(net=net, epoch=epoch, loader=testloader, criterion=criterion, loss_tracker=test_loss_tracker, acc_tracker=test_acc_tracker)\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('Total training time: {} seconds'.format(total_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_HZvA3VqlNa"
   },
   "source": [
    "---\n",
    "<font color='red'>**PART 3.4:**</font> [25 points]\n",
    "\n",
    "<font color='red'>**Deliverables**</font>\n",
    "1. Use the `SparseConv2d` (the layer using *unstructured pruning*) in *Code Cell 3.4* to replace the `SparseConv2d` (the layer using *structured pruning*) in *Code Cell 3.1*.\n",
    "2. Implement the `unstructured_pruning` function in *Code Cell 3.4* and use it to replace the `filter_l1_pruning` function in *Code Cell 3.3*. Or make a copy of *Code Cell 3.3*.\n",
    "3. Re-run training with `unstructured_pruning` (i.e., *Code Cell 3.1, 3.2, 3.3*). You may make copies of those code cells if that is easier for you.\n",
    "4. Compare the 3 test accuracy curves among the baseline ConvNet model from <font color='red'>**PART 1.2**</font>, the structured pruning model from <font color='red'>**PART 3.3**</font>, and the unstructured pruning model from <font color='red'>**PART 3.4**</font> in a single plot.\n",
    "5. Describe any observations in the comparison of structured and unstructured pruning. (150 words maximum)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jbU5kiBvLxS"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 3.4\n",
    "\n",
    "class SparseConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                     padding=1):\n",
    "        super(SparseConv2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = _make_pair(stride)\n",
    "        self.padding = _make_pair(padding)\n",
    "\n",
    "        # initialize weights of this layer\n",
    "        self._weight = nn.Parameter(torch.randn([self.out_channels, self.in_channels,\n",
    "                                                        self.kernel_size, self.kernel_size]))\n",
    "        stdv = 1. / math.sqrt(in_channels)\n",
    "        self._weight.data.uniform_(-stdv, stdv)\n",
    "        # initialize mask\n",
    "        # Since we are going to zero out the whole filter, the number of\n",
    "        # elements in the mask is equal to the number of filters.\n",
    "        self.register_buffer('_mask', torch.ones_like(self._weight))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.conv2d(x, self.weight, stride=self.stride,\n",
    "                        padding=self.padding)\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self._mask * self._weight\n",
    "\n",
    "\n",
    "def unnstructured_pruning(net, prune_percent):\n",
    "    for i, layer in enumerate(get_sparse_conv2d_layers(net)):\n",
    "        num_nonzero = layer._mask.sum().item()\n",
    "        num_total = layer._mask.numel()\n",
    "        num_prune = round(num_total * prune_percent)\n",
    "        sparsity = 100.0 * (1 - (num_nonzero / num_total))\n",
    "        print(num_prune, num_total, prune_percent)\n",
    "\n",
    "        # PART 3.4: Implement pruning by settings elements in layer._mask\n",
    "        #           to zero corresponding to weights with the smallest magnitude\n",
    "        #           in layer._weight.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxR1xQbEC0rX"
   },
   "source": [
    "---\n",
    "\n",
    "### **4. Parameter Efficient Fine-tuning**\n",
    "\n",
    "---\n",
    "\n",
    "In this section, you will be modifying a model to enable parameter efficient fine-tuning (PEFT).\n",
    "We will combine two techniques: freezing portions of a model, and applying low-rank updates.\n",
    "\n",
    "\n",
    "Specifically, you will using the library from [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685).\n",
    "We recommend reading Section 4 as it describes the method in detail.\n",
    "LoRA can decrease the computation and memory requirements during training by reducing the number of parameters updated.\n",
    "\n",
    "Instead of simply applying a full update $\\Delta W$ to a frozen weight matrix $W_0 \\in R^{d \\times k}$, we can represent that update using low-rank decomposition. Namely, $\\Delta W = BA$, where $B \\in R^{d \\times r}$ , $ A \\in R^{r \\times k}$, and the rank $r << min(d, k)$, resulting in:\n",
    "\n",
    "$$\n",
    "W_0 + \\Delta W = W_0 + BA\n",
    "$$\n",
    "\n",
    "As we are using *low-rank approximation*, it would be more precise to state that $\\Delta W \\approx BA$, resulting in:\n",
    "\n",
    "$$\n",
    "W_0 + \\Delta W \\approx W_0 + BA\n",
    "$$\n",
    "\n",
    "\n",
    "In this section you will be responsible for creating a partially frozen network that applies low-rank updates to a classifier when training on a new [dataset](https://datashare.ed.ac.uk/handle/10283/3192). Code Cells 4.0a-c download and initialize the `cinic-10` dataset, as well as install the [LoRA library](https://github.com/microsoft/LoRA/tree/main).\n",
    "\n",
    "In summary, you will:\n",
    "1. Load pre-trained ResNet-18\n",
    "2. Modify the network to have frozen early layers\n",
    "3. Further modify the network to have a LoRA classifier layer\n",
    "4. Compare the fine-tuning performance of: unfrozen, partially frozen, and partially frozen + LoRA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lccOFLUl_qz6"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 4.0a\n",
    "!mkdir -p data/cinic-10\n",
    "!curl -L https://datashare.is.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz | tar xz -C data/cinic-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "oi0csjogKEa1"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 4.0b\n",
    "cinic_traindir = os.path.join(\"data\", \"cinic-10\", \"train\")\n",
    "cinic_testdir = os.path.join(\"data\", \"cinic-10\", \"test\")\n",
    "\n",
    "cinic_mean = [0.47889522, 0.47227842, 0.43047404]\n",
    "cinic_std = [0.24205776, 0.23828046, 0.25874835]\n",
    "cinic_normalize = transforms.Normalize(mean=cinic_mean, std=cinic_std)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cinic_mean, std=cinic_std)\n",
    "])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cinic_mean, std=cinic_std)\n",
    "])\n",
    "\n",
    "c_trainset = datasets.ImageFolder(root=cinic_traindir, transform=train_transform)\n",
    "c_trainloader = torch.utils.data.DataLoader(c_trainset,\n",
    "                                          batch_size=128,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2)\n",
    "\n",
    "c_testset = datasets.ImageFolder(root=cinic_testdir, transform=transform)\n",
    "c_testloader = torch.utils.data.DataLoader(c_testset,\n",
    "                                         batch_size=100,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=2)\n",
    "\n",
    "classes = ('airplane', 'automobile', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "BZqELvqMMeAe"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 4.0c\n",
    "!pip install loralib\n",
    "import loralib as lora\n",
    "\n",
    "device = 'cuda'\n",
    "default_criterion = nn.CrossEntropyLoss\n",
    "default_epochs = 1\n",
    "default_lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRVzZY52sXHd"
   },
   "source": [
    "---\n",
    "<font color='red'>**PART 4.1:**</font> [25 points]\n",
    "\n",
    "<font color='red'>**Deliverables**</font>\n",
    "1. In *Code Cell 4.1*, instantiate a pre-trained ResNet-18 model (`IMAGENET1K_V1` weights) using the `torchvision` library.\n",
    "2. In *Code Cell 4.2*, implement the freezing of earlier layers in the model. As long as the final classification layer is not frozen, you are free to choose whichever layers to freeze. You may want to start with leaving the last 4 conv layers unfrozen.\n",
    "3. In *Code Cell 4.3*, replace the final classification layer of the ResNet-18 model with a LoRA linear layer.\n",
    "4. You will train and evaluate the aforementioned models in *Code Cell 4.4* through *Code Cell 4.6*. Plot the test accuracy curves (accuracy vs epoch) on the same graph for these three models: the baseline pre-trained model (`pt_net`), the partially frozen model (`net_freeze`), and the model with both frozen layers and a LoRA classification layer (`net_freeze_lora`). Note: when making adjustments to your models, make sure to run *Code Cells 4.2 - 4.6* **in sequential order** to avoid unexpected results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7396BlrFoZZ"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 4.1\n",
    "\n",
    "# PART 4.1: Implement!\n",
    "# Read the documentation at https://pytorch.org/vision/stable/models.html\n",
    "# Create a Resnet-18 network with weights that were pretrained on IMAGENET1K_V1.\n",
    "from torchvision.models import resnet18 # [ must import resnet18]\n",
    "# TODO: implement!\n",
    "pt_net =\n",
    "\n",
    "# Need criterion and optimizer per model\n",
    "pt_net_criterion = default_criterion()\n",
    "pt_net_optimizer = torch.optim.SGD(pt_net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Get the baseline accuracy\n",
    "pt_net.to(device)\n",
    "test(net=pt_net, epoch=0, loader=c_testloader, criterion=pt_net_criterion, loss_tracker=[], acc_tracker=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNked_TT8Muu"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 4.2\n",
    "\n",
    "# PART 4.1: Implement!\n",
    "# Freeze earlier layers of the model (e.g., layer0 -> layerN). [7.5 pts]\n",
    "# Modifications should be applied to net_freeze.\n",
    "# Hint: you may want to use the named_parameters() method, and leave biases\n",
    "# unfrozen.\n",
    "net_freeze = deepcopy(pt_net)\n",
    "\n",
    "# Code that freezes layers of network\n",
    "# TODO: implement!\n",
    "\n",
    "# Need criterion and optimizer per model\n",
    "net_freeze_criterion = default_criterion()\n",
    "net_freeze_optimizer = torch.optim.SGD(net_freeze.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Get the baseline accuracy\n",
    "net_freeze.to(device)\n",
    "test(net=net_freeze, epoch=0, loader=c_testloader, criterion=net_freeze_criterion, loss_tracker=[], acc_tracker=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPb9GVn0cw7R"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 4.3\n",
    "\n",
    "# PART 4.1: Implement!\n",
    "# Replace the classifer layer with a LoRA linear layer [7.5 pts]\n",
    "# Modifications should be applied to net_freeze_lora.\n",
    "# We have already installed and imported this library as \"lora\"\n",
    "# Library reference: https://github.com/microsoft/LoRA/tree/main\n",
    "# Hint: when replacing the layer, you may want to keep a copy of the original\n",
    "# weights. You may also want try different ranks (but r=10 should suffice).\n",
    "net_freeze_lora = deepcopy(net_freeze)\n",
    "\n",
    "# Modify network to have a LoRA linear classifer\n",
    "# TODO: implement!\n",
    "\n",
    "# Need criterion and optimizer per model\n",
    "net_freeze_lora_criterion = default_criterion()\n",
    "net_freeze_lora_optimizer = torch.optim.SGD(net_freeze_lora.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Get the baseline accuracy\n",
    "net_freeze_lora.to(device)\n",
    "test(net=net_freeze_lora, epoch=0, loader=c_testloader, criterion=net_freeze_lora_criterion, loss_tracker=[], acc_tracker=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQKqwdi427bP"
   },
   "source": [
    "---\n",
    "<font color='red'>**PART 4.2:**</font> [10 points]\n",
    "\n",
    "<font color='red'>**Deliverables**</font>\n",
    "1. You will train and evaluate the aforementioned models in *Code Cell 4.4* through *Code Cell 4.6*. Plot the test accuracy curves (accuracy vs epoch) on the same graph for these three models: the baseline pre-trained model (`pt_net`), the partially frozen model (`net_freeze`), and the model with both frozen layers and a LoRA classification layer (`net_freeze_lora`). Note: if you make any adjustments to your models, make sure to run *Code Cells 4.1 - 4.3* in **sequential order and before** *Code Cells 4.4 - 4.6* to avoid unexpected behavior.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIyjez_s6qDI"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 4.4\n",
    "\n",
    "# Fine-tune and test vanilla pre-trained network\n",
    "pt_net.to(device)\n",
    "for epoch in range(default_epochs):\n",
    "    train(net=pt_net, epoch=epoch, loader=c_trainloader,\n",
    "          criterion=pt_net_criterion, optimizer=pt_net_optimizer,\n",
    "          loss_tracker=[], acc_tracker=[])\n",
    "    test(net=pt_net, epoch=epoch, loader=c_testloader,\n",
    "         criterion=pt_net_criterion,\n",
    "         loss_tracker=[], acc_tracker=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwPcgIbUUY77"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 4.5\n",
    "\n",
    "# Fine-tune and test partially frozen network\n",
    "net_freeze.to(device)\n",
    "for epoch in range(default_epochs):\n",
    "    train(net=net_freeze, epoch=epoch, loader=c_trainloader,\n",
    "          criterion=net_freeze_criterion, optimizer=net_freeze_optimizer,\n",
    "          loss_tracker=[], acc_tracker=[])\n",
    "    test(net=net_freeze, epoch=epoch,\n",
    "         loader=c_testloader, criterion=net_freeze_criterion,\n",
    "         loss_tracker=[], acc_tracker=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87oowJVjdBjD"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 4.6\n",
    "\n",
    "# Fine-tune and test partially frozen network with LoRA classifier\n",
    "net_freeze_lora.to(device)\n",
    "for epoch in range(default_epochs):\n",
    "    train(net=net_freeze_lora, epoch=epoch, loader=c_trainloader,\n",
    "          criterion=net_freeze_lora_criterion, optimizer=net_freeze_lora_optimizer,\n",
    "          loss_tracker=[], acc_tracker=[])\n",
    "    test(net=net_freeze_lora, epoch=epoch,\n",
    "         loader=c_testloader, criterion=net_freeze_lora_criterion,\n",
    "         loss_tracker=[], acc_tracker=[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wf4LCS9A24Hz"
   },
   "source": [
    "---\n",
    "\n",
    "### **5. Transformers**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "LVXezM8y27E4"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 5.1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upUwXL-e3f2q"
   },
   "source": [
    "In this question, your team will be implementing the transformer model, focusing on the original transformer model introduced in the paper \"Attention is All You Need\" by Vaswani et al. (2017).\n",
    "\n",
    "In this programming assignment, your team will implement all the components of the transformer model. In the next programming assignment, you will use the components you have implemented here to train a small language model. For this, your team has the option to use PyTorch or TensorFlow for implementation (For Tensorflow, make sure to replace the class definitions).\n",
    "\n",
    "You will find Figure 1 in the paper very helpful for this problem set. Be sure to carefully review relevant PyTorch/TensorFlow documentation. In your writeup, you will need to write and justify the hyperparameters that you will need to implement the transformer model.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1TX5V8DRzYlGFv0OMoE6qcW8f-tM5WDH5\" width=\"500\"/>\n",
    "\n",
    "\n",
    "# I: Implementing Components of the Transformer\n",
    "\n",
    "## 1) Positional Encoding and Embedding\n",
    "---\n",
    "\n",
    "**a)** Positional Encoding\n",
    "\n",
    "The input to the transformer model is just the set of vectors without any notion of order. Without recurrent or convolutional layers we need to inject some information about the position of the words in the sentence. If this is not done, the set of words \"I am a student\" and \"am I a student\" would be treated as the same input.\n",
    "\n",
    "In the original transformer model, the authors use the positional encoding of the form:\n",
    "\n",
    "$$PE_{(pos, 2i)} = sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = cos(pos / 10000^{2i/d_{model}})$$\n",
    "\n",
    "where $pos$ is the position and $i$ is the dimension. Implement this positional encoding function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zktZGguZ3gva"
   },
   "outputs": [],
   "source": "#@title Code Cell 5.2\n\ndef positional_encoding(length, depth):\n  \"\"\"\n  Generate positional encoding matrix using sine and cosine functions.\n\n  Args:\n    length: Maximum sequence length (corresponds to 'pos' in the formula)\n    depth: Embedding dimension (corresponds to 'd_model' in the formula)\n\n  Returns:\n    Positional encoding matrix of shape (length, depth)\n  \"\"\"\n  # Create position indices: [0, 1, 2, ..., length-1]\n  positions = np.arange(length)[:, np.newaxis]  # Shape: (length, 1)\n\n  # Create dimension indices: [0, 1, 2, ..., depth-1]\n  dimensions = np.arange(depth)[np.newaxis, :]  # Shape: (1, depth)\n\n  # Calculate the angle rates: 1 / (10000^(2i/d_model))\n  # For even indices (2i), for odd indices (2i+1), we use floor division\n  angle_rates = 1 / np.power(10000, (2 * (dimensions // 2)) / np.float32(depth))\n\n  # Calculate angles: pos * angle_rates\n  angle_rads = positions * angle_rates  # Shape: (length, depth)\n\n  # Apply sin to even indices (2i) and cos to odd indices (2i+1)\n  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # Apply sine to even indices\n  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # Apply cosine to odd indices\n\n  # Convert to PyTorch tensor and add batch dimension\n  pos_encoding = torch.tensor(angle_rads, dtype=torch.float32)\n\n  return pos_encoding"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6tlUL9p4Zve"
   },
   "source": [
    "**b)** Plot the positional encoding with length 2048 and depth 256. Comment on your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMJB1pJH4aFP"
   },
   "outputs": [],
   "source": "#@title Code Cell 5.3\n\n# Generate positional encoding with length=2048 and depth=256\npos_enc = positional_encoding(2048, 256)\n\n# Plot the positional encoding\nplt.figure(figsize=(15, 5))\nplt.pcolormesh(pos_enc.numpy(), cmap='RdBu')\nplt.xlabel('Depth (Embedding Dimension)')\nplt.ylabel('Position')\nplt.colorbar()\nplt.title('Positional Encoding: Sine and Cosine Patterns')\nplt.show()\n\n# Comment: The plot shows alternating sine and cosine wave patterns.\n# Lower dimensions (left) have slower-varying patterns (lower frequencies),\n# while higher dimensions (right) have faster-varying patterns (higher frequencies).\n# This allows the model to attend to both local and global positional information."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "095486Jt4lJ8"
   },
   "source": [
    "**c)** Now, construct `PositionalEmbedding` class that finds the embedding vector of the token and adds the positional encoding to it. `You may use the torch.nn.Embedding`/`tf.keras.layers.Embedding` layer to find the token embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dL6jL5s34le2"
   },
   "outputs": [],
   "source": "#@title Code Cell 5.4\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, vocab_size, d_model, max_len=2048):\n      \"\"\"\n      Combines token embeddings with positional encodings.\n\n      Args:\n        vocab_size: Size of the vocabulary\n        d_model: Embedding dimension\n        max_len: Maximum sequence length\n      \"\"\"\n      super(PositionalEmbedding, self).__init__()\n      self.d_model = d_model\n\n      # Token embedding layer: converts token indices to dense vectors\n      self.embedding = nn.Embedding(vocab_size, d_model)\n\n      # Generate positional encoding and register as buffer (not a trainable parameter)\n      self.pos_encoding = positional_encoding(max_len, d_model)\n      self.register_buffer('positional_encoding', self.pos_encoding)\n\n    def forward(self, x):\n      \"\"\"\n      Forward pass: compute token embeddings and add positional encodings.\n\n      Args:\n        x: Input tensor of token indices, shape (batch_size, seq_len)\n\n      Returns:\n        Embedding with positional encoding, shape (batch_size, seq_len, d_model)\n      \"\"\"\n      # Get sequence length from input\n      seq_len = x.shape[1]\n\n      # Get token embeddings and scale by sqrt(d_model) as in the paper\n      x = self.embedding(x) * math.sqrt(self.d_model)\n\n      # Add positional encoding (only up to sequence length)\n      x = x + self.positional_encoding[:seq_len, :]\n\n      return x"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "PMp0xC5K4sj4"
   },
   "outputs": [],
   "source": [
    "#@title Code Cell 5.5\n",
    "vocab_size = 10\n",
    "d_model = 16\n",
    "max_len = 20\n",
    "\n",
    "model = PositionalEmbedding(vocab_size, d_model, max_len=max_len)\n",
    "\n",
    "input_tensor = torch.randint(0, vocab_size, (2, 5))\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "\n",
    "print(\"Input Tensor Shape:\")\n",
    "print(input_tensor.shape)\n",
    "\n",
    "print(\"\\nOutput Tensor Shape:\")\n",
    "print(output.shape)\n",
    "\n",
    "\n",
    "def test_positional_embedding_shape():\n",
    "    expected_shape = (2, 5, d_model)\n",
    "    assert output.shape == expected_shape, f\"Expected shape {expected_shape}, but got {output.shape}\"\n",
    "\n",
    "# Running the test\n",
    "test_positional_embedding_shape()\n",
    "print(\"\\nShape test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9n8W2AxH46aD"
   },
   "source": [
    "## 2) Attention Layers\n",
    "\n",
    "The Attention mechanism is at the center of the transformer architecture. Recall that the self-attention output is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\quad \\text{(1)}\n",
    "$$\n",
    "\n",
    "where $Q$, $K$, and $V$ are the query, key, and value matrices, respectively. $d_k$ is the dimension of each of the key vectors, and it helps with normalizing dot products that can become large. Here is a short intuition for each of them:\n",
    "\n",
    "- **Query (Q)** represents something that is looking to gather information, indicating which information is important to the model.\n",
    "\n",
    "- **Key (K)** represents the information that is being looked at, indicating which information is important to the model.\n",
    "\n",
    "- **Value (V)** represents the information that is being outputted/passed along.\n",
    "\n",
    "\n",
    "This allows us to define the multi-head attention, which concatenates different attention layer, allowing the model to focus on different representations of the input in different positions. We define the multi-head attention as follows:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\n",
    "\\quad \\text{(2)}\n",
    "$$\n",
    "\n",
    "where each head is defined as:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\\quad \\text{(3)}\n",
    "$$\n",
    "\n",
    "where $W_i^Q$, $W_i^K$, and $W_i^V$ are the weight matrices for the query, key, and value vectors, respectively. $W^O$ is the weight matrix that is applied to the concatenated output of the heads.\n",
    "\n",
    "\n",
    "\n",
    "The Query, Key, and Value vectors are determined by multiplying the input embedding matrix with the weight matrices $W^Q$, $W^K$, and $W^V$, respectively. These are the parameters that are learned during the training process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5l4tyBgy5T-J"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "**a)** The global self-attention layer processes context sequence and is designed to capture the dependencies between the words in the sequence. For the attention layer equation (2), we know that the input to each of the heads in the multi-head attention are determined by the following:\n",
    "\n",
    "\\begin{equation}\n",
    "XW^Q, \\quad K = XW^K, \\quad V = XW^V\n",
    "\\end{equation}\n",
    "\n",
    "respectively, where $X$ is the input embedding matrix. From this, implement the `GlobalSelfAttention` class. Ensure within the global-self attention layer, you add the normalization and the skip connection. **This will need to be done for all the attention layers you implement in the problem set.** Note that you are **allowed** to use the **torch.nn.MultiheadAttention** class or TensorFlow equivalent.\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1PoY5zupGP46K6fs0qJMDynzJecBBmT0s\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWYmJixm5NAE"
   },
   "outputs": [],
   "source": "#@title Code Cell 5.6\nclass GlobalSelfAttention(nn.Module):\n    def __init__(self, **kwargs):\n        \"\"\"\n        Global self-attention layer with layer normalization and residual connection.\n        Uses PyTorch's MultiheadAttention module.\n\n        Args:\n          d_model: Embedding dimension\n          num_heads: Number of attention heads\n          dropout_rate: Dropout probability\n        \"\"\"\n        super(GlobalSelfAttention, self).__init__()\n        self.d_model = kwargs['d_model']\n        self.num_heads = kwargs['num_heads']\n\n        # Multi-head attention layer\n        self.mha = nn.MultiheadAttention(\n            embed_dim=self.d_model,\n            num_heads=self.num_heads,\n            dropout=kwargs.get('dropout_rate', 0.1),\n            batch_first=True  # Input shape: (batch, seq, feature)\n        )\n\n        # Layer normalization\n        self.layernorm = nn.LayerNorm(self.d_model)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass with self-attention, residual connection, and layer norm.\n\n        Args:\n          x: Input tensor, shape (batch_size, seq_len, d_model)\n\n        Returns:\n          Output tensor with same shape as input\n        \"\"\"\n        # Self-attention: Q, K, V all come from the same input x\n        # attn_output shape: (batch_size, seq_len, d_model)\n        attn_output, _ = self.mha(x, x, x, need_weights=False)\n\n        # Residual connection: add input to attention output\n        x = x + attn_output\n\n        # Layer normalization\n        x = self.layernorm(x)\n\n        return x"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLUtn_DZ5yPE"
   },
   "source": [
    "\n",
    "\n",
    "**b)** The cross attention layer connects the encoder and the decoder together. Implement the `CrossAttention` class, paying attention to the fact that the query comes from the decoder, while the key and value come from the encoder. (So in your call function, you will have 2 inputs)\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1s54R7qkXE8hXgn9XnPGD7sYUFKR08RLV\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7S1q0cC5yx3"
   },
   "outputs": [],
   "source": "#@title Code Cell 5.7\nclass CrossAttention(nn.Module):\n    def __init__(self, **kwargs):\n        \"\"\"\n        Cross-attention layer connecting encoder and decoder.\n        Query comes from decoder, Key and Value come from encoder.\n\n        Args:\n          d_model: Embedding dimension\n          num_heads: Number of attention heads\n          dropout_rate: Dropout probability\n        \"\"\"\n        super(CrossAttention, self).__init__()\n        self.d_model = kwargs['d_model']\n        self.num_heads = kwargs['num_heads']\n\n        # Multi-head attention layer\n        self.mha = nn.MultiheadAttention(\n            embed_dim=self.d_model,\n            num_heads=self.num_heads,\n            dropout=kwargs.get('dropout_rate', 0.1),\n            batch_first=True\n        )\n\n        # Layer normalization\n        self.layernorm = nn.LayerNorm(self.d_model)\n\n    def forward(self, x, context):\n        \"\"\"\n        Forward pass with cross-attention.\n\n        Args:\n          x: Query from decoder, shape (batch_size, target_seq_len, d_model)\n          context: Key and Value from encoder, shape (batch_size, source_seq_len, d_model)\n\n        Returns:\n          Output tensor, shape (batch_size, target_seq_len, d_model)\n        \"\"\"\n        # Cross-attention: Q from x (decoder), K and V from context (encoder)\n        attn_output, _ = self.mha(x, context, context, need_weights=False)\n\n        # Residual connection: add decoder input to attention output\n        x = x + attn_output\n\n        # Layer normalization\n        x = self.layernorm(x)\n\n        return x"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2swFnjJe6CYX"
   },
   "source": [
    "\n",
    "\n",
    "**c)** The causal self-attention layer is similar to the global self-attention layer we discussed in (a), but it is designed for the decoder. To prevent the model from looking into the future, we apply a causal mask to the attention weights, ensuring that the model only attends to the previous tokens. Implement the `CausalSelfAttention` class below.\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1pXMWcB-lIWjRtFxjBTZ73S9Mn1PE-YLc\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4l7E37k6C2k"
   },
   "outputs": [],
   "source": "#@title Code Cell 5.8\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, **kwargs):\n        \"\"\"\n        Causal self-attention layer for decoder with masking to prevent looking ahead.\n\n        Args:\n          d_model: Embedding dimension\n          num_heads: Number of attention heads\n          dropout_rate: Dropout probability\n        \"\"\"\n        super(CausalSelfAttention, self).__init__()\n        self.d_model = kwargs['d_model']\n        self.num_heads = kwargs['num_heads']\n\n        # Multi-head attention layer\n        self.mha = nn.MultiheadAttention(\n            embed_dim=self.d_model,\n            num_heads=self.num_heads,\n            dropout=kwargs.get('dropout_rate', 0.1),\n            batch_first=True\n        )\n\n        # Layer normalization\n        self.layernorm = nn.LayerNorm(self.d_model)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass with causal self-attention.\n\n        Args:\n          x: Input tensor, shape (batch_size, seq_len, d_model)\n\n        Returns:\n          Output tensor with same shape as input\n        \"\"\"\n        batch_size, seq_len, d_model = x.shape\n\n        # Create causal mask: upper triangular matrix of -inf values\n        # This prevents attention to future positions\n        causal_mask = torch.triu(\n            torch.ones(seq_len, seq_len, device=x.device) * float('-inf'),\n            diagonal=1  # Start from diagonal=1 to keep current position\n        )\n\n        # Apply causal self-attention with mask\n        attn_output, _ = self.mha(\n            x, x, x,\n            attn_mask=causal_mask,\n            need_weights=False\n        )\n\n        # Residual connection\n        x = x + attn_output\n\n        # Layer normalization\n        x = self.layernorm(x)\n\n        return x"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_Ufg8Xm6JcL"
   },
   "source": [
    "# 3) Additional Layers - Feedforward and Layer Normalization\n",
    "\n",
    "After the attention layers in the encoder and the decoder, we have a simple feedforward neural network. It consists of two linear transformations with a ReLU activation in between, and a dropout layer (with dropout rate 0.1) after the linear transformations.\n",
    "\n",
    "**Implement this network**, and add the layer normalization and the residual connection to the feedforward network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUEiTEEL6Jxa"
   },
   "outputs": [],
   "source": "#@title Code Cell 5.9\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, dff, dropout_rate=0.1):\n        \"\"\"\n        Feedforward network with two linear layers, ReLU, dropout, layer norm, and residual.\n\n        Args:\n          d_model: Input and output dimension\n          dff: Hidden layer dimension (typically 4 * d_model)\n          dropout_rate: Dropout probability\n        \"\"\"\n        super(FeedForward, self).__init__()\n\n        # First linear transformation: d_model -> dff\n        self.linear1 = nn.Linear(d_model, dff)\n\n        # ReLU activation\n        self.relu = nn.ReLU()\n\n        # Second linear transformation: dff -> d_model\n        self.linear2 = nn.Linear(dff, d_model)\n\n        # Dropout layer applied after second linear transformation\n        self.dropout = nn.Dropout(dropout_rate)\n\n        # Layer normalization\n        self.layernorm = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through feedforward network.\n\n        Args:\n          x: Input tensor, shape (batch_size, seq_len, d_model)\n\n        Returns:\n          Output tensor with same shape as input\n        \"\"\"\n        # Store input for residual connection\n        residual = x\n\n        # First linear layer\n        x = self.linear1(x)\n\n        # ReLU activation\n        x = self.relu(x)\n\n        # Second linear layer\n        x = self.linear2(x)\n\n        # Dropout\n        x = self.dropout(x)\n\n        # Residual connection: add original input\n        x = x + residual\n\n        # Layer normalization\n        x = self.layernorm(x)\n\n        return x"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TsFEcYM6pV1"
   },
   "source": [
    "# II Implementing the Transformer Model\n",
    "\n",
    "## 1) Encoder\n",
    "---\n",
    "\n",
    "**a)** Now, we have all the components we need to construct the transformer model. We will start by implementing the encoder layer that consists global self attention and feedforward network. Implement the `EncoderLayer` class below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5TeaYrqNvSZ"
   },
   "outputs": [],
   "source": "#@title Code Cell 5.10\nclass EncoderLayer(nn.Module):\n    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n        \"\"\"\n        Single encoder layer: global self-attention + feedforward network.\n\n        Args:\n          d_model: Embedding dimension\n          num_heads: Number of attention heads\n          dff: Feedforward hidden dimension\n          dropout_rate: Dropout probability\n        \"\"\"\n        super(EncoderLayer, self).__init__()\n\n        # Global self-attention layer\n        self.self_attention = GlobalSelfAttention(\n            d_model=d_model,\n            num_heads=num_heads,\n            dropout_rate=dropout_rate\n        )\n\n        # Feedforward network\n        self.ffn = FeedForward(d_model, dff, dropout_rate)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through encoder layer.\n\n        Args:\n          x: Input tensor, shape (batch_size, seq_len, d_model)\n\n        Returns:\n          Output tensor with same shape as input\n        \"\"\"\n        # Apply self-attention (includes residual + layer norm)\n        x = self.self_attention(x)\n\n        # Apply feedforward network (includes residual + layer norm)\n        x = self.ffn(x)\n\n        return x"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loLVxnbGNvSZ"
   },
   "source": [
    "**b)** Implement the `Encoder` class that stacks multiple encoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOxuDDYLNvSZ"
   },
   "outputs": [],
   "source": "#@title Code Cell 5.11\nclass Encoder(nn.Module):\n    def __init__(self, num_layers, d_model, num_heads,\n                 dff, vocab_size, dropout_rate=0.1):\n        \"\"\"\n        Complete encoder: stacks multiple encoder layers.\n\n        Args:\n          num_layers: Number of encoder layers to stack\n          d_model: Embedding dimension\n          num_heads: Number of attention heads\n          dff: Feedforward hidden dimension\n          vocab_size: Size of input vocabulary\n          dropout_rate: Dropout probability\n        \"\"\"\n        super().__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        # Positional embedding layer\n        self.pos_embedding = PositionalEmbedding(vocab_size, d_model)\n\n        # Stack of encoder layers using ModuleList\n        self.enc_layers = nn.ModuleList([\n            EncoderLayer(\n                d_model=d_model,\n                num_heads=num_heads,\n                dff=dff,\n                dropout_rate=dropout_rate\n            )\n            for _ in range(num_layers)\n        ])\n\n        # Dropout applied after positional embedding\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through entire encoder.\n\n        Args:\n          x: Input token indices, shape (batch_size, seq_len)\n\n        Returns:\n          Encoded representation, shape (batch_size, seq_len, d_model)\n        \"\"\"\n        # Apply positional embedding\n        x = self.pos_embedding(x)\n\n        # Apply dropout after embedding\n        x = self.dropout(x)\n\n        # Pass through each encoder layer sequentially\n        for enc_layer in self.enc_layers:\n            x = enc_layer(x)\n\n        return x"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnVvjPPlNvSa"
   },
   "source": [
    "## 2) Decoder\n",
    "---\n",
    "\n",
    "**a)** Implement the `DecoderLayer` class that consists of causal self attention, cross attention, and feedforward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAJMoYPyNvSa"
   },
   "outputs": [],
   "source": "#@title Code Cell 5.12\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n        \"\"\"\n        Single decoder layer: causal self-attention + cross-attention + feedforward.\n\n        Args:\n          d_model: Embedding dimension\n          num_heads: Number of attention heads\n          dff: Feedforward hidden dimension\n          dropout_rate: Dropout probability\n        \"\"\"\n        super(DecoderLayer, self).__init__()\n\n        # Causal self-attention layer (decoder attends to its own previous outputs)\n        self.causal_self_attention = CausalSelfAttention(\n            d_model=d_model,\n            num_heads=num_heads,\n            dropout_rate=dropout_rate\n        )\n\n        # Cross-attention layer (decoder attends to encoder outputs)\n        self.cross_attention = CrossAttention(\n            d_model=d_model,\n            num_heads=num_heads,\n            dropout_rate=dropout_rate\n        )\n\n        # Feedforward network\n        self.ffn = FeedForward(d_model, dff, dropout_rate)\n\n    def forward(self, x, context):\n        \"\"\"\n        Forward pass through decoder layer.\n\n        Args:\n          x: Decoder input, shape (batch_size, target_seq_len, d_model)\n          context: Encoder output, shape (batch_size, source_seq_len, d_model)\n\n        Returns:\n          Decoder output, shape (batch_size, target_seq_len, d_model)\n        \"\"\"\n        # Apply causal self-attention (includes residual + layer norm)\n        x = self.causal_self_attention(x)\n\n        # Apply cross-attention with encoder output (includes residual + layer norm)\n        x = self.cross_attention(x, context)\n\n        # Apply feedforward network (includes residual + layer norm)\n        x = self.ffn(x)\n\n        return x"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ljbbomt_NvSa"
   },
   "source": [
    "**b)** Implement the `Decoder` class that stacks multiple decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbEGkA2MNvSa"
   },
   "outputs": [],
   "source": "#@title Code Cell 5.13\nclass Decoder(nn.Module):\n    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n        \"\"\"\n        Complete decoder: stacks multiple decoder layers.\n\n        Args:\n          num_layers: Number of decoder layers to stack\n          d_model: Embedding dimension\n          num_heads: Number of attention heads\n          dff: Feedforward hidden dimension\n          vocab_size: Size of target vocabulary\n          dropout_rate: Dropout probability\n        \"\"\"\n        super(Decoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        # Positional embedding layer for target sequence\n        self.pos_embedding = PositionalEmbedding(vocab_size, d_model)\n\n        # Stack of decoder layers using ModuleList\n        self.dec_layers = nn.ModuleList([\n            DecoderLayer(\n                d_model=d_model,\n                num_heads=num_heads,\n                dff=dff,\n                dropout_rate=dropout_rate\n            )\n            for _ in range(num_layers)\n        ])\n\n        # Dropout applied after positional embedding\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x, context):\n        \"\"\"\n        Forward pass through entire decoder.\n\n        Args:\n          x: Target token indices, shape (batch_size, target_seq_len)\n          context: Encoder output, shape (batch_size, source_seq_len, d_model)\n\n        Returns:\n          Decoded representation, shape (batch_size, target_seq_len, d_model)\n        \"\"\"\n        # Apply positional embedding to target sequence\n        x = self.pos_embedding(x)\n\n        # Apply dropout after embedding\n        x = self.dropout(x)\n\n        # Pass through each decoder layer sequentially, providing encoder context\n        for dec_layer in self.dec_layers:\n            x = dec_layer(x, context)\n\n        return x"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBiJWmzRNvSa"
   },
   "source": [
    "## 3) Transformer - Putting it all together\n",
    "---\n",
    "We now have the encoder and the decoder ready, all you have to do is to put them together and add a final dense layer with a softmax activation to get the output. Implement the `Transformer` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0NZ6phGNvSa"
   },
   "outputs": [],
   "source": "#@title Code Cell 5.14\nclass Transformer(nn.Module):\n    def __init__(self, num_layers, d_model, num_heads, dff,\n                 input_vocab_size, target_vocab_size, dropout_rate=0.1):\n        \"\"\"\n        Complete Transformer model: encoder + decoder + final output layer.\n\n        Args:\n          num_layers: Number of encoder/decoder layers\n          d_model: Embedding dimension\n          num_heads: Number of attention heads\n          dff: Feedforward hidden dimension\n          input_vocab_size: Size of source vocabulary\n          target_vocab_size: Size of target vocabulary\n          dropout_rate: Dropout probability\n        \"\"\"\n        super(Transformer, self).__init__()\n\n        # Encoder stack\n        self.encoder = Encoder(\n            num_layers=num_layers,\n            d_model=d_model,\n            num_heads=num_heads,\n            dff=dff,\n            vocab_size=input_vocab_size,\n            dropout_rate=dropout_rate\n        )\n\n        # Decoder stack\n        self.decoder = Decoder(\n            num_layers=num_layers,\n            d_model=d_model,\n            num_heads=num_heads,\n            dff=dff,\n            vocab_size=target_vocab_size,\n            dropout_rate=dropout_rate\n        )\n\n        # Final linear layer to project to target vocabulary\n        self.final_layer = nn.Linear(d_model, target_vocab_size)\n\n    def forward(self, context, x):\n        \"\"\"\n        Forward pass through entire transformer.\n\n        Args:\n          context: Source token indices, shape (batch_size, source_seq_len)\n          x: Target token indices, shape (batch_size, target_seq_len)\n\n        Returns:\n          Logits over target vocabulary, shape (batch_size, target_seq_len, target_vocab_size)\n        \"\"\"\n        # Encode source sequence\n        context = self.encoder(context)  # Shape: (batch, source_seq_len, d_model)\n\n        # Decode target sequence with encoder context\n        x = self.decoder(x, context)  # Shape: (batch, target_seq_len, d_model)\n\n        # Project to target vocabulary to get logits\n        logits = self.final_layer(x)  # Shape: (batch, target_seq_len, target_vocab_size)\n\n        return logits"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVankMgQvxo7"
   },
   "source": [
    "Run the test function below to make sure your Transformer can initialize properly and the dimensions work out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTYIOYitsO7a"
   },
   "outputs": [],
   "source": [
    "# @title Test Cell\n",
    "def run_test():\n",
    "    # Transformer hyperparameters\n",
    "    num_layers = 4\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    dff = 2048\n",
    "    input_vocab_size = 8000\n",
    "    target_vocab_size = 8001\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "    transformer_model = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                                    input_vocab_size, target_vocab_size, dropout_rate)\n",
    "\n",
    "    batch_size = 64\n",
    "    context_len = 201\n",
    "    target_len = 200\n",
    "\n",
    "    context = torch.randint(0, input_vocab_size, (batch_size, context_len))\n",
    "    x = torch.randint(0, target_vocab_size, (batch_size, target_len))\n",
    "\n",
    "    print(context.shape)\n",
    "\n",
    "    logits = transformer_model(context, x)\n",
    "\n",
    "    print(f\"logits shape: {logits.shape}\")\n",
    "    assert logits.shape == (batch_size, target_len, target_vocab_size), \\\n",
    "        f\"Expected shape {(batch_size, target_len, target_vocab_size)}, but got {logits.shape}\"\n",
    "\n",
    "run_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmtOiXgE-SgS"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWECUclZ-3j8"
   },
   "source": [
    "---\n",
    "\n",
    "### **6. Depthwise Separable Convolutions**\n",
    "\n",
    "---\n",
    "\n",
    "In this section, you will convert the previous convolutional neural network to use depthwise separable convolutions rather than standard convolutions. Doing so will reduce the number of parameters in the neural network and enable deployment on resource-limited devices.\n",
    "\n",
    "A depthwise separable convolution separates each channel of the 3D input, performs convolution individually on each channel, then \"aggregates\" the channels with a standard 1x1 convolution on the output of the previous step.\n",
    "\n",
    "We refer you to this [blog](https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec) and [paper](https://arxiv.org/abs/1704.04861) for more details about depthwise separable convolution.\n",
    "\n",
    "> **Note:** Before running *Code Cell 6.1*, please run *Code Cell 1.1* → *Code Cell 1.3* to load the functions `train()`, `test()`, `trainloader`, `testloader`, and `conv_block()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPa0Z6h9-sYT"
   },
   "source": [
    "---\n",
    "<font color='red'>**PART 6.1:**</font> [25 points]\n",
    "\n",
    "<font color='red'>**Deliverables**</font>\n",
    "\n",
    "1. Implement the `DepthwiseSeparableConvolution` module in PyTorch.\n",
    "(<font color=\"red\">Do not use the \"`groups`\" parameter of [pytorch `nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv#torch.nn.Conv2d) to implement this -- we want you to understand the details and mechanics of the operation!</font>)\n",
    "2. Verify the correctness of `DepthwiseSeparableConvolution`. (*Code Cell 6.1* should print \"Success...!\" after you run it.)\n",
    "3. Verify that `DepthwiseSeparableConvolution` reduces the number of parameters by a factor of the number of output channels by printing out the number of parameters for both a standard convolution and the depthwise separable convolution. By what factor did depthwise separable convolutions reduce the parameters? Does this match what should theoretically happen? (50 words maximum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XaLZtx9_VNz"
   },
   "outputs": [],
   "source": "#@title Code Cell 6.1\nimport numpy as np\n\nclass DepthwiseSeparableConvolutionSolution(nn.Module):\n  def __init__(self, C_in, C_out, W_H):\n      self.C_in = C_in\n      self.C_out = C_out\n      self.W_H = W_H\n      super(DepthwiseSeparableConvolutionSolution, self).__init__()\n      self.depthwise = nn.Conv2d(C_in, C_in, kernel_size=W_H, groups=C_in, bias=False)\n      self.pointwise = nn.Conv2d(C_in, C_out, kernel_size=1, bias=False)\n\n  def forward(self, x):\n      # Pad\n      N,C_in,H_in,W_in = x.size()\n      p_left = (self.W_H-1+1)//2\n      p_right = self.W_H-1-p_left\n      p_top = p_left\n      p_bottom = p_right\n      x = torch.nn.functional.pad(x, (p_left, p_right, p_top, p_bottom))\n      out = self.depthwise(x)\n      out = self.pointwise(out)\n      return out\n\nclass DepthwiseSeparableConvolution(nn.Module):\n  def __init__(self, C_in, C_out, W_H):\n    # C_in - input channels\n    # C_out - output channels\n    # W_H - width = height dimension of filters\n    super(DepthwiseSeparableConvolution, self).__init__()\n\n    self.C_in = C_in\n    self.C_out = C_out\n    self.W_H = W_H\n\n    # Please do not change this -- this will be overwritten during testing!\n    self.kernel_weights = [nn.Parameter(torch.Tensor(self.W_H, self.W_H)) for i in range(self.C_in)]\n    self.one_d_conv_weight = nn.Parameter(torch.Tensor(self.C_out, self.C_in, 1, 1))\n\n    for i,k in enumerate(self.kernel_weights):\n      self.register_parameter(\"k_%d\" % i, k)\n\n    self.reset_parameters()\n\n  def reset_parameters(self) -> None:\n    for k in self.kernel_weights+[self.one_d_conv_weight]:\n      torch.nn.init.kaiming_uniform_(k, a=math.sqrt(5))\n\n  def forward(self, x):\n    # Input shape: (N, C_in, H_in, W_in)\n    # Output shape: (N, C_out, H_in, W_in) (uses same padding for same dimension)\n    # Note: N is batch size\n\n    # Step 1: Calculate padding for \"same\" convolution\n    # Padding formula: pad = (kernel_size - 1) / 2 for odd kernel, distributed for even\n    p_left = (self.W_H - 1 + 1) // 2  # Left/top padding\n    p_right = self.W_H - 1 - p_left   # Right/bottom padding\n    \n    # Apply padding to input: (left, right, top, bottom)\n    x_padded = F.pad(x, (p_left, p_right, p_left, p_right))\n\n    # Step 2: Depthwise convolution - apply each filter to its corresponding channel\n    depthwise_outputs = []\n    for i in range(self.C_in):\n      # Extract single channel from input: shape (N, 1, H_padded, W_padded)\n      channel_input = x_padded[:, i:i+1, :, :]\n      \n      # Extract corresponding kernel weight and reshape for conv2d\n      # Shape: (1, 1, W_H, W_H) - one output channel, one input channel\n      kernel = self.kernel_weights[i].unsqueeze(0).unsqueeze(0)\n      \n      # Apply convolution to this channel only (no padding needed, already padded)\n      channel_output = F.conv2d(channel_input, kernel, stride=1, padding=0)\n      \n      depthwise_outputs.append(channel_output)\n    \n    # Concatenate all channel outputs along channel dimension\n    # Shape: (N, C_in, H_in, W_in)\n    depthwise_result = torch.cat(depthwise_outputs, dim=1)\n\n    # Step 3: Pointwise convolution (1x1 conv) to aggregate channels\n    # This combines information across channels to produce C_out output channels\n    output = F.conv2d(depthwise_result, self.one_d_conv_weight, stride=1, padding=0)\n\n    return output\n\n# DepthwiseSeparableConvolution test case:\n# You may use this to understand how depthwise conv works!\n#\n# Input of shape (batch=1, c_in=2, h_in=2, h_out2):\n# x:\n# [1,2] [4,5]\n# [3,4] [5,6]\n#\n# Depthwise conv (c_in=2, c_out=2, width/height=2)\n# f:\n# [1,1] [2,2]\n# [1,1] [2,2]\n#\n# 1d conv weight:\n# fd:\n# [2,3], [4,5]\n#\n# Operation:\n# conv(x[0], f[0]), conv(x[1], f[1]) = x' =\n# [1 3 ] [8 18]\n# [4 10] [18 40]\n#\n# conv1d(x', fd)\n# [26 60 ] [44 102]\n# [62 140] [106 240]\n\nm = DepthwiseSeparableConvolution(2,2,2)\n\nm.kernel_weights = [nn.Parameter(torch.from_numpy(x).float()) for x in [\n                     np.array([[1,1], [1,1]]),\n                     np.array([[2,2], [2,2]])]]\nm.one_d_conv_weight = nn.Parameter(torch.from_numpy(np.array(\n    [[2,3],[4,5]]).reshape((2,2,1,1))).float())\n\n#######################################################\n# Solution version -- feel free to test against this\n#######################################################\n# m = DepthwiseSeparableConvolutionSolution(2,2,2)\n# m.depthwise.weight = nn.Parameter(torch.from_numpy(np.array([\n#                       [[1,1], [1,1]],\n#                       [[2,2], [2,2]],\n# ]).reshape(2,1,2,2)).float())\n# m.pointwise.weight = nn.Parameter(torch.from_numpy(np.array([\n#                       [2,3],[4,5]\n# ]).reshape(2,2,1,1)).float())\n##############################################\n\nx = torch.from_numpy(np.array([\n                 [[1,2],[3,4]],\n                 [[4,5],[5,6]]\n]).reshape(1,2,2,2)).float()\n\ngroundtruth = np.array([[[26,60],[62,140]], [[44, 102],[106,240]]])\nerr = np.linalg.norm(m(x).detach().numpy()-groundtruth)\nif err <= 1e-5:\n  print(\"Success: err: %f!\" % err)\nelse:\n  print(\"Failure: err: %f!\" % err)\n\n# Compare number of parameters for depthwise separable conv and standard conv\nstandard_conv = torch.nn.Conv2d(10, 20, 5, bias=False)\ndwise_conv = DepthwiseSeparableConvolution(10, 20, 5)\n\n# Count parameters in a module\ndef count_params(module):\n  \"\"\"Count the total number of parameters in a PyTorch module.\"\"\"\n  return sum(p.numel() for p in module.parameters() if p.requires_grad)\n\n# Print parameter counts\nstandard_params = count_params(standard_conv)\ndwise_params = count_params(dwise_conv)\nreduction_factor = standard_params / dwise_params\n\nprint(f\"Standard Conv Parameters: {standard_params}\")\nprint(f\"Depthwise Separable Conv Parameters: {dwise_params}\")\nprint(f\"Reduction Factor: {reduction_factor:.2f}x\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deELXI9K_eol"
   },
   "source": [
    "---\n",
    "<font color='red'>**PART 6.2:**</font> [10 points]\n",
    "\n",
    "<font color='red'>**Deliverables**</font>\n",
    "\n",
    "1. Implement the `dw_conv_block` function which performs in sequence: depthwise separable convolution, batch normalization, ReLU.\n",
    "\n",
    "2. Train the neural network for several epochs with your implementation of `DepthwiseSeparableConvolution`; compare the runtime with `DepthwiseSeparableConvolutionSolution`. How much faster is the solution versus your implementation? Why might this be? (50 words maximum)\n",
    "\n",
    "3. Train the neural network for 100 epochs with `DepthwiseSeparableConvolutionSolution` and plot the training error and test accuracy (x-axis is epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQbElkAJ_kgJ"
   },
   "outputs": [],
   "source": "#@title Code Cell 6.2\n\ndef dw_conv_block(in_channels, out_channels, kernel_size=3):\n    '''\n    dw_conv_block performs in sequence:\n    - Depthwise Separable Convolution (in_channels -> out_channels)\n    - Batch Normalization\n    - ReLU activation\n    \n    Args:\n      in_channels: Number of input channels\n      out_channels: Number of output channels\n      kernel_size: Size of convolutional kernel (default 3x3)\n    \n    Returns:\n      nn.Sequential module containing the three operations\n    '''\n    return nn.Sequential(\n        # Depthwise separable convolution\n        DepthwiseSeparableConvolutionSolution(in_channels, out_channels, kernel_size),\n        # Batch normalization for output channels\n        nn.BatchNorm2d(out_channels),\n        # ReLU activation\n        nn.ReLU(inplace=True)\n    )\n\n\nclass DWConvNet(nn.Module):\n    '''\n    Modified CNN with depthwise separable convs.\n    Uses depthwise separable convolutions in layers 2, 4, 7, and 9.\n    '''\n    def __init__(self):\n        super(DWConvNet, self).__init__()\n\n        # Build model with mix of standard and depthwise separable convolutions\n        self.model = nn.Sequential(\n            conv_block(3, 32),                    # Layer 1: standard conv\n            dw_conv_block(32, 32),                # Layer 2: depthwise separable\n            conv_block(32, 64, stride=2),         # Layer 3: standard conv (downsample)\n            dw_conv_block(64, 64),                # Layer 4: depthwise separable\n            conv_block(64, 64),                   # Layer 5: standard conv\n            conv_block(64, 128, stride=2),        # Layer 6: standard conv (downsample)\n            dw_conv_block(128, 128),              # Layer 7: depthwise separable\n            conv_block(128, 256),                 # Layer 8: standard conv\n            dw_conv_block(256, 256),              # Layer 9: depthwise separable\n            nn.AdaptiveAvgPool2d(1)               # Global average pooling to 1x1\n        )\n\n        # Final classification layer\n        self.classifier = nn.Linear(256, 10)\n\n    def forward(self, x):\n        '''\n        Forward pass through the network.\n\n        Args:\n          x: Input image tensor, shape (batch_size, 3, 32, 32)\n\n        Returns:\n          Class logits, shape (batch_size, 10)\n        '''\n        # Apply convolutional layers\n        h = self.model(x)\n\n        # Flatten spatial dimensions\n        B, C, x, y = h.shape\n        h = h.view(B, C*x*y)  # Should be (B, 256) after AdaptiveAvgPool2d(1)\n        \n        # Apply classifier\n        return self.classifier(h)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kF_zy4pO_0mF"
   },
   "outputs": [],
   "source": "#@title Code Cell 6.3\n# We will use Adam with default learning rate instead of SGD.\n# You will find that the Adam optimizer converges much faster than SGD!\n\ntorch.manual_seed(43) # to give stable randomness\n\ndevice = 'cuda'\nnet = DWConvNet()\nnet = net.to(device)\n\n# Set these parameters based on PART 1.2\n# From Part 1.2, the best learning rate schedule was MultiStepLR\n# with milestones at every 25 epochs (25, 50, 75)\nepochs = 100\nmilestones = [25, 50, 75]  # Decrease learning rate by factor of 10 at these epochs\n\ncriterion = nn.CrossEntropyLoss()\n# Using Adam optimizer (converges faster than SGD for this architecture)\noptimizer = torch.optim.Adam(net.parameters())\n# MultiStepLR scheduler to decay learning rate at milestones\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n                                                 milestones=milestones,\n                                                 gamma=0.1)\n\n# Track training and test metrics\ntrain_loss_tracker, train_acc_tracker = [], []\ntest_loss_tracker, test_acc_tracker = [], []\n\nprint('Training for {} epochs and milestones {}'.format(\n      epochs, milestones))\n\nstart_time = time.time()\n# Training loop over all epochs\nfor epoch in range(0, epochs):\n    # Train for one epoch\n    train(net=net, epoch=epoch, loader=trainloader, criterion=criterion, \n          optimizer=optimizer, loss_tracker=train_loss_tracker, \n          acc_tracker=train_acc_tracker)\n    # Evaluate on test set\n    test(net=net, epoch=epoch, loader=testloader, criterion=criterion, \n         loss_tracker=test_loss_tracker, acc_tracker=test_acc_tracker)\n    # Update learning rate according to schedule\n    scheduler.step()\n\ntotal_time = time.time() - start_time\nprint('Total training time: {} seconds'.format(total_time))"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}